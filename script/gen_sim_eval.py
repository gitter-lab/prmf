#!/usr/bin/env python
import csv
import sys, argparse
import os, os.path
import numpy as np
import pandas as pd
import networkx as nx
import factorlib as fl
from factorlib import script_utils
import re

# TODO 
# used to represent the ground truth pathways which are assumed to be the first K_LATENT pathways in the pathways file 
K_LATENT = 30
PATHWAY_INT_REGEXP = re.compile('pathway(\d+)')
SIM_DIR_REGEXP = re.compile('sim(\d+)')

def append_if_exists(ll, fp):
  if os.path.exists(fp):
    ll.append(fp)

def main():
  parser = argparse.ArgumentParser(description="""
Script to evaluate matrix factorization methods where data is generated by nmf_pathway_sim_gen.py
Evaluation is reported as a box and whisker plot for each method where we are measuring the 
number of ground truth pathways that each method recovers in each simulated run 
""")
  parser.add_argument("--indir", help="Argument used as outdir to gen_sim_pipeline.py")
  parser.add_argument("--outdir", help="Directory to write results including a box and whisker plot for all the methods evaluated")
  args = parser.parse_args()
  script_utils.log_script(sys.argv)

  sim_dirs = []
  for fname in os.listdir(args.indir):
    match_data = SIM_DIR_REGEXP.match(fname)
    if match_data is not None:
      sim_dirs.append(os.path.join(args.indir, fname))
  pathways_files = []
  nmf_gene_by_latent_files = []
  prmf_obj_files = []
  prmf_gene_by_latent_files = []
  plier_pathway_by_latent_files = []
  plier_gene_by_latent_files = []
  for sim_dir in sim_dirs:
    append_if_exists(pathay_files, os.path.join(sim_dir, 'pathways_file.txt'))
    append_if_exists(nmf_gene_by_latent_files, os.path.join(sim_dir, 'nmf', 'V.csv'))
    append_if_exists(prmf_gene_by_latent_files, os.path.join(sim_dir, 'prmf', 'V.csv'))
    append_if_exists(plier_gene_by_latent_files, os.path.join(sim_dir, 'plier', 'Z.csv'))
    append_if_exists(prmf_obj_files, os.path.join(sim_dir, 'prmf', 'obj.txt'))
    append_if_exists(plier_pathway_by_latent_files, os.path.join(sim_dir, 'plier', 'U.csv'))

  if len(nmf_gene_by_latent_files) > 0:
    nmf_vals, list_nmf_pathway_latent_scores_df = eval_nmf_runs(pathways_files, nmf_gene_by_latent_files)
    print('nmf')
    print(nmf_vals)

  if len(prmf_obj_files) > 0:
    print('prmf')
    prmf_vals = eval_prmf_runs(pathways_files, prmf_obj_files)
    prmf_vals_by_mass = eval_nmf_runs(pathways_files, prmf_gene_by_latent_files)
    print(prmf_vals)
    print(prmf_vals_by_mass)

  if len(plier_pathway_by_latent_files) > 0:
    print('plier')
    plier_vals = eval_plier_runs(pathways_files, plier_pathway_by_latent_files)
    print(plier_vals)

  # TODO box plot

def eval_nmf_runs(pathways_files, nmf_gene_by_latent_files):
  # TODO assumes true pathways are numbered 0 to 29, instead pass them as an argument
  vals = np.zeros((len(nmf_gene_by_latent_files),))
  if len(pathways_files) != len(nmf_gene_by_latent_files):
    raise Exception("len(pathways_files) = {} != {} = len(nmf_gene_by_latent_files)".format(len(pathways_files), len(nmf_gene_by_latent_files)))

  list_pathway_latent_scores_df = []
  for i in range(len(pathways_files)):
    pathways_file = pathways_files[i]
    pathways = parse_pathways(pathways_file)
    pathway_names = parse_pathway_names(pathways_file)

    nmf_gene_by_latent_file = nmf_gene_by_latent_files[i]
    nmf_gene_by_latent_df = pd.read_csv(nmf_gene_by_latent_file, index_col=0)
    n_gene, k_latent = nmf_gene_by_latent_df.shape

    pathway_latent_scores = np.zeros((len(pathways), K_LATENT))
    for j in range(len(pathways)): 
      pathway_members = list(map(lambda x: x[1]['name'], pathways[j].nodes(data=True))) 
      pathway_members_set = set(pathway_members) 
      all_names = set(nmf_gene_by_latent_df.index) 
      non_pathway_members = all_names - pathway_members_set 
      for k in range(K_LATENT): 
        gene_by_latent_slice = nmf_gene_by_latent_df.loc[pathway_members,"LV{}".format(k)]
        pathway_latent_scores[j,k] = np.mean(gene_by_latent_slice)

    pathway_latent_scores_df = pd.DataFrame(pathway_latent_scores, index=pathway_names, columns=map(lambda x: "LV{}".format(x), range(k_latent)))
    list_pathway_latent_scores_df.append(pathway_latent_scores_df)

    latent_to_pathway = pathway_latent_scores_df.idxmax(axis=0)
    selected_pathways = map(lambda x: x[1], latent_to_pathway.items())
    count = 0 
    for pathway in selected_pathways:
      if parse_pathway_int(pathway) < K_LATENT:
        count += 1
    vals[i] = count

  return vals, list_pathway_latent_scores_df

def eval_prmf_runs(pathways_files, prmf_obj_files):
  if len(pathways_files) != len(prmf_obj_files):
    raise Exception("len(pathways_files) = {} != {} = len(prmf_obj_files)".format(len(pathways_files), len(prmf_obj_files)))

  vals = np.zeros((len(prmf_obj_files),))
  for i in range(len(prmf_obj_files)):
    prmf_obj_file = prmf_obj_files[i]
    latent_to_pathway = fl.parse_pathway_obj(prmf_obj_file)
    for k, v in latent_to_pathway.items():
      pathway_int = parse_pathway_int(v)
      if pathway_int < K_LATENT:
        vals[i] += 1
  return vals

def eval_plier_runs(pathways_files, plier_pathway_by_latent_files):
  if len(pathways_files) != len(plier_pathway_by_latent_files):
    raise Exception("len(pathways_files) = {} != {} = len(plier_pathway_by_latent_files)".format(len(pathways_files), len(plier_pathway_by_latent_files)))

  vals = np.zeros((len(plier_pathway_by_latent_files),))
  for i in range(len(plier_pathway_by_latent_files)):
    plier_pathway_by_latent_file = plier_pathway_by_latent_files[i]
    plier_pathway_by_latent_df = pd.read_csv(plier_pathway_by_latent_file, index_col=0)
    p_pathways, k_latent = plier_pathway_by_latent_df.shape

    argmax_df = plier_pathway_by_latent_df.idxmax(axis=0)
    for k in range(k_latent):
      pathway_name = argmax_df.iloc[k]
      latent_name = plier_pathway_by_latent_df.columns[k]

      # max must be greater than 0 to be counted
      argmax_val = plier_pathway_by_latent_df.loc[pathway_name, latent_name]
      if(argmax_val > 0):
        pathway_int = parse_pathway_int(pathway_name)
        if pathway_int < K_LATENT:
          vals[i] += 1
  return vals

def get_true_pathways(pathways_file):
  rv = []
  with open(pathways_file, 'r') as fh:
    i = 0
    for line in fh:
      line = line.rstrip()
      rv.append(line)
      i += 1
      if i >= 30:
        break
  return rv

def parse_pathways(pathways_file):
  rv = []
  with open(pathways_file, 'r') as fh:
    for line in fh:
      line = line.rstrip()
      G = nx.read_graphml(line)
      rv.append(G)
  return rv

def parse_pathway_names(pathways_file):
  rv = []
  with open(pathways_file, 'r') as fh:
    for line in fh:
      line = line.rstrip()
      bn_ext = os.path.basename(line)
      bn, ext = os.path.splitext(bn_ext)
      rv.append(bn)
  return rv

def parse_pathway_int(pathway_fp):
  match_data = PATHWAY_INT_REGEXP.search(pathway_fp)
  # TODO currently just propagating the match error if it occurs
  # need to do something if match_data is None
  return int(match_data.group(1))

if __name__ == "__main__":
  main()
