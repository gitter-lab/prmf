{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Pathway-Regularized Matrix Factorization (PRMF) Slides from ISMB2018 Dependencies Python (3.7.0) numpy scipy matplotlib sklearn networkx R (3.5.1) KEGGREST KEGGgraph graph igraph argparse","title":"Home"},{"location":"#pathway-regularized-matrix-factorization-prmf","text":"Slides from ISMB2018","title":"Pathway-Regularized Matrix Factorization (PRMF)"},{"location":"#dependencies","text":"Python (3.7.0) numpy scipy matplotlib sklearn networkx R (3.5.1) KEGGREST KEGGgraph graph igraph argparse","title":"Dependencies"},{"location":"reference/prmf/","text":"Module prmf View Source import sys import math import os , os.path import re import itertools as it import numpy as np import networkx as nx import sklearn import sklearn.metrics import scipy import scipy.sparse as sp from scipy.stats import gamma from scipy.sparse.linalg import norm class FactorLibException ( Exception ): pass # TODO copied from ppi.irefindex def transform_nearest_neighbors ( G , k = 10 , attr = 'weight' , attr_type = 'similarity' ): \"\"\" Remove edges in G. For each node, keep at most 10 of its neighbors. Kept neighbors have the highest 'weight' edge attribute. Returns a copy of G and does not modify G. Parameters ---------- G : nx.Graph k : int default 10 attr : str default 'weight' attr_type : str 'similarity': nearest neighbors are those that are most similar: keep neighbors with highest edge weight 'distance': nearest neighbers are those that are least distant from each other: keep neighbors with least edge weight Returns ----- H : nx.Graph k-nearest neighbor graph note that a vertex v may have more than k edges incident to it if its k'>k neighbor has v in its k-nearest neighbors \"\"\" attr_types = [ 'similarity' , 'distance' ] if ( attr_type not in attr_types ): raise ArgumentError ( \"attr_type: {} should be one of {} \" . format ( attr_type , \",\" . join ( attr_types ))) H = nx . Graph () # TODO assumes undirected for node in G . nodes_iter (): inc_edges = G . edges ([ node ]) edge_weight_pairs = [] for edge in inc_edges : weight = G [ edge [ 0 ]][ edge [ 1 ]][ attr ] pair = ( edge , weight ) edge_weight_pairs . append ( pair ) if ( attr_type == 'similarity' ): edge_weight_pairs = sorted ( edge_weight_pairs , key = lambda x : x [ 1 ], reverse = True ) else : edge_weight_pairs = sorted ( edge_weight_pairs , key = lambda x : x [ 1 ]) for pair in edge_weight_pairs [: k ]: edge = pair [ 0 ] weight = pair [ 1 ] H . add_edge ( edge [ 0 ], edge [ 1 ], { 'weight' : weight }) return H def relabel_nodes ( G , node_attribute ): \"\"\" Wrapper around nx.relabel_nodes for scripts: relabel nodes using the <node_attribute> \"\"\" if node_attribute is not None : mapping = {} for node in G . nodes (): node_attrs = G . node [ node ] if node_attribute in node_attrs : mapping [ node ] = node_attrs [ node_attribute ] G = nx . relabel_nodes ( G , mapping ) return G def rbf_similarity ( stdev , x , y ): return math . exp ( - ( np . abs ( x - y ) / stdev ) ** 2 ) def combine_graphs ( Gs , weights ): \"\"\" Combine graphs into a single graph by constructing a weighted sum of their edge weights G_1 := (V_1, E_1) G_2 := (V_2, E_2) where E_i is a set of tuples (u,v,w) G := (V_1 \\cup V_2, E) TODO \"\"\" def get_edge_weight ( G , u , v ): # assumes edge is already known to be present in G weight = G [ u ][ v ] . get ( 'weight' ) if weight is None : weight = 1 return weight total_weight = 0.0 for weight in weights : total_weight += weight G_rv = nx . Graph () for i , G in enumerate ( Gs ): graph_weight = weights [ i ] for edge in G . edges_iter (): weight = get_edge_weight ( G , * edge ) if G_rv . has_edge ( * edge ): weight_rv = G_rv [ edge [ 0 ]][ edge [ 1 ]][ 'weight' ] G_rv [ edge [ 0 ]][ edge [ 1 ]][ 'weight' ] = weight_rv + graph_weight * weight else : G_rv . add_edge ( edge [ 0 ], edge [ 1 ], { 'weight' : graph_weight * weight }) return G_rv def diffusion ( M , adj , alpha = 0.7 , tol = 10e-6 ): # TODO equation, M, alpha \"\"\"Network propagation iterative process Iterative algorithm for apply propagation using random walk on a network: Initialize:: X1 = M Repeat:: X2 = alpha * X1.A + (1-alpha) * M X1 = X2 Until:: norm(X2-X1) < tol Where:: A : degree-normalized adjacency matrix Parameters ---------- M : sparse matrix Data matrix to be diffused. adj : sparse matrix Adjacency matrice. alpha : float, default: 0.7 Diffusion/propagation factor with 0 <= alpha <= 1. For alpha = 0 : no diffusion. For alpha = 1 : tol : float, default: 10e-6 Convergence threshold. Returns ------- X2 : sparse matrix Smoothed matrix. Notes ----- Copied from the stratipy Python library \"\"\" n = adj . shape [ 0 ] adj = adj + sp . eye ( n ) d = sp . dia_matrix (( np . array ( adj . sum ( axis = 0 )) **- 1 , [ 0 ]), shape = ( n , n )) A = adj . dot ( d ) X1 = M X2 = alpha * X1 . dot ( A ) + ( 1 - alpha ) * M i = 0 while norm ( X2 - X1 ) > tol : X1 = X2 X2 = alpha * X1 . dot ( A ) + ( 1 - alpha ) * M i += 1 return X2 def parse_ws_delim ( fh ): return parse_nodelist ( fh ) # TODO move to prmf_runner.py def parse_pathway_obj ( fp ): \"\"\" Extract mapping of latent factor to pathway fp \"\"\" rv = {} regexp = re . compile ( r '(\\d+)\\s*->\\s*(.+)' ) with open ( fp ) as fh : for line in fh : line = line . rstrip () match_data = regexp . match ( line ) if match_data is not None : rv [ int ( match_data . group ( 1 ))] = match_data . group ( 2 ) return rv # TODO move to prmf_runner.py def parse_init ( fp ): \"\"\" Extract filepaths used to initialize nmf_pathway.py as reported by its output stored in <fp> \"\"\" rv = [] with open ( fp , 'r' ) as fh : for line in fh : line = line . rstrip () rv . append ( line ) return rv def prepare_nodelist ( lists ): \"\"\" Combine lists of nodes into a single master list for use by nmf_pathway and other functions that accept a nodelist as input \"\"\" set_all = set () for ll in lists : for item in ll : set_all . add ( item ) rv = sorted ( set_all ) return rv def parse_gene_lists ( nodelist , gene_list_fps ): \"\"\" Parse gene lists into a sparse scipy array Parameters ---------- nodelist : list of str gene identifiers that define the assignment of array indexes to genes gene_lists : list of Returns ------- mat : sparse csc matrix \"\"\" # parse gene lists gene_lists = [] for gene_path in gene_list_fps : with open ( gene_path ) as fh : gene_lists . append ( parse_ws_delim ( fh )) # verify gene lists present in ppi_db def get_row_vec_for_gene_list ( gene_list ): row_vec , missing = embed_ids ( nodelist , gene_list ) sys . stderr . write ( \"missing {} / {} node identifiers: {} \\n \" . format ( len ( missing ), len ( gene_list ), \", \" . join ( missing ))) return row_vec row_vecs = map ( get_row_vec_for_gene_list , gene_lists ) mat = sp . vstack ( row_vecs ) return mat def parse_nodelist ( fh ): \"\"\" Return a list of node identifiers which maps the list index to the identifier \"\"\" rv = [] for line in fh : line = line . rstrip () words = line . split () for word in words : rv . append ( word ) return rv def parse_pathways ( graphml_fps ): Gs = list ( map ( lambda x : nx . read_graphml ( x ), graphml_fps )) return Gs def parse_pathways_dir ( pathways_dir ): \"\"\" Parse .graphml files in <pathways_dir> Returns ------- Gs : list of nx.DiGraph parsed pathways in <pathways_dir> \"\"\" graphmls = [] for fname in os . listdir ( pathways_dir ): basename , ext = os . path . splitext ( fname ) if ext == \".graphml\" : graphmls . append ( os . path . join ( pathways_dir , fname )) return parse_pathways ( graphmls ) def parse_seedlist ( fp , filetype = None , node_attr = 'name' ): \"\"\" Parse the file with filepath <fp> as a seed list: a list of node identifier strings. Parameters ---------- fp : str a file containing gene identifiers filetype : str, None the file type, informs parsing if None, determined by the extension of fp \"\"\" if filetype is None : fp_no_ext , ext = os . path . splitext ( fp ) if ext == '.graphml' : filetype = 'graphml' elif ext == '.txt' : filetype = 'txt' if filetype is None : raise FactorLibException ( \"Cannot determine filetype\" ) seedlist = None if filetype == 'graphml' : G = nx . read_graphml ( fp ) G = relabel_nodes ( G , node_attr ) seedlist = G . nodes () elif filetype == 'txt' : seedlist = [] with open ( fp , 'r' ) as fh : for line in fh : line = line . rstrip () seedlist . append ( line ) return seedlist def embed_ids ( all_ids , ids ): \"\"\" Construct a binary vector of length len(<all_ids>) with a 1 in the positions associated with each id in <ids> \"\"\" row_vec = np . zeros ( len ( all_ids )) missing = [] for id in ids : try : index = all_ids . index ( id ) row_vec [ index ] = 1 except ValueError : missing . append ( id ) row_vec_sparse = sp . csc_matrix ( row_vec ) return ( row_vec_sparse , missing ) def embed_arr ( all_col_names , some_col_names , arr ): \"\"\" Embed <arr> with len(some_col_names) number of columns in a larger array with len(all_col_names) number of columns. The larger array is constructed and returned. \"\"\" m , n = arr . shape if len ( some_col_names ) != n : raise FactorLibException ( 'some_col_names != #columns of arr: {} != {} ' . format ( some_col_names , n )) n2 = len ( all_col_names ) rv = np . zeros (( m , n2 )) all_name_to_ind = {} for i , name in enumerate ( all_col_names ): all_name_to_ind [ name ] = i for i in range ( m ): for j , name in enumerate ( some_col_names ): j2 = all_name_to_ind [ name ] rv [ i , j2 ] = arr [ i , j ] return rv def get_adj_mat ( G ): \"\"\"Represent ppi network as adjacency matrix Parameters ---------- G : networkx graph ppi network, see get_ppi() Returns ------- adj : square sparse scipy matrix (i,j) has a 1 if there is an interaction reported by irefindex ids : list same length as adj, ith index contains irefindex unique identifier for gene whose interactions are reported in the ith row of adj \"\"\" ids = G . nodes () adj = nx . to_scipy_sparse_matrix ( G , nodelist = ids , dtype = bool ) return adj , ids def weighted_union ( Gs , weight_attr = 'weight' ): \"\"\" Construct a weighted graph that is the union of graphs <Gs>. For each edge in a network in <Gs>, use <weight_attr> as the edge weight or 1 if that attribute is not present. Each edge in G_rv has its <weight_attr> set to the average weight of <Gs> that the edge appears in. That is, networks without the edge do not pull down the average toward 0. Parameters ---------- Gs : iterable of nx.Graph weight_attr : str name of edge attribute to use as weight Returns ------- G_rv : nx.Graph \"\"\" G_rv = nx . Graph () edge_to_count = {} def get_edge_weight ( G , edge ): weight = G [ edge [ 0 ]][ edge [ 1 ]] . get ( weight_attr ) if weight is None : weight = 1 return weight for G in Gs : for edge in G . edges_iter (): weight = get_edge_weight ( G , edge ) prev_weight = 0 if G_rv . has_edge ( * edge ): prev_weight = get_edge_weight ( G_rv , edge ) weight_sum = prev_weight + weight G_rv . add_edge ( edge [ 0 ], edge [ 1 ], { weight_attr : weight_sum }) if edge in edge_to_count : edge_to_count [ edge ] += 1 else : edge_to_count [ edge ] = 1 for node in G . nodes_iter (): if node not in G_rv : G_rv . add_node ( node ) for edge , count in edge_to_count . items (): weight = get_edge_weight ( G_rv , edge ) avg_weight = float ( weight ) / count G_rv [ edge [ 0 ]][ edge [ 1 ]][ weight_attr ] = avg_weight return G_rv def mat_to_bipartite ( mat ): G = nx . Graph () row_node_ids = [] col_node_ids = [] n_rows , n_cols = mat . shape for i in range ( n_rows ): node_i = \"r {} \" . format ( i ) row_node_ids . append ( node_i ) G . add_node ( node_i , bipartite = 0 ) for j in range ( n_cols ): node_j = \"c {} \" . format ( j ) col_node_ids . append ( node_j ) G . add_node ( node_j , bipartite = 1 ) for i in range ( n_rows ): for j in range ( n_cols ): G . add_edge ( \"r {} \" . format ( i ), \"c {} \" . format ( j ), { 'weight' : mat [ i , j ]}) return G , row_node_ids , col_node_ids def to_unit_vector ( vec ): rv = None norm = np . linalg . norm ( vec ) if ( norm == 0 ): rv = vec else : rv = vec / norm return rv def nodelists_to_mat ( sub_lists , uni_list ): \"\"\"Represent gene nodelist as a matrix with shape (len(uni_list), len(sub_lists)) Parameters ---------- sub_lists : list of str uni_list : list of str Returns ------- mat : np.array matrix where each column is associated with a nodelist in <sub_lists> and has a 1 in each row where that gene is in the node list TODO ---- For a single pathway, this representation has n_genes_in_pathway \"units of signal\" while the sampled signal has units <= n_genes_in_pathway (because a subset is sampled) \"\"\" node_to_index = {} for i , node in enumerate ( uni_list ): node_to_index [ node ] = i n_pathways = len ( sub_lists ) mat = np . zeros (( len ( uni_list ), n_pathways )) for pathway_ind in range ( len ( sub_lists )): pathway = sub_lists [ pathway_ind ] for node in pathway : index = node_to_index [ node ] if index is None : sys . stderr . write ( \"Unrecognized gene ID: {} \\n \" . format ( node )) else : mat [ index , pathway_ind ] = 1 return mat def normalize_num_pathways ( W_mat , pathways_mat ): \"\"\" Prior to matching latent factors to pathways, this function MAY be invoked to transform W_mat or pathways_mat (whichever is smaller) to make it so the number of latent factors is the same as the number of pathways by adding a binary vector of all zeros (the empty pathway). Parameters ---------- W_mat : np.array array with latent factors as columns and genes as rows with gene loadings into latent factors in cells pathways_mat : np.array array with pathways as columns and genes as rows with a 1 or 0 in each cell indicating whether that gene is present in the pathway or not Returns ------- W_mat : np.array See description pathways_mat : np.array See description \"\"\" # reshape 1D vectors if needed shp = W_mat . shape if ( len ( shp ) == 1 ): W_mat = W_mat . reshape (( shp [ 0 ], 1 )) shp = pathways_mat . shape if ( len ( shp ) == 1 ): pathways_mat = pathways_mat . reshape (( shp [ 0 ], 1 )) n_genes , n_latent_factors = W_mat . shape shp = pathways_mat . shape n_genes2 , n_pathways = pathways_mat . shape if ( n_genes != n_genes2 ): raise ValueError ( \" {} != {} : number of genes identified by W_mat is not the same as those identified by pathways_mat\" . format ( n_genes , n_genes2 )) arrs = [ W_mat , pathways_mat ] small_arr = - 1 large_arr = - 1 diff = 0 if ( n_latent_factors < n_pathways ): small_arr = 0 large_arr = 1 diff = n_pathways - n_latent_factors elif ( n_pathways < n_latent_factors ): small_arr = 1 large_arr = 0 diff = n_latent_factors - n_pathways # else do nothing cols = np . zeros (( n_genes , diff )) arrs [ small_arr ] = np . concatenate (( arrs [ small_arr ], cols ), axis = 1 ) return tuple ( arrs ) def match ( W_mat , pathways_mat ): return match_pr ( W_mat , pathways_mat ) def match_pr ( W_mat , pathways_mat ): n_genes , n_factors = W_mat . shape n_genes2 , n_pathways = pathways_mat . shape if ( n_genes != n_genes2 ): raise ValueError ( \"n_genes != n_genes2 : {} != {} \" . format ( n_genes , n_genes2 )) # prepare max weight matching by AUC aucs = np . zeros (( n_factors , n_pathways )) for i in range ( n_factors ): factor_vec = W_mat [:, i ] for j in range ( n_pathways ): pathway_vec = pathways_mat [:, j ] y_score = factor_vec y_true = pathway_vec auc = sklearn . metrics . average_precision_score ( y_true , y_score ) aucs [ i , j ] = auc # use networkx for max weight matching G , factor_node_ids , pathway_node_ids = mat_to_bipartite ( aucs ) matching = nx . max_weight_matching ( G ) # I don't like the return value from networkx, reorganize data rv = transform_matching ( G , matching , factor_node_ids ) return rv def match_dist ( W_mat , pathways_mat ): \"\"\" Match latent factors to pathways Parameters ---------- W_mat : np.array pathways_mat : np.array Returns ------- rv : list of tpl each tpl is of the form (<latent_factor_id>, <pathway_id>, <distance>) where W_mat latent factors are identified with names \"r0\", \"r1\", etc. (for \"row\" of the distance matrix constructed in this function) associated with each latent factor in W_mat; and pathways are identified with names \"c0\", \"c1\", etc. (for \"column\" of the distance matrix) TODO ---- other type of transformation of distance to similarity? RBF? implement version that uses hypergeometic p-values as distance measure? \"\"\" n_genes , n_factors = W_mat . shape n_genes2 , n_pathways = pathways_mat . shape if ( n_genes != n_genes2 ): raise ValueError ( \"n_genes != n_genes2 : {} != {} \" . format ( n_genes , n_genes2 )) #if(n_factors != n_pathways): # raise ValueError(\"n_factors != n_pathways: {} != {}\".format(n_factors, n_pathways)) dists = np . zeros (( n_factors , n_pathways )) max_unit_vec_dist = math . sqrt ( 2 ) for i in range ( n_factors ): factor_vec = W_mat [:, i ] # scale distance by size of latent factor and pathway factor_vec_unit = to_unit_vector ( factor_vec ) for j in range ( n_pathways ): pathway_vec = pathways_mat [:, j ] pathway_vec_unit = to_unit_vector ( pathway_vec ) dists [ i , j ] = np . linalg . norm ( factor_vec_unit - pathway_vec_unit ) dists_inv = 1 - ( dists / max_unit_vec_dist ) # use networkx for max weight matching G , factor_node_ids , pathway_node_ids = mat_to_bipartite ( dists_inv ) matching = nx . max_weight_matching ( G ) # I don't like the return value from networkx, reorganize data rv = transform_matching ( G , matching , factor_node_ids ) return rv def transform_matching ( G , matching , factor_node_ids ): \"\"\" Transform the return value of a matching from NetworkX into a different data structure Returns ------- rv : list of (factor_id, pathway_id, weight) \"\"\" rv = [] for factor_id in factor_node_ids : # not all nodes used in matching necessarily if factor_id in matching : pathway_id = matching [ factor_id ] weight = G [ factor_id ][ pathway_id ][ 'weight' ] tpl = ( factor_id , pathway_id , weight ) rv . append ( tpl ) else : # TODO temporary: why are they not included? # usually not included due to different numbers of latent factors and pathways weights = [] for neighbor in G . neighbors ( factor_id ): weights . append ( G [ factor_id ][ neighbor ][ 'weight' ]) sys . stderr . write ( \"Factor {} not included in matching; weights: \" . format ( factor_id ) + \" ; \" . join ( map ( str , weights )) + \" \\n \" ) return rv def sample ( G , n , t = 3 , alpha_portion = 0.9 , seed = None , node_order = None ): \"\"\" Sample <n> random values according to a distribution specified by G. Each value is a len(node_order) size vector of positive reals. If a node in G has no parents, its value is drawn according to Gamma(t^alpha_portion, t^(1-alpha_portion)). t is a reparameterization so that t is a location parameter. The default alpha_portion is chosen to be high so that the distribution is less uniform and more peaked around that location. Nodes which are present in <node_order> but not <G> are also have values drawn according to the aforementioned distribution. A node Y which has parents X_1, ..., X_k is specified by Y = \\sum_k Gamma(X_k^alpha_portion, X_k^(1-alpha_portion)) Y also has a Gamma distribution but it is not easily expressed. For Y = \\sum_k Gamma(alpha_k, beta), Y ~ Gamma(\\sum_k alpha_k, beta). The Y here is different but close. \"\"\" loc = 0 # TODO assumes connected if not G . is_directed (): raise FactorLibException ( 'G must be directed' ) if seed is not None : np . random . seed ( seed ) if node_order is None : node_order = G . nodes () m = len ( node_order ) rv = np . zeros (( m , n )) node_to_ind = {} for i , node in enumerate ( node_order ): node_to_ind [ node ] = i roots = [ n for n , d in G . in_degree () . items () if d == 0 ] for j in range ( n ): for root in roots : root_ind = node_to_ind [ root ] alpha = math . pow ( t , alpha_portion ) beta = math . pow ( t , 1 - alpha_portion ) rv [ root_ind , j ] = gamma . rvs ( alpha , loc , beta ) for source , target in nx . dfs_edges ( G , root ): source_ind = node_to_ind [ source ] target_ind = node_to_ind [ target ] alpha = math . pow ( rv [ source_ind , j ], alpha_portion ) beta = math . pow ( rv [ source_ind , j ], 1 - alpha_portion ) rv [ target_ind , j ] += gamma . rvs ( alpha , loc , beta ) other_nodes = set ( node_order ) - set ( G . nodes ()) for j in range ( n ): for node in other_nodes : node_ind = node_to_ind [ node ] alpha = math . pow ( t , alpha_portion ) beta = math . pow ( t , 1 - alpha_portion ) rv [ node_ind , j ] = gamma . rvs ( alpha , loc , beta ) return rv def compute_man_reg ( w , L ): return ( L . dot ( w )) . dot ( w ) def parse_achilles ( fp ): \"\"\" Parse Achilles data from http://portals.broadinstitute.org/achilles/datasets/18/download Parameters ---------- fp : str filepath to 'gene_dependency' file mentioned in Achilles README \"\"\" data = None row_to_cell_line = [] gene_names = None with open ( fp ) as fh : for line in fh : line = line . rstrip () gene_names = line . split ( ',' )[ 1 :] break for line in fh : ind = line . index ( ',' ) cell_line = line [ 0 : ind ] row_to_cell_line . append ( cell_line ) data = np . genfromtxt ( fp , delimiter = \",\" , skip_header = 1 ) data = data [:, 1 :] gene_names = achilles_to_hugo ( gene_names ) return data , row_to_cell_line , gene_names def achilles_to_hugo ( gene_names ): \"\"\" Transform gene identifiers from \"HUGO (Entrez)\" to \"HUGO\" \"\"\" regexp = re . compile ( r '([\\w-]+)\\s\\(\\w+\\)' ) rv = [] for i , gene_name in enumerate ( gene_names ): match_data = regexp . match ( gene_name ) if match_data is not None : rv . append ( match_data . group ( 1 )) else : raise FactorLibException ( \"Gene name {} at position {} does not match the specified format\" . format ( gene_name , i )) return rv def filter_vec_by_graph ( G , vec , nodelist ): \"\"\" Return a subset of vec and nodelist corresponding to the nodes defined in G \"\"\" inds = [] nodelist_sub = [] for i , node in enumerate ( nodelist ): if node in G : inds . append ( i ) nodelist_sub . append ( node ) vec_sub = vec [ inds ] return vec_sub , nodelist_sub def measure_cv_performance ( gene_by_latent_train , data_test ): \"\"\" Measure NMF model performance on held out data. Performance is evaluated based on the model's ability to reconstuct held out samples. \\hat{u} := arg min_{u} || x - uV^T || s.t. u >= 0 \\hat{x} := \\hat{u} V^T error = || x - \\hat{x} || normalized_error = error / ||x|| Parameters ---------- gene_by_latent_train : np.array V in the above equations data_test : np.array X in the above equations Returns ------- error : np.array normalized error for each sample \"\"\" m_samples , n_genes = data_test . shape error = np . zeros ( m_samples ,) # TODO multi-sample version of nnls? for m in range ( m_samples ): u_hat , err = scipy . optimize . nnls ( gene_by_latent_train , data_test [ m ,:]) error [ m ] = err / np . linalg . norm ( data_test [ m ,:]) return error Sub-modules prmf.ampl prmf.ensembl prmf.image_processing prmf.intersect prmf.plot prmf.prmf_args prmf.script_utils prmf.string_db Functions achilles_to_hugo def achilles_to_hugo ( gene_names ) Transform gene identifiers from \"HUGO (Entrez)\" to \"HUGO\" View Source def achilles_to_hugo ( gene_names ): \"\"\" Transform gene identifiers from \" HUGO ( Entrez ) \" to \" HUGO \" \"\"\" regexp = re . compile ( r '([\\w-]+)\\s\\(\\w+\\)' ) rv = [] for i , gene_name in enumerate ( gene_names ): match_data = regexp . match ( gene_name ) if match_data is not None : rv . append ( match_data . group ( 1 )) else : raise FactorLibException ( \"Gene name {} at position {} does not match the specified format\" . format ( gene_name , i )) return rv combine_graphs def combine_graphs ( Gs , weights ) Combine graphs into a single graph by constructing a weighted sum of their edge weights G_1 := (V_1, E_1) G_2 := (V_2, E_2) where E_i is a set of tuples (u,v,w) G := (V_1 \\cup V_2, E) TODO View Source def combine_graphs ( Gs , weights ) : \"\"\" Combine graphs into a single graph by constructing a weighted sum of their edge weights G_1 := (V_1, E_1) G_2 := (V_2, E_2) where E_i is a set of tuples (u,v,w) G := (V_1 \\cup V_2, E) TODO \"\"\" def get_edge_weight ( G , u , v ) : # assumes edge is already known to be present in G weight = G [ u ][ v ] . get ( 'weight' ) if weight is None : weight = 1 return weight total_weight = 0.0 for weight in weights : total_weight += weight G_rv = nx . Graph () for i , G in enumerate ( Gs ) : graph_weight = weights [ i ] for edge in G . edges_iter () : weight = get_edge_weight ( G , * edge ) if G_rv . has_edge ( * edge ) : weight_rv = G_rv [ edge[0 ] ] [ edge[1 ] ] [ 'weight' ] G_rv [ edge[0 ] ] [ edge[1 ] ] [ 'weight' ] = weight_rv + graph_weight * weight else : G_rv . add_edge ( edge [ 0 ] , edge [ 1 ] , { 'weight' : graph_weight * weight } ) return G_rv compute_man_reg def compute_man_reg ( w , L ) View Source def compute_man_reg ( w , L ): return ( L . dot ( w )). dot ( w ) diffusion def diffusion ( M , adj , alpha = 0.7 , tol = 1e-05 ) Network propagation iterative process Iterative algorithm for apply propagation using random walk on a network: Initialize:: X1 = M Repeat :: X2 = alpha * X1 . A + ( 1 - alpha ) * M X1 = X2 Until :: norm ( X2 - X1 ) < tol Where :: A : degree - normalized adjacency matrix Parameters M : sparse matrix Data matrix to be diffused. adj : sparse matrix Adjacency matrice. alpha : float, default: 0.7 Diffusion/propagation factor with 0 <= alpha <= 1. For alpha = 0 : no diffusion. For alpha = 1 : tol : float, default: 10e-6 Convergence threshold. Returns X2 : sparse matrix Smoothed matrix. Notes Copied from the stratipy Python library View Source def diffusion ( M , adj , alpha = 0 . 7 , tol = 10 e - 6 ): # TODO equation , M , alpha \"\"\"Network propagation iterative process Iterative algorithm for apply propagation using random walk on a network: Initialize:: X1 = M Repeat:: X2 = alpha * X1.A + (1-alpha) * M X1 = X2 Until:: norm(X2-X1) < tol Where:: A : degree-normalized adjacency matrix Parameters ---------- M : sparse matrix Data matrix to be diffused. adj : sparse matrix Adjacency matrice. alpha : float, default: 0.7 Diffusion/propagation factor with 0 <= alpha <= 1. For alpha = 0 : no diffusion. For alpha = 1 : tol : float, default: 10e-6 Convergence threshold. Returns ------- X2 : sparse matrix Smoothed matrix. Notes ----- Copied from the stratipy Python library \"\"\" n = adj . shape [ 0 ] adj = adj + sp . eye ( n ) d = sp . dia_matrix (( np . array ( adj . sum ( axis = 0 )) **- 1 , [ 0 ]), shape = ( n , n )) A = adj . dot ( d ) X1 = M X2 = alpha * X1 . dot ( A ) + ( 1 - alpha ) * M i = 0 while norm ( X2 - X1 ) > tol : X1 = X2 X2 = alpha * X1 . dot ( A ) + ( 1 - alpha ) * M i += 1 return X2 embed_arr def embed_arr ( all_col_names , some_col_names , arr ) Embed with len(some_col_names) number of columns in a larger array with len(all_col_names) number of columns. The larger array is constructed and returned. View Source def embed_arr ( all_col_names , some_col_names , arr ) : \"\"\" Embed <arr> with len(some_col_names) number of columns in a larger array with len(all_col_names) number of columns. The larger array is constructed and returned. \"\"\" m , n = arr . shape if len ( some_col_names ) != n : raise FactorLibException ( 'some_col_names != #columns of arr: {} != {}' . format ( some_col_names , n )) n2 = len ( all_col_names ) rv = np . zeros (( m , n2 )) all_name_to_ind = {} for i , name in enumerate ( all_col_names ) : all_name_to_ind [ name ] = i for i in range ( m ) : for j , name in enumerate ( some_col_names ) : j2 = all_name_to_ind [ name ] rv [ i,j2 ] = arr [ i,j ] return rv embed_ids def embed_ids ( all_ids , ids ) Construct a binary vector of length len( ) with a 1 in the positions associated with each id in View Source def embed_ids ( all_ids , ids ) : \"\"\" Construct a binary vector of length len(<all_ids>) with a 1 in the positions associated with each id in <ids> \"\"\" row_vec = np . zeros ( len ( all_ids )) missing = [] for id in ids : try : index = all_ids . index ( id ) row_vec [ index ] = 1 except ValueError : missing . append ( id ) row_vec_sparse = sp . csc_matrix ( row_vec ) return ( row_vec_sparse , missing ) filter_vec_by_graph def filter_vec_by_graph ( G , vec , nodelist ) Return a subset of vec and nodelist corresponding to the nodes defined in G View Source def filter_vec_by_graph ( G , vec , nodelist ) : \"\"\" Return a subset of vec and nodelist corresponding to the nodes defined in G \"\"\" inds = [] nodelist_sub = [] for i , node in enumerate ( nodelist ) : if node in G : inds . append ( i ) nodelist_sub . append ( node ) vec_sub = vec [ inds ] return vec_sub , nodelist_sub get_adj_mat def get_adj_mat ( G ) Represent ppi network as adjacency matrix Parameters G : networkx graph ppi network, see get_ppi() Returns adj : square sparse scipy matrix (i,j) has a 1 if there is an interaction reported by irefindex ids : list same length as adj, ith index contains irefindex unique identifier for gene whose interactions are reported in the ith row of adj View Source def get_adj_mat ( G ): \"\"\"Represent ppi network as adjacency matrix Parameters ---------- G : networkx graph ppi network, see get_ppi() Returns ------- adj : square sparse scipy matrix (i,j) has a 1 if there is an interaction reported by irefindex ids : list same length as adj, ith index contains irefindex unique identifier for gene whose interactions are reported in the ith row of adj \"\"\" ids = G . nodes () adj = nx . to_scipy_sparse_matrix ( G , nodelist = ids , dtype = bool ) return adj , ids mat_to_bipartite def mat_to_bipartite ( mat ) View Source def mat_to_bipartite ( mat ): G = nx . Graph () row_node_ids = [] col_node_ids = [] n_rows , n_cols = mat . shape for i in range ( n_rows ): node_i = \"r{}\" . format ( i ) row_node_ids . append ( node_i ) G . add_node ( node_i , bipartite = 0 ) for j in range ( n_cols ): node_j = \"c{}\" . format ( j ) col_node_ids . append ( node_j ) G . add_node ( node_j , bipartite = 1 ) for i in range ( n_rows ): for j in range ( n_cols ): G . add_edge ( \"r{}\" . format ( i ), \"c{}\" . format ( j ), { 'weight' : mat [ i , j ] } ) return G , row_node_ids , col_node_ids match def match ( W_mat , pathways_mat ) View Source def match ( W_mat , pathways_mat ): return match_pr ( W_mat , pathways_mat ) match_dist def match_dist ( W_mat , pathways_mat ) Match latent factors to pathways Parameters W_mat : np.array pathways_mat : np.array Returns rv : list of tpl each tpl is of the form ( , , ) where W_mat latent factors are identified with names \"r0\", \"r1\", etc. (for \"row\" of the distance matrix constructed in this function) associated with each latent factor in W_mat; and pathways are identified with names \"c0\", \"c1\", etc. (for \"column\" of the distance matrix) TODO other type of transformation of distance to similarity? RBF? implement version that uses hypergeometic p-values as distance measure? View Source def match_dist ( W_mat , pathways_mat ): \"\"\" Match latent factors to pathways Parameters ---------- W_mat : np.array pathways_mat : np.array Returns ------- rv : list of tpl each tpl is of the form (<latent_factor_id>, <pathway_id>, <distance>) where W_mat latent factors are identified with names \" r0 \", \" r1 \", etc. (for \" row \" of the distance matrix constructed in this function) associated with each latent factor in W_mat; and pathways are identified with names \" c0 \", \" c1 \", etc. (for \" column \" of the distance matrix) TODO ---- other type of transformation of distance to similarity? RBF? implement version that uses hypergeometic p-values as distance measure? \"\"\" n_genes , n_factors = W_mat . shape n_genes2 , n_pathways = pathways_mat . shape if ( n_genes != n_genes2 ): raise ValueError ( \"n_genes != n_genes2 : {} != {}\" . format ( n_genes , n_genes2 )) # if ( n_factors != n_pathways ): # raise ValueError ( \"n_factors != n_pathways: {} != {}\" . format ( n_factors , n_pathways )) dists = np . zeros (( n_factors , n_pathways )) max_unit_vec_dist = math . sqrt ( 2 ) for i in range ( n_factors ): factor_vec = W_mat [:, i ] # scale distance by size of latent factor and pathway factor_vec_unit = to_unit_vector ( factor_vec ) for j in range ( n_pathways ): pathway_vec = pathways_mat [:, j ] pathway_vec_unit = to_unit_vector ( pathway_vec ) dists [ i , j ] = np . linalg . norm ( factor_vec_unit - pathway_vec_unit ) dists_inv = 1 - ( dists / max_unit_vec_dist ) # use networkx for max weight matching G , factor_node_ids , pathway_node_ids = mat_to_bipartite ( dists_inv ) matching = nx . max_weight_matching ( G ) # I don ' t like the return value from networkx , reorganize data rv = transform_matching ( G , matching , factor_node_ids ) return rv match_pr def match_pr ( W_mat , pathways_mat ) View Source def match_pr ( W_mat , pathways_mat ): n_genes , n_factors = W_mat . shape n_genes2 , n_pathways = pathways_mat . shape if ( n_genes != n_genes2 ): raise ValueError ( \"n_genes != n_genes2 : {} != {}\" . format ( n_genes , n_genes2 )) # prepare max weight matching by AUC aucs = np . zeros (( n_factors , n_pathways )) for i in range ( n_factors ): factor_vec = W_mat [:, i ] for j in range ( n_pathways ): pathway_vec = pathways_mat [:, j ] y_score = factor_vec y_true = pathway_vec auc = sklearn . metrics . average_precision_score ( y_true , y_score ) aucs [ i , j ] = auc # use networkx for max weight matching G , factor_node_ids , pathway_node_ids = mat_to_bipartite ( aucs ) matching = nx . max_weight_matching ( G ) # I don ' t like the return value from networkx , reorganize data rv = transform_matching ( G , matching , factor_node_ids ) return rv measure_cv_performance def measure_cv_performance ( gene_by_latent_train , data_test ) Measure NMF model performance on held out data. Performance is evaluated based on the model's ability to reconstuct held out samples. \\hat{u} := arg min_{u} || x - uV^T || s.t. u >= 0 \\hat{x} := \\hat{u} V^T error = || x - \\hat{x} || normalized_error = error / ||x|| Parameters gene_by_latent_train : np.array V in the above equations data_test : np.array X in the above equations Returns error : np.array normalized error for each sample View Source def measure_cv_performance ( gene_by_latent_train , data_test ) : \"\"\" Measure NMF model performance on held out data. Performance is evaluated based on the model's ability to reconstuct held out samples. \\hat{u} := arg min_{u} || x - uV^T || s.t. u >= 0 \\hat{x} := \\hat{u} V^T error = || x - \\hat{x} || normalized_error = error / ||x|| Parameters ---------- gene_by_latent_train : np.array V in the above equations data_test : np.array X in the above equations Returns ------- error : np.array normalized error for each sample \"\"\" m_samples , n_genes = data_test . shape error = np . zeros ( m_samples ,) # TODO multi - sample version of nnls ? for m in range ( m_samples ) : u_hat , err = scipy . optimize . nnls ( gene_by_latent_train , data_test [ m,: ] ) error [ m ] = err / np . linalg . norm ( data_test [ m,: ] ) return error nodelists_to_mat def nodelists_to_mat ( sub_lists , uni_list ) Represent gene nodelist as a matrix with shape (len(uni_list), len(sub_lists)) Parameters sub_lists : list of str uni_list : list of str Returns mat : np.array matrix where each column is associated with a nodelist in and has a 1 in each row where that gene is in the node list TODO For a single pathway, this representation has n_genes_in_pathway \"units of signal\" while the sampled signal has units <= n_genes_in_pathway (because a subset is sampled) View Source def nodelists_to_mat ( sub_lists , uni_list ) : \"\"\"Represent gene nodelist as a matrix with shape (len(uni_list), len(sub_lists)) Parameters ---------- sub_lists : list of str uni_list : list of str Returns ------- mat : np.array matrix where each column is associated with a nodelist in <sub_lists> and has a 1 in each row where that gene is in the node list TODO ---- For a single pathway, this representation has n_genes_in_pathway \" units of signal \" while the sampled signal has units <= n_genes_in_pathway (because a subset is sampled) \"\"\" node_to_index = {} for i , node in enumerate ( uni_list ) : node_to_index [ node ] = i n_pathways = len ( sub_lists ) mat = np . zeros (( len ( uni_list ), n_pathways )) for pathway_ind in range ( len ( sub_lists )) : pathway = sub_lists [ pathway_ind ] for node in pathway : index = node_to_index [ node ] if index is None : sys . stderr . write ( \"Unrecognized gene ID: {}\\n\" . format ( node )) else : mat [ index,pathway_ind ] = 1 return mat normalize_num_pathways def normalize_num_pathways ( W_mat , pathways_mat ) Prior to matching latent factors to pathways, this function MAY be invoked to transform W_mat or pathways_mat (whichever is smaller) to make it so the number of latent factors is the same as the number of pathways by adding a binary vector of all zeros (the empty pathway). Parameters W_mat : np.array array with latent factors as columns and genes as rows with gene loadings into latent factors in cells pathways_mat : np.array array with pathways as columns and genes as rows with a 1 or 0 in each cell indicating whether that gene is present in the pathway or not Returns W_mat : np.array See description pathways_mat : np.array See description View Source def normalize_num_pathways ( W_mat , pathways_mat ) : \"\"\" Prior to matching latent factors to pathways, this function MAY be invoked to transform W_mat or pathways_mat (whichever is smaller) to make it so the number of latent factors is the same as the number of pathways by adding a binary vector of all zeros (the empty pathway). Parameters ---------- W_mat : np.array array with latent factors as columns and genes as rows with gene loadings into latent factors in cells pathways_mat : np.array array with pathways as columns and genes as rows with a 1 or 0 in each cell indicating whether that gene is present in the pathway or not Returns ------- W_mat : np.array See description pathways_mat : np.array See description \"\"\" # reshape 1 D vectors if needed shp = W_mat . shape if ( len ( shp ) == 1 ) : W_mat = W_mat . reshape (( shp [ 0 ] , 1 )) shp = pathways_mat . shape if ( len ( shp ) == 1 ) : pathways_mat = pathways_mat . reshape (( shp [ 0 ] , 1 )) n_genes , n_latent_factors = W_mat . shape shp = pathways_mat . shape n_genes2 , n_pathways = pathways_mat . shape if ( n_genes != n_genes2 ) : raise ValueError ( \"{} != {}: number of genes identified by W_mat is not the same as those identified by pathways_mat\" . format ( n_genes , n_genes2 )) arrs = [ W_mat, pathways_mat ] small_arr = - 1 large_arr = - 1 diff = 0 if ( n_latent_factors < n_pathways ) : small_arr = 0 large_arr = 1 diff = n_pathways - n_latent_factors elif ( n_pathways < n_latent_factors ) : small_arr = 1 large_arr = 0 diff = n_latent_factors - n_pathways # else do nothing cols = np . zeros (( n_genes , diff )) arrs [ small_arr ] = np . concatenate (( arrs [ small_arr ] , cols ), axis = 1 ) return tuple ( arrs ) parse_achilles def parse_achilles ( fp ) Parse Achilles data from http://portals.broadinstitute.org/achilles/datasets/18/download Parameters fp : str filepath to 'gene_dependency' file mentioned in Achilles README View Source def parse_achilles ( fp ): \"\"\" Parse Achilles data from http://portals.broadinstitute.org/achilles/datasets/18/download Parameters ---------- fp : str filepath to 'gene_dependency' file mentioned in Achilles README \"\"\" data = None row_to_cell_line = [] gene_names = None with open ( fp ) as fh : for line in fh : line = line . rstrip () gene_names = line . split ( ',' )[ 1 :] break for line in fh : ind = line . index ( ',' ) cell_line = line [ 0 : ind ] row_to_cell_line . append ( cell_line ) data = np . genfromtxt ( fp , delimiter = \",\" , skip_header = 1 ) data = data [:, 1 :] gene_names = achilles_to_hugo ( gene_names ) return data , row_to_cell_line , gene_names parse_gene_lists def parse_gene_lists ( nodelist , gene_list_fps ) Parse gene lists into a sparse scipy array Parameters nodelist : list of str gene identifiers that define the assignment of array indexes to genes gene_lists : list of Returns mat : sparse csc matrix View Source def parse_gene_lists ( nodelist , gene_list_fps ): \"\"\" Parse gene lists into a sparse scipy array Parameters ---------- nodelist : list of str gene identifiers that define the assignment of array indexes to genes gene_lists : list of Returns ------- mat : sparse csc matrix \"\"\" # parse gene lists gene_lists = [] for gene_path in gene_list_fps : with open ( gene_path ) as fh : gene_lists . append ( parse_ws_delim ( fh )) # verify gene lists present in ppi_db def get_row_vec_for_gene_list ( gene_list ): row_vec , missing = embed_ids ( nodelist , gene_list ) sys . stderr . write ( \"missing {}/{} node identifiers: {}\\n\" . format ( len ( missing ), len ( gene_list ), \", \" . join ( missing ))) return row_vec row_vecs = map ( get_row_vec_for_gene_list , gene_lists ) mat = sp . vstack ( row_vecs ) return mat parse_init def parse_init ( fp ) Extract filepaths used to initialize nmf_pathway.py as reported by its output stored in View Source def parse_init ( fp ): \"\"\" Extract filepaths used to initialize nmf_pathway.py as reported by its output stored in <fp> \"\"\" rv = [] with open ( fp , 'r' ) as fh : for line in fh : line = line . rstrip () rv . append ( line ) return rv parse_nodelist def parse_nodelist ( fh ) Return a list of node identifiers which maps the list index to the identifier View Source def parse_nodelist ( fh ): \"\"\" Return a list of node identifiers which maps the list index to the identifier \"\"\" rv = [] for line in fh : line = line . rstrip () words = line . split () for word in words : rv . append ( word ) return rv parse_pathway_obj def parse_pathway_obj ( fp ) Extract mapping of latent factor to pathway fp View Source def parse_pathway_obj ( fp ): \"\"\" Extract mapping of latent factor to pathway fp \"\"\" rv = {} regexp = re . compile ( r '(\\d+)\\s*->\\s*(.+)' ) with open ( fp ) as fh : for line in fh : line = line . rstrip () match_data = regexp . match ( line ) if match_data is not None : rv [ int ( match_data . group ( 1 ))] = match_data . group ( 2 ) return rv parse_pathways def parse_pathways ( graphml_fps ) View Source def parse_pathways ( graphml_fps ): Gs = list ( map ( lambda x : nx . read_graphml ( x ), graphml_fps )) return Gs parse_pathways_dir def parse_pathways_dir ( pathways_dir ) Parse .graphml files in Returns Gs : list of nx.DiGraph parsed pathways in View Source def parse_pathways_dir ( pathways_dir ): \"\"\" Parse .graphml files in <pathways_dir> Returns ------- Gs : list of nx.DiGraph parsed pathways in <pathways_dir> \"\"\" graphmls = [] for fname in os . listdir ( pathways_dir ): basename , ext = os . path . splitext ( fname ) if ext == \".graphml\" : graphmls . append ( os . path . join ( pathways_dir , fname )) return parse_pathways ( graphmls ) parse_seedlist def parse_seedlist ( fp , filetype = None , node_attr = 'name' ) Parse the file with filepath as a seed list: a list of node identifier strings. Parameters fp : str a file containing gene identifiers filetype : str, None the file type, informs parsing if None, determined by the extension of fp View Source def parse_seedlist ( fp , filetype = None , node_attr = 'name' ): \"\"\" Parse the file with filepath <fp> as a seed list: a list of node identifier strings. Parameters ---------- fp : str a file containing gene identifiers filetype : str, None the file type, informs parsing if None, determined by the extension of fp \"\"\" if filetype is None : fp_no_ext , ext = os . path . splitext ( fp ) if ext == '.graphml' : filetype = 'graphml' elif ext == '.txt' : filetype = 'txt' if filetype is None : raise FactorLibException ( \"Cannot determine filetype\" ) seedlist = None if filetype == 'graphml' : G = nx . read_graphml ( fp ) G = relabel_nodes ( G , node_attr ) seedlist = G . nodes () elif filetype == 'txt' : seedlist = [] with open ( fp , 'r' ) as fh : for line in fh : line = line . rstrip () seedlist . append ( line ) return seedlist parse_ws_delim def parse_ws_delim ( fh ) View Source def parse_ws_delim ( fh ): return parse_nodelist ( fh ) prepare_nodelist def prepare_nodelist ( lists ) Combine lists of nodes into a single master list for use by nmf_pathway and other functions that accept a nodelist as input View Source def prepare_nodelist ( lists ): \"\"\" Combine lists of nodes into a single master list for use by nmf_pathway and other functions that accept a nodelist as input \"\"\" set_all = set () for ll in lists : for item in ll : set_all . add ( item ) rv = sorted ( set_all ) return rv rbf_similarity def rbf_similarity ( stdev , x , y ) View Source def rbf_similarity ( stdev , x , y ): return math . exp ( - ( np . abs ( x - y ) / stdev ) ** 2 ) relabel_nodes def relabel_nodes ( G , node_attribute ) Wrapper around nx.relabel_nodes for scripts: relabel nodes using the View Source def relabel_nodes ( G , node_attribute ) : \"\"\" Wrapper around nx.relabel_nodes for scripts: relabel nodes using the <node_attribute> \"\"\" if node_attribute is not None : mapping = {} for node in G . nodes () : node_attrs = G . node [ node ] if node_attribute in node_attrs : mapping [ node ] = node_attrs [ node_attribute ] G = nx . relabel_nodes ( G , mapping ) return G sample def sample ( G , n , t = 3 , alpha_portion = 0.9 , seed = None , node_order = None ) Sample random values according to a distribution specified by G. Each value is a len(node_order) size vector of positive reals. If a node in G has no parents, its value is drawn according to Gamma(t^alpha_portion, t^(1-alpha_portion)). t is a reparameterization so that t is a location parameter. The default alpha_portion is chosen to be high so that the distribution is less uniform and more peaked around that location. Nodes which are present in but not are also have values drawn according to the aforementioned distribution. A node Y which has parents X_1, ..., X_k is specified by Y = \\sum_k Gamma(X_k^alpha_portion, X_k^(1-alpha_portion)) Y also has a Gamma distribution but it is not easily expressed. For Y = \\sum_k Gamma(alpha_k, beta), Y ~ Gamma(\\sum_k alpha_k, beta). The Y here is different but close. View Source def sample ( G , n , t = 3 , alpha_portion = 0.9 , seed = None , node_order = None ) : \"\"\" Sample <n> random values according to a distribution specified by G. Each value is a len(node_order) size vector of positive reals. If a node in G has no parents, its value is drawn according to Gamma(t^alpha_portion, t^(1-alpha_portion)). t is a reparameterization so that t is a location parameter. The default alpha_portion is chosen to be high so that the distribution is less uniform and more peaked around that location. Nodes which are present in <node_order> but not <G> are also have values drawn according to the aforementioned distribution. A node Y which has parents X_1, ..., X_k is specified by Y = \\sum_k Gamma(X_k^alpha_portion, X_k^(1-alpha_portion)) Y also has a Gamma distribution but it is not easily expressed. For Y = \\sum_k Gamma(alpha_k, beta), Y ~ Gamma(\\sum_k alpha_k, beta). The Y here is different but close. \"\"\" loc = 0 # TODO assumes connected if not G . is_directed () : raise FactorLibException ( 'G must be directed' ) if seed is not None : np . random . seed ( seed ) if node_order is None : node_order = G . nodes () m = len ( node_order ) rv = np . zeros (( m , n )) node_to_ind = {} for i , node in enumerate ( node_order ) : node_to_ind [ node ] = i roots = [ n for n,d in G.in_degree().items() if d==0 ] for j in range ( n ) : for root in roots : root_ind = node_to_ind [ root ] alpha = math . pow ( t , alpha_portion ) beta = math . pow ( t , 1 - alpha_portion ) rv [ root_ind,j ] = gamma . rvs ( alpha , loc , beta ) for source , target in nx . dfs_edges ( G , root ) : source_ind = node_to_ind [ source ] target_ind = node_to_ind [ target ] alpha = math . pow ( rv [ source_ind,j ] , alpha_portion ) beta = math . pow ( rv [ source_ind,j ] , 1 - alpha_portion ) rv [ target_ind,j ] += gamma . rvs ( alpha , loc , beta ) other_nodes = set ( node_order ) - set ( G . nodes ()) for j in range ( n ) : for node in other_nodes : node_ind = node_to_ind [ node ] alpha = math . pow ( t , alpha_portion ) beta = math . pow ( t , 1 - alpha_portion ) rv [ node_ind, j ] = gamma . rvs ( alpha , loc , beta ) return rv to_unit_vector def to_unit_vector ( vec ) View Source def to_unit_vector ( vec ): rv = None norm = np . linalg . norm ( vec ) if ( norm == 0 ): rv = vec else : rv = vec / norm return rv transform_matching def transform_matching ( G , matching , factor_node_ids ) Transform the return value of a matching from NetworkX into a different data structure Returns rv : list of (factor_id, pathway_id, weight) View Source def transform_matching ( G , matching , factor_node_ids ) : \"\"\" Transform the return value of a matching from NetworkX into a different data structure Returns ------- rv : list of (factor_id, pathway_id, weight) \"\"\" rv = [] for factor_id in factor_node_ids : # not all nodes used in matching necessarily if factor_id in matching : pathway_id = matching [ factor_id ] weight = G [ factor_id ][ pathway_id ][ 'weight' ] tpl = ( factor_id , pathway_id , weight ) rv . append ( tpl ) else : # TODO temporary : why are they not included ? # usually not included due to different numbers of latent factors and pathways weights = [] for neighbor in G . neighbors ( factor_id ) : weights . append ( G [ factor_id ][ neighbor ][ 'weight' ] ) sys . stderr . write ( \"Factor {} not included in matching; weights: \" . format ( factor_id ) + \" ; \" . join ( map ( str , weights )) + \"\\n\" ) return rv transform_nearest_neighbors def transform_nearest_neighbors ( G , k = 10 , attr = 'weight' , attr_type = 'similarity' ) Remove edges in G. For each node, keep at most 10 of its neighbors. Kept neighbors have the highest 'weight' edge attribute. Returns a copy of G and does not modify G. Parameters G : nx.Graph k : int default 10 attr : str default 'weight' attr_type : str 'similarity': nearest neighbors are those that are most similar: keep neighbors with highest edge weight 'distance': nearest neighbers are those that are least distant from each other: keep neighbors with least edge weight Returns H : nx.Graph k-nearest neighbor graph note that a vertex v may have more than k edges incident to it if its k'>k neighbor has v in its k-nearest neighbors View Source def transform_nearest_neighbors ( G , k = 10 , attr = 'weight' , attr_type = 'similarity' ) : \"\"\" Remove edges in G. For each node, keep at most 10 of its neighbors. Kept neighbors have the highest 'weight' edge attribute. Returns a copy of G and does not modify G. Parameters ---------- G : nx.Graph k : int default 10 attr : str default 'weight' attr_type : str 'similarity': nearest neighbors are those that are most similar: keep neighbors with highest edge weight 'distance': nearest neighbers are those that are least distant from each other: keep neighbors with least edge weight Returns ----- H : nx.Graph k-nearest neighbor graph note that a vertex v may have more than k edges incident to it if its k'>k neighbor has v in its k-nearest neighbors \"\"\" attr_types = [ 'similarity', 'distance' ] if ( attr_type not in attr_types ) : raise ArgumentError ( \"attr_type: {} should be one of {}\" . format ( attr_type , \",\" . join ( attr_types ))) H = nx . Graph () # TODO assumes undirected for node in G . nodes_iter () : inc_edges = G . edges ( [ node ] ) edge_weight_pairs = [] for edge in inc_edges : weight = G [ edge[0 ] ] [ edge[1 ] ] [ attr ] pair = ( edge , weight ) edge_weight_pairs . append ( pair ) if ( attr_type == 'similarity' ) : edge_weight_pairs = sorted ( edge_weight_pairs , key = lambda x : x [ 1 ] , reverse = True ) else : edge_weight_pairs = sorted ( edge_weight_pairs , key = lambda x : x [ 1 ] ) for pair in edge_weight_pairs [ :k ] : edge = pair [ 0 ] weight = pair [ 1 ] H . add_edge ( edge [ 0 ] , edge [ 1 ] , { 'weight' : weight } ) return H weighted_union def weighted_union ( Gs , weight_attr = 'weight' ) Construct a weighted graph that is the union of graphs . For each edge in a network in , use as the edge weight or 1 if that attribute is not present. Each edge in G_rv has its set to the average weight of that the edge appears in. That is, networks without the edge do not pull down the average toward 0. Parameters Gs : iterable of nx.Graph weight_attr : str name of edge attribute to use as weight Returns G_rv : nx.Graph View Source def weighted_union ( Gs , weight_attr = 'weight' ) : \"\"\" Construct a weighted graph that is the union of graphs <Gs>. For each edge in a network in <Gs>, use <weight_attr> as the edge weight or 1 if that attribute is not present. Each edge in G_rv has its <weight_attr> set to the average weight of <Gs> that the edge appears in. That is, networks without the edge do not pull down the average toward 0. Parameters ---------- Gs : iterable of nx.Graph weight_attr : str name of edge attribute to use as weight Returns ------- G_rv : nx.Graph \"\"\" G_rv = nx . Graph () edge_to_count = {} def get_edge_weight ( G , edge ) : weight = G [ edge[0 ] ] [ edge[1 ] ] . get ( weight_attr ) if weight is None : weight = 1 return weight for G in Gs : for edge in G . edges_iter () : weight = get_edge_weight ( G , edge ) prev_weight = 0 if G_rv . has_edge ( * edge ) : prev_weight = get_edge_weight ( G_rv , edge ) weight_sum = prev_weight + weight G_rv . add_edge ( edge [ 0 ] , edge [ 1 ] , { weight_attr : weight_sum } ) if edge in edge_to_count : edge_to_count [ edge ] += 1 else : edge_to_count [ edge ] = 1 for node in G . nodes_iter () : if node not in G_rv : G_rv . add_node ( node ) for edge , count in edge_to_count . items () : weight = get_edge_weight ( G_rv , edge ) avg_weight = float ( weight ) / count G_rv [ edge[0 ] ] [ edge[1 ] ] [ weight_attr ] = avg_weight return G_rv Classes FactorLibException class FactorLibException ( / , * args , ** kwargs ) Common base class for all non-exit exceptions. View Source class FactorLibException ( Exception ): pass Ancestors (in MRO) builtins.Exception builtins.BaseException Class variables args Methods with_traceback def with_traceback ( ... ) Exception.with_traceback(tb) -- set self. traceback to tb and return self.","title":"Index"},{"location":"reference/prmf/#module-prmf","text":"View Source import sys import math import os , os.path import re import itertools as it import numpy as np import networkx as nx import sklearn import sklearn.metrics import scipy import scipy.sparse as sp from scipy.stats import gamma from scipy.sparse.linalg import norm class FactorLibException ( Exception ): pass # TODO copied from ppi.irefindex def transform_nearest_neighbors ( G , k = 10 , attr = 'weight' , attr_type = 'similarity' ): \"\"\" Remove edges in G. For each node, keep at most 10 of its neighbors. Kept neighbors have the highest 'weight' edge attribute. Returns a copy of G and does not modify G. Parameters ---------- G : nx.Graph k : int default 10 attr : str default 'weight' attr_type : str 'similarity': nearest neighbors are those that are most similar: keep neighbors with highest edge weight 'distance': nearest neighbers are those that are least distant from each other: keep neighbors with least edge weight Returns ----- H : nx.Graph k-nearest neighbor graph note that a vertex v may have more than k edges incident to it if its k'>k neighbor has v in its k-nearest neighbors \"\"\" attr_types = [ 'similarity' , 'distance' ] if ( attr_type not in attr_types ): raise ArgumentError ( \"attr_type: {} should be one of {} \" . format ( attr_type , \",\" . join ( attr_types ))) H = nx . Graph () # TODO assumes undirected for node in G . nodes_iter (): inc_edges = G . edges ([ node ]) edge_weight_pairs = [] for edge in inc_edges : weight = G [ edge [ 0 ]][ edge [ 1 ]][ attr ] pair = ( edge , weight ) edge_weight_pairs . append ( pair ) if ( attr_type == 'similarity' ): edge_weight_pairs = sorted ( edge_weight_pairs , key = lambda x : x [ 1 ], reverse = True ) else : edge_weight_pairs = sorted ( edge_weight_pairs , key = lambda x : x [ 1 ]) for pair in edge_weight_pairs [: k ]: edge = pair [ 0 ] weight = pair [ 1 ] H . add_edge ( edge [ 0 ], edge [ 1 ], { 'weight' : weight }) return H def relabel_nodes ( G , node_attribute ): \"\"\" Wrapper around nx.relabel_nodes for scripts: relabel nodes using the <node_attribute> \"\"\" if node_attribute is not None : mapping = {} for node in G . nodes (): node_attrs = G . node [ node ] if node_attribute in node_attrs : mapping [ node ] = node_attrs [ node_attribute ] G = nx . relabel_nodes ( G , mapping ) return G def rbf_similarity ( stdev , x , y ): return math . exp ( - ( np . abs ( x - y ) / stdev ) ** 2 ) def combine_graphs ( Gs , weights ): \"\"\" Combine graphs into a single graph by constructing a weighted sum of their edge weights G_1 := (V_1, E_1) G_2 := (V_2, E_2) where E_i is a set of tuples (u,v,w) G := (V_1 \\cup V_2, E) TODO \"\"\" def get_edge_weight ( G , u , v ): # assumes edge is already known to be present in G weight = G [ u ][ v ] . get ( 'weight' ) if weight is None : weight = 1 return weight total_weight = 0.0 for weight in weights : total_weight += weight G_rv = nx . Graph () for i , G in enumerate ( Gs ): graph_weight = weights [ i ] for edge in G . edges_iter (): weight = get_edge_weight ( G , * edge ) if G_rv . has_edge ( * edge ): weight_rv = G_rv [ edge [ 0 ]][ edge [ 1 ]][ 'weight' ] G_rv [ edge [ 0 ]][ edge [ 1 ]][ 'weight' ] = weight_rv + graph_weight * weight else : G_rv . add_edge ( edge [ 0 ], edge [ 1 ], { 'weight' : graph_weight * weight }) return G_rv def diffusion ( M , adj , alpha = 0.7 , tol = 10e-6 ): # TODO equation, M, alpha \"\"\"Network propagation iterative process Iterative algorithm for apply propagation using random walk on a network: Initialize:: X1 = M Repeat:: X2 = alpha * X1.A + (1-alpha) * M X1 = X2 Until:: norm(X2-X1) < tol Where:: A : degree-normalized adjacency matrix Parameters ---------- M : sparse matrix Data matrix to be diffused. adj : sparse matrix Adjacency matrice. alpha : float, default: 0.7 Diffusion/propagation factor with 0 <= alpha <= 1. For alpha = 0 : no diffusion. For alpha = 1 : tol : float, default: 10e-6 Convergence threshold. Returns ------- X2 : sparse matrix Smoothed matrix. Notes ----- Copied from the stratipy Python library \"\"\" n = adj . shape [ 0 ] adj = adj + sp . eye ( n ) d = sp . dia_matrix (( np . array ( adj . sum ( axis = 0 )) **- 1 , [ 0 ]), shape = ( n , n )) A = adj . dot ( d ) X1 = M X2 = alpha * X1 . dot ( A ) + ( 1 - alpha ) * M i = 0 while norm ( X2 - X1 ) > tol : X1 = X2 X2 = alpha * X1 . dot ( A ) + ( 1 - alpha ) * M i += 1 return X2 def parse_ws_delim ( fh ): return parse_nodelist ( fh ) # TODO move to prmf_runner.py def parse_pathway_obj ( fp ): \"\"\" Extract mapping of latent factor to pathway fp \"\"\" rv = {} regexp = re . compile ( r '(\\d+)\\s*->\\s*(.+)' ) with open ( fp ) as fh : for line in fh : line = line . rstrip () match_data = regexp . match ( line ) if match_data is not None : rv [ int ( match_data . group ( 1 ))] = match_data . group ( 2 ) return rv # TODO move to prmf_runner.py def parse_init ( fp ): \"\"\" Extract filepaths used to initialize nmf_pathway.py as reported by its output stored in <fp> \"\"\" rv = [] with open ( fp , 'r' ) as fh : for line in fh : line = line . rstrip () rv . append ( line ) return rv def prepare_nodelist ( lists ): \"\"\" Combine lists of nodes into a single master list for use by nmf_pathway and other functions that accept a nodelist as input \"\"\" set_all = set () for ll in lists : for item in ll : set_all . add ( item ) rv = sorted ( set_all ) return rv def parse_gene_lists ( nodelist , gene_list_fps ): \"\"\" Parse gene lists into a sparse scipy array Parameters ---------- nodelist : list of str gene identifiers that define the assignment of array indexes to genes gene_lists : list of Returns ------- mat : sparse csc matrix \"\"\" # parse gene lists gene_lists = [] for gene_path in gene_list_fps : with open ( gene_path ) as fh : gene_lists . append ( parse_ws_delim ( fh )) # verify gene lists present in ppi_db def get_row_vec_for_gene_list ( gene_list ): row_vec , missing = embed_ids ( nodelist , gene_list ) sys . stderr . write ( \"missing {} / {} node identifiers: {} \\n \" . format ( len ( missing ), len ( gene_list ), \", \" . join ( missing ))) return row_vec row_vecs = map ( get_row_vec_for_gene_list , gene_lists ) mat = sp . vstack ( row_vecs ) return mat def parse_nodelist ( fh ): \"\"\" Return a list of node identifiers which maps the list index to the identifier \"\"\" rv = [] for line in fh : line = line . rstrip () words = line . split () for word in words : rv . append ( word ) return rv def parse_pathways ( graphml_fps ): Gs = list ( map ( lambda x : nx . read_graphml ( x ), graphml_fps )) return Gs def parse_pathways_dir ( pathways_dir ): \"\"\" Parse .graphml files in <pathways_dir> Returns ------- Gs : list of nx.DiGraph parsed pathways in <pathways_dir> \"\"\" graphmls = [] for fname in os . listdir ( pathways_dir ): basename , ext = os . path . splitext ( fname ) if ext == \".graphml\" : graphmls . append ( os . path . join ( pathways_dir , fname )) return parse_pathways ( graphmls ) def parse_seedlist ( fp , filetype = None , node_attr = 'name' ): \"\"\" Parse the file with filepath <fp> as a seed list: a list of node identifier strings. Parameters ---------- fp : str a file containing gene identifiers filetype : str, None the file type, informs parsing if None, determined by the extension of fp \"\"\" if filetype is None : fp_no_ext , ext = os . path . splitext ( fp ) if ext == '.graphml' : filetype = 'graphml' elif ext == '.txt' : filetype = 'txt' if filetype is None : raise FactorLibException ( \"Cannot determine filetype\" ) seedlist = None if filetype == 'graphml' : G = nx . read_graphml ( fp ) G = relabel_nodes ( G , node_attr ) seedlist = G . nodes () elif filetype == 'txt' : seedlist = [] with open ( fp , 'r' ) as fh : for line in fh : line = line . rstrip () seedlist . append ( line ) return seedlist def embed_ids ( all_ids , ids ): \"\"\" Construct a binary vector of length len(<all_ids>) with a 1 in the positions associated with each id in <ids> \"\"\" row_vec = np . zeros ( len ( all_ids )) missing = [] for id in ids : try : index = all_ids . index ( id ) row_vec [ index ] = 1 except ValueError : missing . append ( id ) row_vec_sparse = sp . csc_matrix ( row_vec ) return ( row_vec_sparse , missing ) def embed_arr ( all_col_names , some_col_names , arr ): \"\"\" Embed <arr> with len(some_col_names) number of columns in a larger array with len(all_col_names) number of columns. The larger array is constructed and returned. \"\"\" m , n = arr . shape if len ( some_col_names ) != n : raise FactorLibException ( 'some_col_names != #columns of arr: {} != {} ' . format ( some_col_names , n )) n2 = len ( all_col_names ) rv = np . zeros (( m , n2 )) all_name_to_ind = {} for i , name in enumerate ( all_col_names ): all_name_to_ind [ name ] = i for i in range ( m ): for j , name in enumerate ( some_col_names ): j2 = all_name_to_ind [ name ] rv [ i , j2 ] = arr [ i , j ] return rv def get_adj_mat ( G ): \"\"\"Represent ppi network as adjacency matrix Parameters ---------- G : networkx graph ppi network, see get_ppi() Returns ------- adj : square sparse scipy matrix (i,j) has a 1 if there is an interaction reported by irefindex ids : list same length as adj, ith index contains irefindex unique identifier for gene whose interactions are reported in the ith row of adj \"\"\" ids = G . nodes () adj = nx . to_scipy_sparse_matrix ( G , nodelist = ids , dtype = bool ) return adj , ids def weighted_union ( Gs , weight_attr = 'weight' ): \"\"\" Construct a weighted graph that is the union of graphs <Gs>. For each edge in a network in <Gs>, use <weight_attr> as the edge weight or 1 if that attribute is not present. Each edge in G_rv has its <weight_attr> set to the average weight of <Gs> that the edge appears in. That is, networks without the edge do not pull down the average toward 0. Parameters ---------- Gs : iterable of nx.Graph weight_attr : str name of edge attribute to use as weight Returns ------- G_rv : nx.Graph \"\"\" G_rv = nx . Graph () edge_to_count = {} def get_edge_weight ( G , edge ): weight = G [ edge [ 0 ]][ edge [ 1 ]] . get ( weight_attr ) if weight is None : weight = 1 return weight for G in Gs : for edge in G . edges_iter (): weight = get_edge_weight ( G , edge ) prev_weight = 0 if G_rv . has_edge ( * edge ): prev_weight = get_edge_weight ( G_rv , edge ) weight_sum = prev_weight + weight G_rv . add_edge ( edge [ 0 ], edge [ 1 ], { weight_attr : weight_sum }) if edge in edge_to_count : edge_to_count [ edge ] += 1 else : edge_to_count [ edge ] = 1 for node in G . nodes_iter (): if node not in G_rv : G_rv . add_node ( node ) for edge , count in edge_to_count . items (): weight = get_edge_weight ( G_rv , edge ) avg_weight = float ( weight ) / count G_rv [ edge [ 0 ]][ edge [ 1 ]][ weight_attr ] = avg_weight return G_rv def mat_to_bipartite ( mat ): G = nx . Graph () row_node_ids = [] col_node_ids = [] n_rows , n_cols = mat . shape for i in range ( n_rows ): node_i = \"r {} \" . format ( i ) row_node_ids . append ( node_i ) G . add_node ( node_i , bipartite = 0 ) for j in range ( n_cols ): node_j = \"c {} \" . format ( j ) col_node_ids . append ( node_j ) G . add_node ( node_j , bipartite = 1 ) for i in range ( n_rows ): for j in range ( n_cols ): G . add_edge ( \"r {} \" . format ( i ), \"c {} \" . format ( j ), { 'weight' : mat [ i , j ]}) return G , row_node_ids , col_node_ids def to_unit_vector ( vec ): rv = None norm = np . linalg . norm ( vec ) if ( norm == 0 ): rv = vec else : rv = vec / norm return rv def nodelists_to_mat ( sub_lists , uni_list ): \"\"\"Represent gene nodelist as a matrix with shape (len(uni_list), len(sub_lists)) Parameters ---------- sub_lists : list of str uni_list : list of str Returns ------- mat : np.array matrix where each column is associated with a nodelist in <sub_lists> and has a 1 in each row where that gene is in the node list TODO ---- For a single pathway, this representation has n_genes_in_pathway \"units of signal\" while the sampled signal has units <= n_genes_in_pathway (because a subset is sampled) \"\"\" node_to_index = {} for i , node in enumerate ( uni_list ): node_to_index [ node ] = i n_pathways = len ( sub_lists ) mat = np . zeros (( len ( uni_list ), n_pathways )) for pathway_ind in range ( len ( sub_lists )): pathway = sub_lists [ pathway_ind ] for node in pathway : index = node_to_index [ node ] if index is None : sys . stderr . write ( \"Unrecognized gene ID: {} \\n \" . format ( node )) else : mat [ index , pathway_ind ] = 1 return mat def normalize_num_pathways ( W_mat , pathways_mat ): \"\"\" Prior to matching latent factors to pathways, this function MAY be invoked to transform W_mat or pathways_mat (whichever is smaller) to make it so the number of latent factors is the same as the number of pathways by adding a binary vector of all zeros (the empty pathway). Parameters ---------- W_mat : np.array array with latent factors as columns and genes as rows with gene loadings into latent factors in cells pathways_mat : np.array array with pathways as columns and genes as rows with a 1 or 0 in each cell indicating whether that gene is present in the pathway or not Returns ------- W_mat : np.array See description pathways_mat : np.array See description \"\"\" # reshape 1D vectors if needed shp = W_mat . shape if ( len ( shp ) == 1 ): W_mat = W_mat . reshape (( shp [ 0 ], 1 )) shp = pathways_mat . shape if ( len ( shp ) == 1 ): pathways_mat = pathways_mat . reshape (( shp [ 0 ], 1 )) n_genes , n_latent_factors = W_mat . shape shp = pathways_mat . shape n_genes2 , n_pathways = pathways_mat . shape if ( n_genes != n_genes2 ): raise ValueError ( \" {} != {} : number of genes identified by W_mat is not the same as those identified by pathways_mat\" . format ( n_genes , n_genes2 )) arrs = [ W_mat , pathways_mat ] small_arr = - 1 large_arr = - 1 diff = 0 if ( n_latent_factors < n_pathways ): small_arr = 0 large_arr = 1 diff = n_pathways - n_latent_factors elif ( n_pathways < n_latent_factors ): small_arr = 1 large_arr = 0 diff = n_latent_factors - n_pathways # else do nothing cols = np . zeros (( n_genes , diff )) arrs [ small_arr ] = np . concatenate (( arrs [ small_arr ], cols ), axis = 1 ) return tuple ( arrs ) def match ( W_mat , pathways_mat ): return match_pr ( W_mat , pathways_mat ) def match_pr ( W_mat , pathways_mat ): n_genes , n_factors = W_mat . shape n_genes2 , n_pathways = pathways_mat . shape if ( n_genes != n_genes2 ): raise ValueError ( \"n_genes != n_genes2 : {} != {} \" . format ( n_genes , n_genes2 )) # prepare max weight matching by AUC aucs = np . zeros (( n_factors , n_pathways )) for i in range ( n_factors ): factor_vec = W_mat [:, i ] for j in range ( n_pathways ): pathway_vec = pathways_mat [:, j ] y_score = factor_vec y_true = pathway_vec auc = sklearn . metrics . average_precision_score ( y_true , y_score ) aucs [ i , j ] = auc # use networkx for max weight matching G , factor_node_ids , pathway_node_ids = mat_to_bipartite ( aucs ) matching = nx . max_weight_matching ( G ) # I don't like the return value from networkx, reorganize data rv = transform_matching ( G , matching , factor_node_ids ) return rv def match_dist ( W_mat , pathways_mat ): \"\"\" Match latent factors to pathways Parameters ---------- W_mat : np.array pathways_mat : np.array Returns ------- rv : list of tpl each tpl is of the form (<latent_factor_id>, <pathway_id>, <distance>) where W_mat latent factors are identified with names \"r0\", \"r1\", etc. (for \"row\" of the distance matrix constructed in this function) associated with each latent factor in W_mat; and pathways are identified with names \"c0\", \"c1\", etc. (for \"column\" of the distance matrix) TODO ---- other type of transformation of distance to similarity? RBF? implement version that uses hypergeometic p-values as distance measure? \"\"\" n_genes , n_factors = W_mat . shape n_genes2 , n_pathways = pathways_mat . shape if ( n_genes != n_genes2 ): raise ValueError ( \"n_genes != n_genes2 : {} != {} \" . format ( n_genes , n_genes2 )) #if(n_factors != n_pathways): # raise ValueError(\"n_factors != n_pathways: {} != {}\".format(n_factors, n_pathways)) dists = np . zeros (( n_factors , n_pathways )) max_unit_vec_dist = math . sqrt ( 2 ) for i in range ( n_factors ): factor_vec = W_mat [:, i ] # scale distance by size of latent factor and pathway factor_vec_unit = to_unit_vector ( factor_vec ) for j in range ( n_pathways ): pathway_vec = pathways_mat [:, j ] pathway_vec_unit = to_unit_vector ( pathway_vec ) dists [ i , j ] = np . linalg . norm ( factor_vec_unit - pathway_vec_unit ) dists_inv = 1 - ( dists / max_unit_vec_dist ) # use networkx for max weight matching G , factor_node_ids , pathway_node_ids = mat_to_bipartite ( dists_inv ) matching = nx . max_weight_matching ( G ) # I don't like the return value from networkx, reorganize data rv = transform_matching ( G , matching , factor_node_ids ) return rv def transform_matching ( G , matching , factor_node_ids ): \"\"\" Transform the return value of a matching from NetworkX into a different data structure Returns ------- rv : list of (factor_id, pathway_id, weight) \"\"\" rv = [] for factor_id in factor_node_ids : # not all nodes used in matching necessarily if factor_id in matching : pathway_id = matching [ factor_id ] weight = G [ factor_id ][ pathway_id ][ 'weight' ] tpl = ( factor_id , pathway_id , weight ) rv . append ( tpl ) else : # TODO temporary: why are they not included? # usually not included due to different numbers of latent factors and pathways weights = [] for neighbor in G . neighbors ( factor_id ): weights . append ( G [ factor_id ][ neighbor ][ 'weight' ]) sys . stderr . write ( \"Factor {} not included in matching; weights: \" . format ( factor_id ) + \" ; \" . join ( map ( str , weights )) + \" \\n \" ) return rv def sample ( G , n , t = 3 , alpha_portion = 0.9 , seed = None , node_order = None ): \"\"\" Sample <n> random values according to a distribution specified by G. Each value is a len(node_order) size vector of positive reals. If a node in G has no parents, its value is drawn according to Gamma(t^alpha_portion, t^(1-alpha_portion)). t is a reparameterization so that t is a location parameter. The default alpha_portion is chosen to be high so that the distribution is less uniform and more peaked around that location. Nodes which are present in <node_order> but not <G> are also have values drawn according to the aforementioned distribution. A node Y which has parents X_1, ..., X_k is specified by Y = \\sum_k Gamma(X_k^alpha_portion, X_k^(1-alpha_portion)) Y also has a Gamma distribution but it is not easily expressed. For Y = \\sum_k Gamma(alpha_k, beta), Y ~ Gamma(\\sum_k alpha_k, beta). The Y here is different but close. \"\"\" loc = 0 # TODO assumes connected if not G . is_directed (): raise FactorLibException ( 'G must be directed' ) if seed is not None : np . random . seed ( seed ) if node_order is None : node_order = G . nodes () m = len ( node_order ) rv = np . zeros (( m , n )) node_to_ind = {} for i , node in enumerate ( node_order ): node_to_ind [ node ] = i roots = [ n for n , d in G . in_degree () . items () if d == 0 ] for j in range ( n ): for root in roots : root_ind = node_to_ind [ root ] alpha = math . pow ( t , alpha_portion ) beta = math . pow ( t , 1 - alpha_portion ) rv [ root_ind , j ] = gamma . rvs ( alpha , loc , beta ) for source , target in nx . dfs_edges ( G , root ): source_ind = node_to_ind [ source ] target_ind = node_to_ind [ target ] alpha = math . pow ( rv [ source_ind , j ], alpha_portion ) beta = math . pow ( rv [ source_ind , j ], 1 - alpha_portion ) rv [ target_ind , j ] += gamma . rvs ( alpha , loc , beta ) other_nodes = set ( node_order ) - set ( G . nodes ()) for j in range ( n ): for node in other_nodes : node_ind = node_to_ind [ node ] alpha = math . pow ( t , alpha_portion ) beta = math . pow ( t , 1 - alpha_portion ) rv [ node_ind , j ] = gamma . rvs ( alpha , loc , beta ) return rv def compute_man_reg ( w , L ): return ( L . dot ( w )) . dot ( w ) def parse_achilles ( fp ): \"\"\" Parse Achilles data from http://portals.broadinstitute.org/achilles/datasets/18/download Parameters ---------- fp : str filepath to 'gene_dependency' file mentioned in Achilles README \"\"\" data = None row_to_cell_line = [] gene_names = None with open ( fp ) as fh : for line in fh : line = line . rstrip () gene_names = line . split ( ',' )[ 1 :] break for line in fh : ind = line . index ( ',' ) cell_line = line [ 0 : ind ] row_to_cell_line . append ( cell_line ) data = np . genfromtxt ( fp , delimiter = \",\" , skip_header = 1 ) data = data [:, 1 :] gene_names = achilles_to_hugo ( gene_names ) return data , row_to_cell_line , gene_names def achilles_to_hugo ( gene_names ): \"\"\" Transform gene identifiers from \"HUGO (Entrez)\" to \"HUGO\" \"\"\" regexp = re . compile ( r '([\\w-]+)\\s\\(\\w+\\)' ) rv = [] for i , gene_name in enumerate ( gene_names ): match_data = regexp . match ( gene_name ) if match_data is not None : rv . append ( match_data . group ( 1 )) else : raise FactorLibException ( \"Gene name {} at position {} does not match the specified format\" . format ( gene_name , i )) return rv def filter_vec_by_graph ( G , vec , nodelist ): \"\"\" Return a subset of vec and nodelist corresponding to the nodes defined in G \"\"\" inds = [] nodelist_sub = [] for i , node in enumerate ( nodelist ): if node in G : inds . append ( i ) nodelist_sub . append ( node ) vec_sub = vec [ inds ] return vec_sub , nodelist_sub def measure_cv_performance ( gene_by_latent_train , data_test ): \"\"\" Measure NMF model performance on held out data. Performance is evaluated based on the model's ability to reconstuct held out samples. \\hat{u} := arg min_{u} || x - uV^T || s.t. u >= 0 \\hat{x} := \\hat{u} V^T error = || x - \\hat{x} || normalized_error = error / ||x|| Parameters ---------- gene_by_latent_train : np.array V in the above equations data_test : np.array X in the above equations Returns ------- error : np.array normalized error for each sample \"\"\" m_samples , n_genes = data_test . shape error = np . zeros ( m_samples ,) # TODO multi-sample version of nnls? for m in range ( m_samples ): u_hat , err = scipy . optimize . nnls ( gene_by_latent_train , data_test [ m ,:]) error [ m ] = err / np . linalg . norm ( data_test [ m ,:]) return error","title":"Module prmf"},{"location":"reference/prmf/#sub-modules","text":"prmf.ampl prmf.ensembl prmf.image_processing prmf.intersect prmf.plot prmf.prmf_args prmf.script_utils prmf.string_db","title":"Sub-modules"},{"location":"reference/prmf/#functions","text":"","title":"Functions"},{"location":"reference/prmf/#achilles_to_hugo","text":"def achilles_to_hugo ( gene_names ) Transform gene identifiers from \"HUGO (Entrez)\" to \"HUGO\" View Source def achilles_to_hugo ( gene_names ): \"\"\" Transform gene identifiers from \" HUGO ( Entrez ) \" to \" HUGO \" \"\"\" regexp = re . compile ( r '([\\w-]+)\\s\\(\\w+\\)' ) rv = [] for i , gene_name in enumerate ( gene_names ): match_data = regexp . match ( gene_name ) if match_data is not None : rv . append ( match_data . group ( 1 )) else : raise FactorLibException ( \"Gene name {} at position {} does not match the specified format\" . format ( gene_name , i )) return rv","title":"achilles_to_hugo"},{"location":"reference/prmf/#combine_graphs","text":"def combine_graphs ( Gs , weights ) Combine graphs into a single graph by constructing a weighted sum of their edge weights G_1 := (V_1, E_1) G_2 := (V_2, E_2) where E_i is a set of tuples (u,v,w) G := (V_1 \\cup V_2, E) TODO View Source def combine_graphs ( Gs , weights ) : \"\"\" Combine graphs into a single graph by constructing a weighted sum of their edge weights G_1 := (V_1, E_1) G_2 := (V_2, E_2) where E_i is a set of tuples (u,v,w) G := (V_1 \\cup V_2, E) TODO \"\"\" def get_edge_weight ( G , u , v ) : # assumes edge is already known to be present in G weight = G [ u ][ v ] . get ( 'weight' ) if weight is None : weight = 1 return weight total_weight = 0.0 for weight in weights : total_weight += weight G_rv = nx . Graph () for i , G in enumerate ( Gs ) : graph_weight = weights [ i ] for edge in G . edges_iter () : weight = get_edge_weight ( G , * edge ) if G_rv . has_edge ( * edge ) : weight_rv = G_rv [ edge[0 ] ] [ edge[1 ] ] [ 'weight' ] G_rv [ edge[0 ] ] [ edge[1 ] ] [ 'weight' ] = weight_rv + graph_weight * weight else : G_rv . add_edge ( edge [ 0 ] , edge [ 1 ] , { 'weight' : graph_weight * weight } ) return G_rv","title":"combine_graphs"},{"location":"reference/prmf/#compute_man_reg","text":"def compute_man_reg ( w , L ) View Source def compute_man_reg ( w , L ): return ( L . dot ( w )). dot ( w )","title":"compute_man_reg"},{"location":"reference/prmf/#diffusion","text":"def diffusion ( M , adj , alpha = 0.7 , tol = 1e-05 ) Network propagation iterative process Iterative algorithm for apply propagation using random walk on a network: Initialize:: X1 = M Repeat :: X2 = alpha * X1 . A + ( 1 - alpha ) * M X1 = X2 Until :: norm ( X2 - X1 ) < tol Where :: A : degree - normalized adjacency matrix","title":"diffusion"},{"location":"reference/prmf/#parameters","text":"M : sparse matrix Data matrix to be diffused. adj : sparse matrix Adjacency matrice. alpha : float, default: 0.7 Diffusion/propagation factor with 0 <= alpha <= 1. For alpha = 0 : no diffusion. For alpha = 1 : tol : float, default: 10e-6 Convergence threshold.","title":"Parameters"},{"location":"reference/prmf/#returns","text":"X2 : sparse matrix Smoothed matrix.","title":"Returns"},{"location":"reference/prmf/#notes","text":"Copied from the stratipy Python library View Source def diffusion ( M , adj , alpha = 0 . 7 , tol = 10 e - 6 ): # TODO equation , M , alpha \"\"\"Network propagation iterative process Iterative algorithm for apply propagation using random walk on a network: Initialize:: X1 = M Repeat:: X2 = alpha * X1.A + (1-alpha) * M X1 = X2 Until:: norm(X2-X1) < tol Where:: A : degree-normalized adjacency matrix Parameters ---------- M : sparse matrix Data matrix to be diffused. adj : sparse matrix Adjacency matrice. alpha : float, default: 0.7 Diffusion/propagation factor with 0 <= alpha <= 1. For alpha = 0 : no diffusion. For alpha = 1 : tol : float, default: 10e-6 Convergence threshold. Returns ------- X2 : sparse matrix Smoothed matrix. Notes ----- Copied from the stratipy Python library \"\"\" n = adj . shape [ 0 ] adj = adj + sp . eye ( n ) d = sp . dia_matrix (( np . array ( adj . sum ( axis = 0 )) **- 1 , [ 0 ]), shape = ( n , n )) A = adj . dot ( d ) X1 = M X2 = alpha * X1 . dot ( A ) + ( 1 - alpha ) * M i = 0 while norm ( X2 - X1 ) > tol : X1 = X2 X2 = alpha * X1 . dot ( A ) + ( 1 - alpha ) * M i += 1 return X2","title":"Notes"},{"location":"reference/prmf/#embed_arr","text":"def embed_arr ( all_col_names , some_col_names , arr ) Embed with len(some_col_names) number of columns in a larger array with len(all_col_names) number of columns. The larger array is constructed and returned. View Source def embed_arr ( all_col_names , some_col_names , arr ) : \"\"\" Embed <arr> with len(some_col_names) number of columns in a larger array with len(all_col_names) number of columns. The larger array is constructed and returned. \"\"\" m , n = arr . shape if len ( some_col_names ) != n : raise FactorLibException ( 'some_col_names != #columns of arr: {} != {}' . format ( some_col_names , n )) n2 = len ( all_col_names ) rv = np . zeros (( m , n2 )) all_name_to_ind = {} for i , name in enumerate ( all_col_names ) : all_name_to_ind [ name ] = i for i in range ( m ) : for j , name in enumerate ( some_col_names ) : j2 = all_name_to_ind [ name ] rv [ i,j2 ] = arr [ i,j ] return rv","title":"embed_arr"},{"location":"reference/prmf/#embed_ids","text":"def embed_ids ( all_ids , ids ) Construct a binary vector of length len( ) with a 1 in the positions associated with each id in View Source def embed_ids ( all_ids , ids ) : \"\"\" Construct a binary vector of length len(<all_ids>) with a 1 in the positions associated with each id in <ids> \"\"\" row_vec = np . zeros ( len ( all_ids )) missing = [] for id in ids : try : index = all_ids . index ( id ) row_vec [ index ] = 1 except ValueError : missing . append ( id ) row_vec_sparse = sp . csc_matrix ( row_vec ) return ( row_vec_sparse , missing )","title":"embed_ids"},{"location":"reference/prmf/#filter_vec_by_graph","text":"def filter_vec_by_graph ( G , vec , nodelist ) Return a subset of vec and nodelist corresponding to the nodes defined in G View Source def filter_vec_by_graph ( G , vec , nodelist ) : \"\"\" Return a subset of vec and nodelist corresponding to the nodes defined in G \"\"\" inds = [] nodelist_sub = [] for i , node in enumerate ( nodelist ) : if node in G : inds . append ( i ) nodelist_sub . append ( node ) vec_sub = vec [ inds ] return vec_sub , nodelist_sub","title":"filter_vec_by_graph"},{"location":"reference/prmf/#get_adj_mat","text":"def get_adj_mat ( G ) Represent ppi network as adjacency matrix","title":"get_adj_mat"},{"location":"reference/prmf/#parameters_1","text":"G : networkx graph ppi network, see get_ppi()","title":"Parameters"},{"location":"reference/prmf/#returns_1","text":"adj : square sparse scipy matrix (i,j) has a 1 if there is an interaction reported by irefindex ids : list same length as adj, ith index contains irefindex unique identifier for gene whose interactions are reported in the ith row of adj View Source def get_adj_mat ( G ): \"\"\"Represent ppi network as adjacency matrix Parameters ---------- G : networkx graph ppi network, see get_ppi() Returns ------- adj : square sparse scipy matrix (i,j) has a 1 if there is an interaction reported by irefindex ids : list same length as adj, ith index contains irefindex unique identifier for gene whose interactions are reported in the ith row of adj \"\"\" ids = G . nodes () adj = nx . to_scipy_sparse_matrix ( G , nodelist = ids , dtype = bool ) return adj , ids","title":"Returns"},{"location":"reference/prmf/#mat_to_bipartite","text":"def mat_to_bipartite ( mat ) View Source def mat_to_bipartite ( mat ): G = nx . Graph () row_node_ids = [] col_node_ids = [] n_rows , n_cols = mat . shape for i in range ( n_rows ): node_i = \"r{}\" . format ( i ) row_node_ids . append ( node_i ) G . add_node ( node_i , bipartite = 0 ) for j in range ( n_cols ): node_j = \"c{}\" . format ( j ) col_node_ids . append ( node_j ) G . add_node ( node_j , bipartite = 1 ) for i in range ( n_rows ): for j in range ( n_cols ): G . add_edge ( \"r{}\" . format ( i ), \"c{}\" . format ( j ), { 'weight' : mat [ i , j ] } ) return G , row_node_ids , col_node_ids","title":"mat_to_bipartite"},{"location":"reference/prmf/#match","text":"def match ( W_mat , pathways_mat ) View Source def match ( W_mat , pathways_mat ): return match_pr ( W_mat , pathways_mat )","title":"match"},{"location":"reference/prmf/#match_dist","text":"def match_dist ( W_mat , pathways_mat ) Match latent factors to pathways","title":"match_dist"},{"location":"reference/prmf/#parameters_2","text":"W_mat : np.array pathways_mat : np.array","title":"Parameters"},{"location":"reference/prmf/#returns_2","text":"rv : list of tpl each tpl is of the form ( , , ) where W_mat latent factors are identified with names \"r0\", \"r1\", etc. (for \"row\" of the distance matrix constructed in this function) associated with each latent factor in W_mat; and pathways are identified with names \"c0\", \"c1\", etc. (for \"column\" of the distance matrix)","title":"Returns"},{"location":"reference/prmf/#todo","text":"other type of transformation of distance to similarity? RBF? implement version that uses hypergeometic p-values as distance measure? View Source def match_dist ( W_mat , pathways_mat ): \"\"\" Match latent factors to pathways Parameters ---------- W_mat : np.array pathways_mat : np.array Returns ------- rv : list of tpl each tpl is of the form (<latent_factor_id>, <pathway_id>, <distance>) where W_mat latent factors are identified with names \" r0 \", \" r1 \", etc. (for \" row \" of the distance matrix constructed in this function) associated with each latent factor in W_mat; and pathways are identified with names \" c0 \", \" c1 \", etc. (for \" column \" of the distance matrix) TODO ---- other type of transformation of distance to similarity? RBF? implement version that uses hypergeometic p-values as distance measure? \"\"\" n_genes , n_factors = W_mat . shape n_genes2 , n_pathways = pathways_mat . shape if ( n_genes != n_genes2 ): raise ValueError ( \"n_genes != n_genes2 : {} != {}\" . format ( n_genes , n_genes2 )) # if ( n_factors != n_pathways ): # raise ValueError ( \"n_factors != n_pathways: {} != {}\" . format ( n_factors , n_pathways )) dists = np . zeros (( n_factors , n_pathways )) max_unit_vec_dist = math . sqrt ( 2 ) for i in range ( n_factors ): factor_vec = W_mat [:, i ] # scale distance by size of latent factor and pathway factor_vec_unit = to_unit_vector ( factor_vec ) for j in range ( n_pathways ): pathway_vec = pathways_mat [:, j ] pathway_vec_unit = to_unit_vector ( pathway_vec ) dists [ i , j ] = np . linalg . norm ( factor_vec_unit - pathway_vec_unit ) dists_inv = 1 - ( dists / max_unit_vec_dist ) # use networkx for max weight matching G , factor_node_ids , pathway_node_ids = mat_to_bipartite ( dists_inv ) matching = nx . max_weight_matching ( G ) # I don ' t like the return value from networkx , reorganize data rv = transform_matching ( G , matching , factor_node_ids ) return rv","title":"TODO"},{"location":"reference/prmf/#match_pr","text":"def match_pr ( W_mat , pathways_mat ) View Source def match_pr ( W_mat , pathways_mat ): n_genes , n_factors = W_mat . shape n_genes2 , n_pathways = pathways_mat . shape if ( n_genes != n_genes2 ): raise ValueError ( \"n_genes != n_genes2 : {} != {}\" . format ( n_genes , n_genes2 )) # prepare max weight matching by AUC aucs = np . zeros (( n_factors , n_pathways )) for i in range ( n_factors ): factor_vec = W_mat [:, i ] for j in range ( n_pathways ): pathway_vec = pathways_mat [:, j ] y_score = factor_vec y_true = pathway_vec auc = sklearn . metrics . average_precision_score ( y_true , y_score ) aucs [ i , j ] = auc # use networkx for max weight matching G , factor_node_ids , pathway_node_ids = mat_to_bipartite ( aucs ) matching = nx . max_weight_matching ( G ) # I don ' t like the return value from networkx , reorganize data rv = transform_matching ( G , matching , factor_node_ids ) return rv","title":"match_pr"},{"location":"reference/prmf/#measure_cv_performance","text":"def measure_cv_performance ( gene_by_latent_train , data_test ) Measure NMF model performance on held out data. Performance is evaluated based on the model's ability to reconstuct held out samples. \\hat{u} := arg min_{u} || x - uV^T || s.t. u >= 0 \\hat{x} := \\hat{u} V^T error = || x - \\hat{x} || normalized_error = error / ||x||","title":"measure_cv_performance"},{"location":"reference/prmf/#parameters_3","text":"gene_by_latent_train : np.array V in the above equations data_test : np.array X in the above equations","title":"Parameters"},{"location":"reference/prmf/#returns_3","text":"error : np.array normalized error for each sample View Source def measure_cv_performance ( gene_by_latent_train , data_test ) : \"\"\" Measure NMF model performance on held out data. Performance is evaluated based on the model's ability to reconstuct held out samples. \\hat{u} := arg min_{u} || x - uV^T || s.t. u >= 0 \\hat{x} := \\hat{u} V^T error = || x - \\hat{x} || normalized_error = error / ||x|| Parameters ---------- gene_by_latent_train : np.array V in the above equations data_test : np.array X in the above equations Returns ------- error : np.array normalized error for each sample \"\"\" m_samples , n_genes = data_test . shape error = np . zeros ( m_samples ,) # TODO multi - sample version of nnls ? for m in range ( m_samples ) : u_hat , err = scipy . optimize . nnls ( gene_by_latent_train , data_test [ m,: ] ) error [ m ] = err / np . linalg . norm ( data_test [ m,: ] ) return error","title":"Returns"},{"location":"reference/prmf/#nodelists_to_mat","text":"def nodelists_to_mat ( sub_lists , uni_list ) Represent gene nodelist as a matrix with shape (len(uni_list), len(sub_lists))","title":"nodelists_to_mat"},{"location":"reference/prmf/#parameters_4","text":"sub_lists : list of str uni_list : list of str","title":"Parameters"},{"location":"reference/prmf/#returns_4","text":"mat : np.array matrix where each column is associated with a nodelist in and has a 1 in each row where that gene is in the node list","title":"Returns"},{"location":"reference/prmf/#todo_1","text":"For a single pathway, this representation has n_genes_in_pathway \"units of signal\" while the sampled signal has units <= n_genes_in_pathway (because a subset is sampled) View Source def nodelists_to_mat ( sub_lists , uni_list ) : \"\"\"Represent gene nodelist as a matrix with shape (len(uni_list), len(sub_lists)) Parameters ---------- sub_lists : list of str uni_list : list of str Returns ------- mat : np.array matrix where each column is associated with a nodelist in <sub_lists> and has a 1 in each row where that gene is in the node list TODO ---- For a single pathway, this representation has n_genes_in_pathway \" units of signal \" while the sampled signal has units <= n_genes_in_pathway (because a subset is sampled) \"\"\" node_to_index = {} for i , node in enumerate ( uni_list ) : node_to_index [ node ] = i n_pathways = len ( sub_lists ) mat = np . zeros (( len ( uni_list ), n_pathways )) for pathway_ind in range ( len ( sub_lists )) : pathway = sub_lists [ pathway_ind ] for node in pathway : index = node_to_index [ node ] if index is None : sys . stderr . write ( \"Unrecognized gene ID: {}\\n\" . format ( node )) else : mat [ index,pathway_ind ] = 1 return mat","title":"TODO"},{"location":"reference/prmf/#normalize_num_pathways","text":"def normalize_num_pathways ( W_mat , pathways_mat ) Prior to matching latent factors to pathways, this function MAY be invoked to transform W_mat or pathways_mat (whichever is smaller) to make it so the number of latent factors is the same as the number of pathways by adding a binary vector of all zeros (the empty pathway).","title":"normalize_num_pathways"},{"location":"reference/prmf/#parameters_5","text":"W_mat : np.array array with latent factors as columns and genes as rows with gene loadings into latent factors in cells pathways_mat : np.array array with pathways as columns and genes as rows with a 1 or 0 in each cell indicating whether that gene is present in the pathway or not","title":"Parameters"},{"location":"reference/prmf/#returns_5","text":"W_mat : np.array See description pathways_mat : np.array See description View Source def normalize_num_pathways ( W_mat , pathways_mat ) : \"\"\" Prior to matching latent factors to pathways, this function MAY be invoked to transform W_mat or pathways_mat (whichever is smaller) to make it so the number of latent factors is the same as the number of pathways by adding a binary vector of all zeros (the empty pathway). Parameters ---------- W_mat : np.array array with latent factors as columns and genes as rows with gene loadings into latent factors in cells pathways_mat : np.array array with pathways as columns and genes as rows with a 1 or 0 in each cell indicating whether that gene is present in the pathway or not Returns ------- W_mat : np.array See description pathways_mat : np.array See description \"\"\" # reshape 1 D vectors if needed shp = W_mat . shape if ( len ( shp ) == 1 ) : W_mat = W_mat . reshape (( shp [ 0 ] , 1 )) shp = pathways_mat . shape if ( len ( shp ) == 1 ) : pathways_mat = pathways_mat . reshape (( shp [ 0 ] , 1 )) n_genes , n_latent_factors = W_mat . shape shp = pathways_mat . shape n_genes2 , n_pathways = pathways_mat . shape if ( n_genes != n_genes2 ) : raise ValueError ( \"{} != {}: number of genes identified by W_mat is not the same as those identified by pathways_mat\" . format ( n_genes , n_genes2 )) arrs = [ W_mat, pathways_mat ] small_arr = - 1 large_arr = - 1 diff = 0 if ( n_latent_factors < n_pathways ) : small_arr = 0 large_arr = 1 diff = n_pathways - n_latent_factors elif ( n_pathways < n_latent_factors ) : small_arr = 1 large_arr = 0 diff = n_latent_factors - n_pathways # else do nothing cols = np . zeros (( n_genes , diff )) arrs [ small_arr ] = np . concatenate (( arrs [ small_arr ] , cols ), axis = 1 ) return tuple ( arrs )","title":"Returns"},{"location":"reference/prmf/#parse_achilles","text":"def parse_achilles ( fp ) Parse Achilles data from http://portals.broadinstitute.org/achilles/datasets/18/download","title":"parse_achilles"},{"location":"reference/prmf/#parameters_6","text":"fp : str filepath to 'gene_dependency' file mentioned in Achilles README View Source def parse_achilles ( fp ): \"\"\" Parse Achilles data from http://portals.broadinstitute.org/achilles/datasets/18/download Parameters ---------- fp : str filepath to 'gene_dependency' file mentioned in Achilles README \"\"\" data = None row_to_cell_line = [] gene_names = None with open ( fp ) as fh : for line in fh : line = line . rstrip () gene_names = line . split ( ',' )[ 1 :] break for line in fh : ind = line . index ( ',' ) cell_line = line [ 0 : ind ] row_to_cell_line . append ( cell_line ) data = np . genfromtxt ( fp , delimiter = \",\" , skip_header = 1 ) data = data [:, 1 :] gene_names = achilles_to_hugo ( gene_names ) return data , row_to_cell_line , gene_names","title":"Parameters"},{"location":"reference/prmf/#parse_gene_lists","text":"def parse_gene_lists ( nodelist , gene_list_fps ) Parse gene lists into a sparse scipy array","title":"parse_gene_lists"},{"location":"reference/prmf/#parameters_7","text":"nodelist : list of str gene identifiers that define the assignment of array indexes to genes gene_lists : list of","title":"Parameters"},{"location":"reference/prmf/#returns_6","text":"mat : sparse csc matrix View Source def parse_gene_lists ( nodelist , gene_list_fps ): \"\"\" Parse gene lists into a sparse scipy array Parameters ---------- nodelist : list of str gene identifiers that define the assignment of array indexes to genes gene_lists : list of Returns ------- mat : sparse csc matrix \"\"\" # parse gene lists gene_lists = [] for gene_path in gene_list_fps : with open ( gene_path ) as fh : gene_lists . append ( parse_ws_delim ( fh )) # verify gene lists present in ppi_db def get_row_vec_for_gene_list ( gene_list ): row_vec , missing = embed_ids ( nodelist , gene_list ) sys . stderr . write ( \"missing {}/{} node identifiers: {}\\n\" . format ( len ( missing ), len ( gene_list ), \", \" . join ( missing ))) return row_vec row_vecs = map ( get_row_vec_for_gene_list , gene_lists ) mat = sp . vstack ( row_vecs ) return mat","title":"Returns"},{"location":"reference/prmf/#parse_init","text":"def parse_init ( fp ) Extract filepaths used to initialize nmf_pathway.py as reported by its output stored in View Source def parse_init ( fp ): \"\"\" Extract filepaths used to initialize nmf_pathway.py as reported by its output stored in <fp> \"\"\" rv = [] with open ( fp , 'r' ) as fh : for line in fh : line = line . rstrip () rv . append ( line ) return rv","title":"parse_init"},{"location":"reference/prmf/#parse_nodelist","text":"def parse_nodelist ( fh ) Return a list of node identifiers which maps the list index to the identifier View Source def parse_nodelist ( fh ): \"\"\" Return a list of node identifiers which maps the list index to the identifier \"\"\" rv = [] for line in fh : line = line . rstrip () words = line . split () for word in words : rv . append ( word ) return rv","title":"parse_nodelist"},{"location":"reference/prmf/#parse_pathway_obj","text":"def parse_pathway_obj ( fp ) Extract mapping of latent factor to pathway fp View Source def parse_pathway_obj ( fp ): \"\"\" Extract mapping of latent factor to pathway fp \"\"\" rv = {} regexp = re . compile ( r '(\\d+)\\s*->\\s*(.+)' ) with open ( fp ) as fh : for line in fh : line = line . rstrip () match_data = regexp . match ( line ) if match_data is not None : rv [ int ( match_data . group ( 1 ))] = match_data . group ( 2 ) return rv","title":"parse_pathway_obj"},{"location":"reference/prmf/#parse_pathways","text":"def parse_pathways ( graphml_fps ) View Source def parse_pathways ( graphml_fps ): Gs = list ( map ( lambda x : nx . read_graphml ( x ), graphml_fps )) return Gs","title":"parse_pathways"},{"location":"reference/prmf/#parse_pathways_dir","text":"def parse_pathways_dir ( pathways_dir ) Parse .graphml files in","title":"parse_pathways_dir"},{"location":"reference/prmf/#returns_7","text":"Gs : list of nx.DiGraph parsed pathways in View Source def parse_pathways_dir ( pathways_dir ): \"\"\" Parse .graphml files in <pathways_dir> Returns ------- Gs : list of nx.DiGraph parsed pathways in <pathways_dir> \"\"\" graphmls = [] for fname in os . listdir ( pathways_dir ): basename , ext = os . path . splitext ( fname ) if ext == \".graphml\" : graphmls . append ( os . path . join ( pathways_dir , fname )) return parse_pathways ( graphmls )","title":"Returns"},{"location":"reference/prmf/#parse_seedlist","text":"def parse_seedlist ( fp , filetype = None , node_attr = 'name' ) Parse the file with filepath as a seed list: a list of node identifier strings.","title":"parse_seedlist"},{"location":"reference/prmf/#parameters_8","text":"fp : str a file containing gene identifiers filetype : str, None the file type, informs parsing if None, determined by the extension of fp View Source def parse_seedlist ( fp , filetype = None , node_attr = 'name' ): \"\"\" Parse the file with filepath <fp> as a seed list: a list of node identifier strings. Parameters ---------- fp : str a file containing gene identifiers filetype : str, None the file type, informs parsing if None, determined by the extension of fp \"\"\" if filetype is None : fp_no_ext , ext = os . path . splitext ( fp ) if ext == '.graphml' : filetype = 'graphml' elif ext == '.txt' : filetype = 'txt' if filetype is None : raise FactorLibException ( \"Cannot determine filetype\" ) seedlist = None if filetype == 'graphml' : G = nx . read_graphml ( fp ) G = relabel_nodes ( G , node_attr ) seedlist = G . nodes () elif filetype == 'txt' : seedlist = [] with open ( fp , 'r' ) as fh : for line in fh : line = line . rstrip () seedlist . append ( line ) return seedlist","title":"Parameters"},{"location":"reference/prmf/#parse_ws_delim","text":"def parse_ws_delim ( fh ) View Source def parse_ws_delim ( fh ): return parse_nodelist ( fh )","title":"parse_ws_delim"},{"location":"reference/prmf/#prepare_nodelist","text":"def prepare_nodelist ( lists ) Combine lists of nodes into a single master list for use by nmf_pathway and other functions that accept a nodelist as input View Source def prepare_nodelist ( lists ): \"\"\" Combine lists of nodes into a single master list for use by nmf_pathway and other functions that accept a nodelist as input \"\"\" set_all = set () for ll in lists : for item in ll : set_all . add ( item ) rv = sorted ( set_all ) return rv","title":"prepare_nodelist"},{"location":"reference/prmf/#rbf_similarity","text":"def rbf_similarity ( stdev , x , y ) View Source def rbf_similarity ( stdev , x , y ): return math . exp ( - ( np . abs ( x - y ) / stdev ) ** 2 )","title":"rbf_similarity"},{"location":"reference/prmf/#relabel_nodes","text":"def relabel_nodes ( G , node_attribute ) Wrapper around nx.relabel_nodes for scripts: relabel nodes using the View Source def relabel_nodes ( G , node_attribute ) : \"\"\" Wrapper around nx.relabel_nodes for scripts: relabel nodes using the <node_attribute> \"\"\" if node_attribute is not None : mapping = {} for node in G . nodes () : node_attrs = G . node [ node ] if node_attribute in node_attrs : mapping [ node ] = node_attrs [ node_attribute ] G = nx . relabel_nodes ( G , mapping ) return G","title":"relabel_nodes"},{"location":"reference/prmf/#sample","text":"def sample ( G , n , t = 3 , alpha_portion = 0.9 , seed = None , node_order = None ) Sample random values according to a distribution specified by G. Each value is a len(node_order) size vector of positive reals. If a node in G has no parents, its value is drawn according to Gamma(t^alpha_portion, t^(1-alpha_portion)). t is a reparameterization so that t is a location parameter. The default alpha_portion is chosen to be high so that the distribution is less uniform and more peaked around that location. Nodes which are present in but not are also have values drawn according to the aforementioned distribution. A node Y which has parents X_1, ..., X_k is specified by Y = \\sum_k Gamma(X_k^alpha_portion, X_k^(1-alpha_portion)) Y also has a Gamma distribution but it is not easily expressed. For Y = \\sum_k Gamma(alpha_k, beta), Y ~ Gamma(\\sum_k alpha_k, beta). The Y here is different but close. View Source def sample ( G , n , t = 3 , alpha_portion = 0.9 , seed = None , node_order = None ) : \"\"\" Sample <n> random values according to a distribution specified by G. Each value is a len(node_order) size vector of positive reals. If a node in G has no parents, its value is drawn according to Gamma(t^alpha_portion, t^(1-alpha_portion)). t is a reparameterization so that t is a location parameter. The default alpha_portion is chosen to be high so that the distribution is less uniform and more peaked around that location. Nodes which are present in <node_order> but not <G> are also have values drawn according to the aforementioned distribution. A node Y which has parents X_1, ..., X_k is specified by Y = \\sum_k Gamma(X_k^alpha_portion, X_k^(1-alpha_portion)) Y also has a Gamma distribution but it is not easily expressed. For Y = \\sum_k Gamma(alpha_k, beta), Y ~ Gamma(\\sum_k alpha_k, beta). The Y here is different but close. \"\"\" loc = 0 # TODO assumes connected if not G . is_directed () : raise FactorLibException ( 'G must be directed' ) if seed is not None : np . random . seed ( seed ) if node_order is None : node_order = G . nodes () m = len ( node_order ) rv = np . zeros (( m , n )) node_to_ind = {} for i , node in enumerate ( node_order ) : node_to_ind [ node ] = i roots = [ n for n,d in G.in_degree().items() if d==0 ] for j in range ( n ) : for root in roots : root_ind = node_to_ind [ root ] alpha = math . pow ( t , alpha_portion ) beta = math . pow ( t , 1 - alpha_portion ) rv [ root_ind,j ] = gamma . rvs ( alpha , loc , beta ) for source , target in nx . dfs_edges ( G , root ) : source_ind = node_to_ind [ source ] target_ind = node_to_ind [ target ] alpha = math . pow ( rv [ source_ind,j ] , alpha_portion ) beta = math . pow ( rv [ source_ind,j ] , 1 - alpha_portion ) rv [ target_ind,j ] += gamma . rvs ( alpha , loc , beta ) other_nodes = set ( node_order ) - set ( G . nodes ()) for j in range ( n ) : for node in other_nodes : node_ind = node_to_ind [ node ] alpha = math . pow ( t , alpha_portion ) beta = math . pow ( t , 1 - alpha_portion ) rv [ node_ind, j ] = gamma . rvs ( alpha , loc , beta ) return rv","title":"sample"},{"location":"reference/prmf/#to_unit_vector","text":"def to_unit_vector ( vec ) View Source def to_unit_vector ( vec ): rv = None norm = np . linalg . norm ( vec ) if ( norm == 0 ): rv = vec else : rv = vec / norm return rv","title":"to_unit_vector"},{"location":"reference/prmf/#transform_matching","text":"def transform_matching ( G , matching , factor_node_ids ) Transform the return value of a matching from NetworkX into a different data structure","title":"transform_matching"},{"location":"reference/prmf/#returns_8","text":"rv : list of (factor_id, pathway_id, weight) View Source def transform_matching ( G , matching , factor_node_ids ) : \"\"\" Transform the return value of a matching from NetworkX into a different data structure Returns ------- rv : list of (factor_id, pathway_id, weight) \"\"\" rv = [] for factor_id in factor_node_ids : # not all nodes used in matching necessarily if factor_id in matching : pathway_id = matching [ factor_id ] weight = G [ factor_id ][ pathway_id ][ 'weight' ] tpl = ( factor_id , pathway_id , weight ) rv . append ( tpl ) else : # TODO temporary : why are they not included ? # usually not included due to different numbers of latent factors and pathways weights = [] for neighbor in G . neighbors ( factor_id ) : weights . append ( G [ factor_id ][ neighbor ][ 'weight' ] ) sys . stderr . write ( \"Factor {} not included in matching; weights: \" . format ( factor_id ) + \" ; \" . join ( map ( str , weights )) + \"\\n\" ) return rv","title":"Returns"},{"location":"reference/prmf/#transform_nearest_neighbors","text":"def transform_nearest_neighbors ( G , k = 10 , attr = 'weight' , attr_type = 'similarity' ) Remove edges in G. For each node, keep at most 10 of its neighbors. Kept neighbors have the highest 'weight' edge attribute. Returns a copy of G and does not modify G.","title":"transform_nearest_neighbors"},{"location":"reference/prmf/#parameters_9","text":"G : nx.Graph k : int default 10 attr : str default 'weight' attr_type : str 'similarity': nearest neighbors are those that are most similar: keep neighbors with highest edge weight 'distance': nearest neighbers are those that are least distant from each other: keep neighbors with least edge weight","title":"Parameters"},{"location":"reference/prmf/#returns_9","text":"H : nx.Graph k-nearest neighbor graph note that a vertex v may have more than k edges incident to it if its k'>k neighbor has v in its k-nearest neighbors View Source def transform_nearest_neighbors ( G , k = 10 , attr = 'weight' , attr_type = 'similarity' ) : \"\"\" Remove edges in G. For each node, keep at most 10 of its neighbors. Kept neighbors have the highest 'weight' edge attribute. Returns a copy of G and does not modify G. Parameters ---------- G : nx.Graph k : int default 10 attr : str default 'weight' attr_type : str 'similarity': nearest neighbors are those that are most similar: keep neighbors with highest edge weight 'distance': nearest neighbers are those that are least distant from each other: keep neighbors with least edge weight Returns ----- H : nx.Graph k-nearest neighbor graph note that a vertex v may have more than k edges incident to it if its k'>k neighbor has v in its k-nearest neighbors \"\"\" attr_types = [ 'similarity', 'distance' ] if ( attr_type not in attr_types ) : raise ArgumentError ( \"attr_type: {} should be one of {}\" . format ( attr_type , \",\" . join ( attr_types ))) H = nx . Graph () # TODO assumes undirected for node in G . nodes_iter () : inc_edges = G . edges ( [ node ] ) edge_weight_pairs = [] for edge in inc_edges : weight = G [ edge[0 ] ] [ edge[1 ] ] [ attr ] pair = ( edge , weight ) edge_weight_pairs . append ( pair ) if ( attr_type == 'similarity' ) : edge_weight_pairs = sorted ( edge_weight_pairs , key = lambda x : x [ 1 ] , reverse = True ) else : edge_weight_pairs = sorted ( edge_weight_pairs , key = lambda x : x [ 1 ] ) for pair in edge_weight_pairs [ :k ] : edge = pair [ 0 ] weight = pair [ 1 ] H . add_edge ( edge [ 0 ] , edge [ 1 ] , { 'weight' : weight } ) return H","title":"Returns"},{"location":"reference/prmf/#weighted_union","text":"def weighted_union ( Gs , weight_attr = 'weight' ) Construct a weighted graph that is the union of graphs . For each edge in a network in , use as the edge weight or 1 if that attribute is not present. Each edge in G_rv has its set to the average weight of that the edge appears in. That is, networks without the edge do not pull down the average toward 0.","title":"weighted_union"},{"location":"reference/prmf/#parameters_10","text":"Gs : iterable of nx.Graph weight_attr : str name of edge attribute to use as weight","title":"Parameters"},{"location":"reference/prmf/#returns_10","text":"G_rv : nx.Graph View Source def weighted_union ( Gs , weight_attr = 'weight' ) : \"\"\" Construct a weighted graph that is the union of graphs <Gs>. For each edge in a network in <Gs>, use <weight_attr> as the edge weight or 1 if that attribute is not present. Each edge in G_rv has its <weight_attr> set to the average weight of <Gs> that the edge appears in. That is, networks without the edge do not pull down the average toward 0. Parameters ---------- Gs : iterable of nx.Graph weight_attr : str name of edge attribute to use as weight Returns ------- G_rv : nx.Graph \"\"\" G_rv = nx . Graph () edge_to_count = {} def get_edge_weight ( G , edge ) : weight = G [ edge[0 ] ] [ edge[1 ] ] . get ( weight_attr ) if weight is None : weight = 1 return weight for G in Gs : for edge in G . edges_iter () : weight = get_edge_weight ( G , edge ) prev_weight = 0 if G_rv . has_edge ( * edge ) : prev_weight = get_edge_weight ( G_rv , edge ) weight_sum = prev_weight + weight G_rv . add_edge ( edge [ 0 ] , edge [ 1 ] , { weight_attr : weight_sum } ) if edge in edge_to_count : edge_to_count [ edge ] += 1 else : edge_to_count [ edge ] = 1 for node in G . nodes_iter () : if node not in G_rv : G_rv . add_node ( node ) for edge , count in edge_to_count . items () : weight = get_edge_weight ( G_rv , edge ) avg_weight = float ( weight ) / count G_rv [ edge[0 ] ] [ edge[1 ] ] [ weight_attr ] = avg_weight return G_rv","title":"Returns"},{"location":"reference/prmf/#classes","text":"","title":"Classes"},{"location":"reference/prmf/#factorlibexception","text":"class FactorLibException ( / , * args , ** kwargs ) Common base class for all non-exit exceptions. View Source class FactorLibException ( Exception ): pass","title":"FactorLibException"},{"location":"reference/prmf/#ancestors-in-mro","text":"builtins.Exception builtins.BaseException","title":"Ancestors (in MRO)"},{"location":"reference/prmf/#class-variables","text":"args","title":"Class variables"},{"location":"reference/prmf/#methods","text":"","title":"Methods"},{"location":"reference/prmf/#with_traceback","text":"def with_traceback ( ... ) Exception.with_traceback(tb) -- set self. traceback to tb and return self.","title":"with_traceback"},{"location":"reference/prmf/ampl/","text":"Module prmf.ampl View Source def ampl_write_sparse_arrs ( arrs , out_fh , arr_name = \"L\" , set_name = \"L_set\" , default = 0 ) : \"\"\" Write to an AMPL data file Ls in the following form: param L default 0 := 1 1 1 1 1 1 2 -1 ... ; where first dimension is the array index, second dimension is row, third dimension is column Parameters ---------- arrs : list of scipy Compressed Sparse Row matrix out_fh : io-like arr_name : str default : float default value for not array entries not mentioned in sparse arrays \"\"\" arrs = list ( arrs ) indent = \" \" set_vs = \" \" . join ( map ( str , map ( lambda x : x + 1 , range ( len ( arrs ))))) out_fh . write ( \"set {} := {};\\n\" . format ( set_name , set_vs )) out_fh . write ( \"param {} default {} :=\\n\" . format ( arr_name , default )) for i , arr in enumerate ( arrs ) : i_arr , j_arr = arr . nonzero () for j , k in zip ( i_arr , j_arr ) : v = arr [ j,k ] out_fh . write ( indent + \" \" . join ( map ( str , [ i+1,j+1,k+1,v ] )) + \"\\n\" ) out_fh . write ( \";\" ) def ampl_write_arr_shape ( out_fh , n_row , n_col , row_name = \"I\" , col_name = \"J\" ) : row_str = \" \" . join ( map ( str , range ( 1 , n_row + 1 ))) col_str = \" \" . join ( map ( str , range ( 1 , n_col + 1 ))) out_fh . write ( \"set {} := {};\\n\" . format ( row_name , row_str )) out_fh . write ( \"set {} := {};\\n\" . format ( col_name , col_str )) def ampl_write_sparse_arr ( arr , out_fh , n_nodes , index_map = None , arr_name = \"X\" , row_name = \"I\" , col_name = \"J\" , default = 0 ) : \"\"\" Single array version of ampl_write_sparse_arrs (that is, the matrix verison, rather than the 3D tensor version). However, the use case is more to embed a dense block matrix inside a larger sparse matrix (which is only sparse because the entries outside the block are 0). This situation arises when we have measurements on a limited set of genes but wish to define a model on Parameters ---------- arr out_fh index_map : dict or None if dict then mapping of array index to array index where the domain index is (origin-0) for <arr> and the range index is (origin-0) for a larger vector space \"\"\" i_arr , j_arr = arr . nonzero () n_rows , n_cols = arr . shape ampl_write_arr_shape ( out_fh , n_nodes , n_cols ) out_fh . write ( \"param {} default {} :=\\n\" . format ( arr_name , default )) for i , j in zip ( i_arr , j_arr ) : v = arr [ i,j ] i_map = j_map = None if index_map is not None : i_map = index_map [ i ] j_map = index_map [ j ] else : i_map = i j_map = j out_fh . write ( \" \" . join ( map ( str , [ i_map+1, j_map+1, v ] )) + \"\\n\" ) def write_ampl_laplacians ( Ls , out_fh , arr_name = \"L\" ) : \"\"\" Write to an AMPL data file Ls in the following form: param L := [1, *, *]: 1 2 3 := 1 1 -1 0 2 -1 2 -1 3 0 -1 1 [2, *, *]: 1 2 3 := 1 1 -1 0 2 -1 1 0 3 0 0 0 ; \"\"\" indent = \" \" out_fh . write ( \"param {} :=\\n\" . format ( arr_name )) for i in range ( len ( Ls )) : L = Ls [ i ] n_row , n_col = L . shape col_str = \" \" . join ( map ( str , range ( 1 , n_col + 1 ))) out_fh . write ( indent + \"[{}, *, *]: {} :=\\n\" . format ( i + 1 , col_str )) for j in range ( n_row ) : out_fh . write ( indent * 2 + \" \" . join ( map ( str , [ j+1 ] + list ( L [ j,: ] ))) + \"\\n\" ) out_fh . write ( indent + \";\" ) def write_ampl_data ( data , out_fh , data_name = \"X\" , row_name = \"I\" , col_name = \"J\" ) : \"\"\" Write an ampl data file similar to the following example: set I := 1 2 3; set J := 1 2 3; param X: 1 2 3 := 1 200 100 150 2 10 15 200 3 100 50 0 ; \"\"\" n_row , n_col = data . shape row_str = \" \" . join ( map ( str , range ( 1 , n_row + 1 ))) col_str = \" \" . join ( map ( str , range ( 1 , n_col + 1 ))) # \"1 .. {}\" . format ( n_row ) # \"1 .. {}\" . format ( n_col ) out_fh . write ( \"set {} := {};\\n\" . format ( row_name , row_str )) out_fh . write ( \"set {} := {};\\n\" . format ( col_name , col_str )) out_fh . write ( \"param {}: {} :=\\n\" . format ( data_name , col_str )) for i in range ( n_row ) : row_num = i + 1 out_fh . write ( \" \" + str ( row_num ) + \" \" + \" \" . join ( map ( str , data [ i,: ] )) + \"\\n\" ) out_fh . write ( \";\\n\" ) def write_ampl_params ( n_components , out_fh , comp_name = \"K_card\" ) : \"\"\" Write an ampl data file in the following form: param K_card := 2; \"\"\" out_fh . write ( \"param {} := {};\" . format ( comp_name , n_components )) Functions ampl_write_arr_shape def ampl_write_arr_shape ( out_fh , n_row , n_col , row_name = 'I' , col_name = 'J' ) View Source def ampl_write_arr_shape ( out_fh , n_row , n_col , row_name = \"I\" , col_name = \"J\" ): row_str = \" \" . join ( map ( str , range ( 1 , n_row + 1 ))) col_str = \" \" . join ( map ( str , range ( 1 , n_col + 1 ))) out_fh . write ( \"set {} := {};\\n\" . format ( row_name , row_str )) out_fh . write ( \"set {} := {};\\n\" . format ( col_name , col_str )) ampl_write_sparse_arr def ampl_write_sparse_arr ( arr , out_fh , n_nodes , index_map = None , arr_name = 'X' , row_name = 'I' , col_name = 'J' , default = 0 ) Single array version of ampl_write_sparse_arrs (that is, the matrix verison, rather than the 3D tensor version). However, the use case is more to embed a dense block matrix inside a larger sparse matrix (which is only sparse because the entries outside the block are 0). This situation arises when we have measurements on a limited set of genes but wish to define a model on Parameters arr out_fh index_map : dict or None if dict then mapping of array index to array index where the domain index is (origin-0) for and the range index is (origin-0) for a larger vector space View Source def ampl_write_sparse_arr ( arr , out_fh , n_nodes , index_map = None , arr_name = \"X\" , row_name = \"I\" , col_name = \"J\" , default = 0 ) : \"\"\" Single array version of ampl_write_sparse_arrs (that is, the matrix verison, rather than the 3D tensor version). However, the use case is more to embed a dense block matrix inside a larger sparse matrix (which is only sparse because the entries outside the block are 0). This situation arises when we have measurements on a limited set of genes but wish to define a model on Parameters ---------- arr out_fh index_map : dict or None if dict then mapping of array index to array index where the domain index is (origin-0) for <arr> and the range index is (origin-0) for a larger vector space \"\"\" i_arr , j_arr = arr . nonzero () n_rows , n_cols = arr . shape ampl_write_arr_shape ( out_fh , n_nodes , n_cols ) out_fh . write ( \"param {} default {} :=\\n\" . format ( arr_name , default )) for i , j in zip ( i_arr , j_arr ) : v = arr [ i,j ] i_map = j_map = None if index_map is not None : i_map = index_map [ i ] j_map = index_map [ j ] else : i_map = i j_map = j out_fh . write ( \" \" . join ( map ( str , [ i_map+1, j_map+1, v ] )) + \"\\n\" ) ampl_write_sparse_arrs def ampl_write_sparse_arrs ( arrs , out_fh , arr_name = 'L' , set_name = 'L_set' , default = 0 ) Write to an AMPL data file Ls in the following form: param L default 0 := 1 1 1 1 1 1 2 -1 ... ; where first dimension is the array index, second dimension is row, third dimension is column Parameters arrs : list of scipy Compressed Sparse Row matrix out_fh : io-like arr_name : str default : float default value for not array entries not mentioned in sparse arrays View Source def ampl_write_sparse_arrs ( arrs , out_fh , arr_name = \"L\" , set_name = \"L_set\" , default = 0 ): \"\"\" Write to an AMPL data file Ls in the following form: param L default 0 := 1 1 1 1 1 1 2 -1 ... ; where first dimension is the array index, second dimension is row, third dimension is column Parameters ---------- arrs : list of scipy Compressed Sparse Row matrix out_fh : io-like arr_name : str default : float default value for not array entries not mentioned in sparse arrays \"\"\" arrs = list ( arrs ) indent = \" \" set_vs = \" \" . join ( map ( str , map ( lambda x : x + 1 , range ( len ( arrs ))))) out_fh . write ( \"set {} := {};\\n\" . format ( set_name , set_vs )) out_fh . write ( \"param {} default {} :=\\n\" . format ( arr_name , default )) for i , arr in enumerate ( arrs ): i_arr , j_arr = arr . nonzero () for j , k in zip ( i_arr , j_arr ): v = arr [ j , k ] out_fh . write ( indent + \" \" . join ( map ( str , [ i + 1 , j + 1 , k + 1 , v ])) + \"\\n\" ) out_fh . write ( \";\" ) write_ampl_data def write_ampl_data ( data , out_fh , data_name = 'X' , row_name = 'I' , col_name = 'J' ) Write an ampl data file similar to the following example: set I := 1 2 3; set J := 1 2 3; param X: 1 2 3 := 1 200 100 150 2 10 15 200 3 100 50 0 ; View Source def write_ampl_data ( data , out_fh , data_name = \"X\" , row_name = \"I\" , col_name = \"J\" ): \"\"\" Write an ampl data file similar to the following example: set I := 1 2 3; set J := 1 2 3; param X: 1 2 3 := 1 200 100 150 2 10 15 200 3 100 50 0 ; \"\"\" n_row , n_col = data . shape row_str = \" \" . join ( map ( str , range ( 1 , n_row + 1 ))) col_str = \" \" . join ( map ( str , range ( 1 , n_col + 1 ))) # \"1 .. {}\" . format ( n_row ) # \"1 .. {}\" . format ( n_col ) out_fh . write ( \"set {} := {};\\n\" . format ( row_name , row_str )) out_fh . write ( \"set {} := {};\\n\" . format ( col_name , col_str )) out_fh . write ( \"param {}: {} :=\\n\" . format ( data_name , col_str )) for i in range ( n_row ): row_num = i + 1 out_fh . write ( \" \" + str ( row_num ) + \" \" + \" \" . join ( map ( str , data [ i ,:])) + \"\\n\" ) out_fh . write ( \";\\n\" ) write_ampl_laplacians def write_ampl_laplacians ( Ls , out_fh , arr_name = 'L' ) Write to an AMPL data file Ls in the following form: param L := [1, , ]: 1 2 3 := 1 1 -1 0 2 -1 2 -1 3 0 -1 1 [2, , ]: 1 2 3 := 1 1 -1 0 2 -1 1 0 3 0 0 0 ; View Source def write_ampl_laplacians ( Ls , out_fh , arr_name = \"L\" ) : \"\"\" Write to an AMPL data file Ls in the following form: param L := [1, *, *]: 1 2 3 := 1 1 -1 0 2 -1 2 -1 3 0 -1 1 [2, *, *]: 1 2 3 := 1 1 -1 0 2 -1 1 0 3 0 0 0 ; \"\"\" indent = \" \" out_fh . write ( \"param {} :=\\n\" . format ( arr_name )) for i in range ( len ( Ls )) : L = Ls [ i ] n_row , n_col = L . shape col_str = \" \" . join ( map ( str , range ( 1 , n_col + 1 ))) out_fh . write ( indent + \"[{}, *, *]: {} :=\\n\" . format ( i + 1 , col_str )) for j in range ( n_row ) : out_fh . write ( indent * 2 + \" \" . join ( map ( str , [ j+1 ] + list ( L [ j,: ] ))) + \"\\n\" ) out_fh . write ( indent + \";\" ) write_ampl_params def write_ampl_params ( n_components , out_fh , comp_name = 'K_card' ) Write an ampl data file in the following form: param K_card := 2; View Source def write_ampl_params ( n_components , out_fh , comp_name = \"K_card\" ): \"\"\" Write an ampl data file in the following form: param K_card := 2; \"\"\" out_fh . write ( \"param {} := {};\" . format ( comp_name , n_components ))","title":"Ampl"},{"location":"reference/prmf/ampl/#module-prmfampl","text":"View Source def ampl_write_sparse_arrs ( arrs , out_fh , arr_name = \"L\" , set_name = \"L_set\" , default = 0 ) : \"\"\" Write to an AMPL data file Ls in the following form: param L default 0 := 1 1 1 1 1 1 2 -1 ... ; where first dimension is the array index, second dimension is row, third dimension is column Parameters ---------- arrs : list of scipy Compressed Sparse Row matrix out_fh : io-like arr_name : str default : float default value for not array entries not mentioned in sparse arrays \"\"\" arrs = list ( arrs ) indent = \" \" set_vs = \" \" . join ( map ( str , map ( lambda x : x + 1 , range ( len ( arrs ))))) out_fh . write ( \"set {} := {};\\n\" . format ( set_name , set_vs )) out_fh . write ( \"param {} default {} :=\\n\" . format ( arr_name , default )) for i , arr in enumerate ( arrs ) : i_arr , j_arr = arr . nonzero () for j , k in zip ( i_arr , j_arr ) : v = arr [ j,k ] out_fh . write ( indent + \" \" . join ( map ( str , [ i+1,j+1,k+1,v ] )) + \"\\n\" ) out_fh . write ( \";\" ) def ampl_write_arr_shape ( out_fh , n_row , n_col , row_name = \"I\" , col_name = \"J\" ) : row_str = \" \" . join ( map ( str , range ( 1 , n_row + 1 ))) col_str = \" \" . join ( map ( str , range ( 1 , n_col + 1 ))) out_fh . write ( \"set {} := {};\\n\" . format ( row_name , row_str )) out_fh . write ( \"set {} := {};\\n\" . format ( col_name , col_str )) def ampl_write_sparse_arr ( arr , out_fh , n_nodes , index_map = None , arr_name = \"X\" , row_name = \"I\" , col_name = \"J\" , default = 0 ) : \"\"\" Single array version of ampl_write_sparse_arrs (that is, the matrix verison, rather than the 3D tensor version). However, the use case is more to embed a dense block matrix inside a larger sparse matrix (which is only sparse because the entries outside the block are 0). This situation arises when we have measurements on a limited set of genes but wish to define a model on Parameters ---------- arr out_fh index_map : dict or None if dict then mapping of array index to array index where the domain index is (origin-0) for <arr> and the range index is (origin-0) for a larger vector space \"\"\" i_arr , j_arr = arr . nonzero () n_rows , n_cols = arr . shape ampl_write_arr_shape ( out_fh , n_nodes , n_cols ) out_fh . write ( \"param {} default {} :=\\n\" . format ( arr_name , default )) for i , j in zip ( i_arr , j_arr ) : v = arr [ i,j ] i_map = j_map = None if index_map is not None : i_map = index_map [ i ] j_map = index_map [ j ] else : i_map = i j_map = j out_fh . write ( \" \" . join ( map ( str , [ i_map+1, j_map+1, v ] )) + \"\\n\" ) def write_ampl_laplacians ( Ls , out_fh , arr_name = \"L\" ) : \"\"\" Write to an AMPL data file Ls in the following form: param L := [1, *, *]: 1 2 3 := 1 1 -1 0 2 -1 2 -1 3 0 -1 1 [2, *, *]: 1 2 3 := 1 1 -1 0 2 -1 1 0 3 0 0 0 ; \"\"\" indent = \" \" out_fh . write ( \"param {} :=\\n\" . format ( arr_name )) for i in range ( len ( Ls )) : L = Ls [ i ] n_row , n_col = L . shape col_str = \" \" . join ( map ( str , range ( 1 , n_col + 1 ))) out_fh . write ( indent + \"[{}, *, *]: {} :=\\n\" . format ( i + 1 , col_str )) for j in range ( n_row ) : out_fh . write ( indent * 2 + \" \" . join ( map ( str , [ j+1 ] + list ( L [ j,: ] ))) + \"\\n\" ) out_fh . write ( indent + \";\" ) def write_ampl_data ( data , out_fh , data_name = \"X\" , row_name = \"I\" , col_name = \"J\" ) : \"\"\" Write an ampl data file similar to the following example: set I := 1 2 3; set J := 1 2 3; param X: 1 2 3 := 1 200 100 150 2 10 15 200 3 100 50 0 ; \"\"\" n_row , n_col = data . shape row_str = \" \" . join ( map ( str , range ( 1 , n_row + 1 ))) col_str = \" \" . join ( map ( str , range ( 1 , n_col + 1 ))) # \"1 .. {}\" . format ( n_row ) # \"1 .. {}\" . format ( n_col ) out_fh . write ( \"set {} := {};\\n\" . format ( row_name , row_str )) out_fh . write ( \"set {} := {};\\n\" . format ( col_name , col_str )) out_fh . write ( \"param {}: {} :=\\n\" . format ( data_name , col_str )) for i in range ( n_row ) : row_num = i + 1 out_fh . write ( \" \" + str ( row_num ) + \" \" + \" \" . join ( map ( str , data [ i,: ] )) + \"\\n\" ) out_fh . write ( \";\\n\" ) def write_ampl_params ( n_components , out_fh , comp_name = \"K_card\" ) : \"\"\" Write an ampl data file in the following form: param K_card := 2; \"\"\" out_fh . write ( \"param {} := {};\" . format ( comp_name , n_components ))","title":"Module prmf.ampl"},{"location":"reference/prmf/ampl/#functions","text":"","title":"Functions"},{"location":"reference/prmf/ampl/#ampl_write_arr_shape","text":"def ampl_write_arr_shape ( out_fh , n_row , n_col , row_name = 'I' , col_name = 'J' ) View Source def ampl_write_arr_shape ( out_fh , n_row , n_col , row_name = \"I\" , col_name = \"J\" ): row_str = \" \" . join ( map ( str , range ( 1 , n_row + 1 ))) col_str = \" \" . join ( map ( str , range ( 1 , n_col + 1 ))) out_fh . write ( \"set {} := {};\\n\" . format ( row_name , row_str )) out_fh . write ( \"set {} := {};\\n\" . format ( col_name , col_str ))","title":"ampl_write_arr_shape"},{"location":"reference/prmf/ampl/#ampl_write_sparse_arr","text":"def ampl_write_sparse_arr ( arr , out_fh , n_nodes , index_map = None , arr_name = 'X' , row_name = 'I' , col_name = 'J' , default = 0 ) Single array version of ampl_write_sparse_arrs (that is, the matrix verison, rather than the 3D tensor version). However, the use case is more to embed a dense block matrix inside a larger sparse matrix (which is only sparse because the entries outside the block are 0). This situation arises when we have measurements on a limited set of genes but wish to define a model on","title":"ampl_write_sparse_arr"},{"location":"reference/prmf/ampl/#parameters","text":"arr out_fh index_map : dict or None if dict then mapping of array index to array index where the domain index is (origin-0) for and the range index is (origin-0) for a larger vector space View Source def ampl_write_sparse_arr ( arr , out_fh , n_nodes , index_map = None , arr_name = \"X\" , row_name = \"I\" , col_name = \"J\" , default = 0 ) : \"\"\" Single array version of ampl_write_sparse_arrs (that is, the matrix verison, rather than the 3D tensor version). However, the use case is more to embed a dense block matrix inside a larger sparse matrix (which is only sparse because the entries outside the block are 0). This situation arises when we have measurements on a limited set of genes but wish to define a model on Parameters ---------- arr out_fh index_map : dict or None if dict then mapping of array index to array index where the domain index is (origin-0) for <arr> and the range index is (origin-0) for a larger vector space \"\"\" i_arr , j_arr = arr . nonzero () n_rows , n_cols = arr . shape ampl_write_arr_shape ( out_fh , n_nodes , n_cols ) out_fh . write ( \"param {} default {} :=\\n\" . format ( arr_name , default )) for i , j in zip ( i_arr , j_arr ) : v = arr [ i,j ] i_map = j_map = None if index_map is not None : i_map = index_map [ i ] j_map = index_map [ j ] else : i_map = i j_map = j out_fh . write ( \" \" . join ( map ( str , [ i_map+1, j_map+1, v ] )) + \"\\n\" )","title":"Parameters"},{"location":"reference/prmf/ampl/#ampl_write_sparse_arrs","text":"def ampl_write_sparse_arrs ( arrs , out_fh , arr_name = 'L' , set_name = 'L_set' , default = 0 ) Write to an AMPL data file Ls in the following form: param L default 0 := 1 1 1 1 1 1 2 -1 ... ; where first dimension is the array index, second dimension is row, third dimension is column","title":"ampl_write_sparse_arrs"},{"location":"reference/prmf/ampl/#parameters_1","text":"arrs : list of scipy Compressed Sparse Row matrix out_fh : io-like arr_name : str default : float default value for not array entries not mentioned in sparse arrays View Source def ampl_write_sparse_arrs ( arrs , out_fh , arr_name = \"L\" , set_name = \"L_set\" , default = 0 ): \"\"\" Write to an AMPL data file Ls in the following form: param L default 0 := 1 1 1 1 1 1 2 -1 ... ; where first dimension is the array index, second dimension is row, third dimension is column Parameters ---------- arrs : list of scipy Compressed Sparse Row matrix out_fh : io-like arr_name : str default : float default value for not array entries not mentioned in sparse arrays \"\"\" arrs = list ( arrs ) indent = \" \" set_vs = \" \" . join ( map ( str , map ( lambda x : x + 1 , range ( len ( arrs ))))) out_fh . write ( \"set {} := {};\\n\" . format ( set_name , set_vs )) out_fh . write ( \"param {} default {} :=\\n\" . format ( arr_name , default )) for i , arr in enumerate ( arrs ): i_arr , j_arr = arr . nonzero () for j , k in zip ( i_arr , j_arr ): v = arr [ j , k ] out_fh . write ( indent + \" \" . join ( map ( str , [ i + 1 , j + 1 , k + 1 , v ])) + \"\\n\" ) out_fh . write ( \";\" )","title":"Parameters"},{"location":"reference/prmf/ampl/#write_ampl_data","text":"def write_ampl_data ( data , out_fh , data_name = 'X' , row_name = 'I' , col_name = 'J' ) Write an ampl data file similar to the following example: set I := 1 2 3; set J := 1 2 3; param X: 1 2 3 := 1 200 100 150 2 10 15 200 3 100 50 0 ; View Source def write_ampl_data ( data , out_fh , data_name = \"X\" , row_name = \"I\" , col_name = \"J\" ): \"\"\" Write an ampl data file similar to the following example: set I := 1 2 3; set J := 1 2 3; param X: 1 2 3 := 1 200 100 150 2 10 15 200 3 100 50 0 ; \"\"\" n_row , n_col = data . shape row_str = \" \" . join ( map ( str , range ( 1 , n_row + 1 ))) col_str = \" \" . join ( map ( str , range ( 1 , n_col + 1 ))) # \"1 .. {}\" . format ( n_row ) # \"1 .. {}\" . format ( n_col ) out_fh . write ( \"set {} := {};\\n\" . format ( row_name , row_str )) out_fh . write ( \"set {} := {};\\n\" . format ( col_name , col_str )) out_fh . write ( \"param {}: {} :=\\n\" . format ( data_name , col_str )) for i in range ( n_row ): row_num = i + 1 out_fh . write ( \" \" + str ( row_num ) + \" \" + \" \" . join ( map ( str , data [ i ,:])) + \"\\n\" ) out_fh . write ( \";\\n\" )","title":"write_ampl_data"},{"location":"reference/prmf/ampl/#write_ampl_laplacians","text":"def write_ampl_laplacians ( Ls , out_fh , arr_name = 'L' ) Write to an AMPL data file Ls in the following form: param L := [1, , ]: 1 2 3 := 1 1 -1 0 2 -1 2 -1 3 0 -1 1 [2, , ]: 1 2 3 := 1 1 -1 0 2 -1 1 0 3 0 0 0 ; View Source def write_ampl_laplacians ( Ls , out_fh , arr_name = \"L\" ) : \"\"\" Write to an AMPL data file Ls in the following form: param L := [1, *, *]: 1 2 3 := 1 1 -1 0 2 -1 2 -1 3 0 -1 1 [2, *, *]: 1 2 3 := 1 1 -1 0 2 -1 1 0 3 0 0 0 ; \"\"\" indent = \" \" out_fh . write ( \"param {} :=\\n\" . format ( arr_name )) for i in range ( len ( Ls )) : L = Ls [ i ] n_row , n_col = L . shape col_str = \" \" . join ( map ( str , range ( 1 , n_col + 1 ))) out_fh . write ( indent + \"[{}, *, *]: {} :=\\n\" . format ( i + 1 , col_str )) for j in range ( n_row ) : out_fh . write ( indent * 2 + \" \" . join ( map ( str , [ j+1 ] + list ( L [ j,: ] ))) + \"\\n\" ) out_fh . write ( indent + \";\" )","title":"write_ampl_laplacians"},{"location":"reference/prmf/ampl/#write_ampl_params","text":"def write_ampl_params ( n_components , out_fh , comp_name = 'K_card' ) Write an ampl data file in the following form: param K_card := 2; View Source def write_ampl_params ( n_components , out_fh , comp_name = \"K_card\" ): \"\"\" Write an ampl data file in the following form: param K_card := 2; \"\"\" out_fh . write ( \"param {} := {};\" . format ( comp_name , n_components ))","title":"write_ampl_params"},{"location":"reference/prmf/ensembl/","text":"Module prmf.ensembl Ensembl.org recommends the use of biomart to map HGNC, ENSG, ENST, ENSP Currently these are implemented in the R script dl_ensembl_map TODO add biomart python API functions here View Source \"\"\" Ensembl.org recommends the use of biomart to map HGNC, ENSG, ENST, ENSP Currently these are implemented in the R script dl_ensembl_map TODO add biomart python API functions here \"\"\" import sys import re import os , os.path import subprocess as sp import glob def download_mapping_tsv ( outdir , release = 'latest' ): \"\"\" Download an Ensembl identifier mapping file (HGNC <-> ENSP) Parameters ---------- outdir: str Directory to place downloaded file in; file will be named as it is on EBI's FTP server release: int or str either 'latest' or a specific release number such as '97' Returns ------- rv : str or None the downloaded filepath or None if it cannot be determined (due to glob wildcards matching multiple files) TODO ---- \"\"\" rv = None url = None if release == 'latest' : url = 'ftp://ftp.ensembl.org/pub/current_tsv/homo_sapiens/Homo_sapiens.GRCh38.*.refseq.tsv.gz' else : url = 'ftp://ftp.ensembl.org/pub/release-{}/tsv/homo_sapiens/Homo_sapiens.GRCh38.{}.refseq.tsv.gz' . format ( release , release ) bn = os . path . basename ( url ) fpath = os . path . join ( outdir , bn ) glob_rv = glob . glob ( fpath ) if len ( glob_rv ) == 1 : rv = glob_rv [ 0 ] args = [ 'wget' , url ] sp . check_call ( args , cwd = outdir ) return rv def parse_mapping_tsv ( fh , key_index = 0 , value_index = 1 , delim = ' \\t ' ): \"\"\" Parse a TSV file like the file at ftp://ftp.ensembl.org/pub/release-94/tsv/homo_sapiens/Homo_sapiens.GRCh38.94.refseq.tsv.gz into a mapping \"\"\" rv = {} if type ( fh ) is str : fh = open ( fh , 'r' ) for line in fh : line = line . rstrip () words = line . split ( delim ) # some values for key_index may be missing, skip those entries in the file if key_index >= len ( words ): continue key = words [ key_index ] value = None if ( len ( words ) > value_index ): value = words [ value_index ] else : # TODO warn? pass if value is not None : if key not in rv : rv [ key ] = set () rv [ key ] . add ( value ) return rv def map_hgnc_to_ensps ( fh ): \"\"\" Return map from HGNC symbol to many ENSPs Parameters ---------- fh : file-like database file from dl_ensembl_map: 4 column TSV of HGNC, ENSG, ENST, ENSP Returns ------- rv : dict mapping of HGNC symbol to a set of ENSPs \"\"\" return parse_mapping_tsv ( fh , key_index = 0 , value_index = 3 ) def filter_one_to_many ( hgnc_to_ensps_map , G ): \"\"\" Return a map from HGNC symbol to one ENSP. The one is chosen to be the first ENSP in G. ENSPs are examined in sorted order. Parameters ---------- hgnc_to_ensps_map : dict return value of map_hgnc_to_ensps copy_map : dict map hgnc to the number of ENSP in the map that also appear in G G : nx.Graph protein-protein interaction network such as STRING \"\"\" rv = {} copy_map = {} for hgnc , ensps in hgnc_to_ensps_map . items (): ensps = sorted ( ensps ) any_ensp_in_G = False for ensp in ensps : if ensp in G : any_ensp_in_G = True if hgnc in rv : # then this is the second ENSP in G if hgnc in copy_map : copy_map [ hgnc ] += 1 else : copy_map [ hgnc ] = 2 else : rv [ hgnc ] = ensp if not any_ensp_in_G : # then use the first ensp in sorted order if len ( ensps ) > 0 : rv [ hgnc ] = ensps [ 0 ] return rv , copy_map def map_ensp_to_hgnc ( fh ): rv = {} for line in fh : line = line . rstrip () words = line . split ( ' \\t ' ) hgnc_symbol = words [ 0 ] ensp_id = None if ( len ( words ) >= 4 ): ensp_id = words [ 3 ] if ensp_id is not None : if ensp_id in rv : sys . stderr . write ( \"Encountered ENSP id {} more than once \\n \" . format ( ensp_id )) rv [ ensp_id ] = hgnc_symbol return rv def apply_map ( word_map , gene_list_fh ): \"\"\" Read words (one per line) from gene_list_fh and apply word_map. Return mapped words as a set. Parameters ---------- word_map : dict mapping of HGNC symbol to set of ENSPs gene_list_fh : file-like newline delimited file of HGNC symbols Returns ------- rv : list sorted list of mapped ENSP identifiers \"\"\" rv = set () for line in gene_list_fh : hgnc_symbol = line . rstrip () ensp_ids = word_map . get ( hgnc_symbol ) if ensp_ids is None : sys . stderr . write ( \"Unrecognized HGNC symbol: {} \\n \" . format ( hgnc_symbol )) else : rv = rv . union ( ensp_ids ) rv = sorted ( rv ) return rv def apply_mapping ( word_map , io_pairs ): \"\"\" For infile, outfile pairs in <io_pairs>, read words separated by PCRE non-word-character regexp '\\W', apply <word_map>, and write mapped words to outfile. Note many words in infile do not need to appear in the <word_map> and will be left unmapped. \"\"\" sep_re = re . compile ( '\\W' ) word = \"\" for infile , outfile in io_pairs : with open ( infile , \"r\" ) as ifh : with open ( outfile , \"w\" ) as ofh : for line in ifh : for char in line : match_data = sep_re . match ( char ) if ( match_data is not None ): # then process word ensp = word_map . get ( word ) if ensp is None : ofh . write ( word + match_data . group ( 0 )) else : ofh . write ( ensp + match_data . group ( 0 )) word = \"\" else : word += char Functions apply_map def apply_map ( word_map , gene_list_fh ) Read words (one per line) from gene_list_fh and apply word_map. Return mapped words as a set. Parameters word_map : dict mapping of HGNC symbol to set of ENSPs gene_list_fh : file-like newline delimited file of HGNC symbols Returns rv : list sorted list of mapped ENSP identifiers View Source def apply_map ( word_map , gene_list_fh ): \"\"\" Read words (one per line) from gene_list_fh and apply word_map. Return mapped words as a set. Parameters ---------- word_map : dict mapping of HGNC symbol to set of ENSPs gene_list_fh : file-like newline delimited file of HGNC symbols Returns ------- rv : list sorted list of mapped ENSP identifiers \"\"\" rv = set () for line in gene_list_fh : hgnc_symbol = line . rstrip () ensp_ids = word_map . get ( hgnc_symbol ) if ensp_ids is None : sys . stderr . write ( \"Unrecognized HGNC symbol: {}\\n\" . format ( hgnc_symbol )) else : rv = rv . union ( ensp_ids ) rv = sorted ( rv ) return rv apply_mapping def apply_mapping ( word_map , io_pairs ) For infile, outfile pairs in , read words separated by PCRE non-word-character regexp '\\W', apply , and write mapped words to outfile. Note many words in infile do not need to appear in the and will be left unmapped. View Source def apply_mapping ( word_map , io_pairs ): \"\"\" For infile, outfile pairs in <io_pairs>, read words separated by PCRE non-word-character regexp '\\W', apply <word_map>, and write mapped words to outfile. Note many words in infile do not need to appear in the <word_map> and will be left unmapped. \"\"\" sep_re = re . compile ( '\\W' ) word = \"\" for infile , outfile in io_pairs : with open ( infile , \"r\" ) as ifh : with open ( outfile , \"w\" ) as ofh : for line in ifh : for char in line : match_data = sep_re . match ( char ) if ( match_data is not None ): # then process word ensp = word_map . get ( word ) if ensp is None : ofh . write ( word + match_data . group ( 0 )) else : ofh . write ( ensp + match_data . group ( 0 )) word = \"\" else : word += char download_mapping_tsv def download_mapping_tsv ( outdir , release = 'latest' ) Download an Ensembl identifier mapping file (HGNC <-> ENSP) Parameters outdir: str Directory to place downloaded file in; file will be named as it is on EBI's FTP server release: int or str either 'latest' or a specific release number such as '97' Returns rv : str or None the downloaded filepath or None if it cannot be determined (due to glob wildcards matching multiple files) TODO View Source def download_mapping_tsv ( outdir , release = 'latest' ) : \"\"\" Download an Ensembl identifier mapping file (HGNC <-> ENSP) Parameters ---------- outdir: str Directory to place downloaded file in; file will be named as it is on EBI's FTP server release: int or str either 'latest' or a specific release number such as '97' Returns ------- rv : str or None the downloaded filepath or None if it cannot be determined (due to glob wildcards matching multiple files) TODO ---- \"\"\" rv = None url = None if release == 'latest' : url = 'ftp://ftp.ensembl.org/pub/current_tsv/homo_sapiens/Homo_sapiens.GRCh38.*.refseq.tsv.gz' else : url = 'ftp://ftp.ensembl.org/pub/release-{}/tsv/homo_sapiens/Homo_sapiens.GRCh38.{}.refseq.tsv.gz' .format ( release , release ) bn = os.path.basename ( url ) fpath = os.path.join ( outdir , bn ) glob_rv = glob.glob ( fpath ) if len ( glob_rv ) == 1 : rv = glob_rv [ 0 ] args = [ 'wget' , url ] sp.check_call ( args , cwd = outdir ) return rv filter_one_to_many def filter_one_to_many ( hgnc_to_ensps_map , G ) Return a map from HGNC symbol to one ENSP. The one is chosen to be the first ENSP in G. ENSPs are examined in sorted order. Parameters hgnc_to_ensps_map : dict return value of map_hgnc_to_ensps copy_map : dict map hgnc to the number of ENSP in the map that also appear in G G : nx.Graph protein-protein interaction network such as STRING View Source def filter_one_to_many ( hgnc_to_ensps_map , G ) : \"\"\" Return a map from HGNC symbol to one ENSP. The one is chosen to be the first ENSP in G. ENSPs are examined in sorted order. Parameters ---------- hgnc_to_ensps_map : dict return value of map_hgnc_to_ensps copy_map : dict map hgnc to the number of ENSP in the map that also appear in G G : nx.Graph protein-protein interaction network such as STRING \"\"\" rv = {} copy_map = {} for hgnc , ensps in hgnc_to_ensps_map . items () : ensps = sorted ( ensps ) any_ensp_in_G = False for ensp in ensps : if ensp in G : any_ensp_in_G = True if hgnc in rv : # then this is the second ENSP in G if hgnc in copy_map : copy_map [ hgnc ] += 1 else : copy_map [ hgnc ] = 2 else : rv [ hgnc ] = ensp if not any_ensp_in_G : # then use the first ensp in sorted order if len ( ensps ) > 0 : rv [ hgnc ] = ensps [ 0 ] return rv , copy_map map_ensp_to_hgnc def map_ensp_to_hgnc ( fh ) View Source def map_ensp_to_hgnc ( fh ) : rv = {} for line in fh : line = line . rstrip () words = line . split ( '\\t' ) hgnc_symbol = words [ 0 ] ensp_id = None if ( len ( words ) >= 4 ) : ensp_id = words [ 3 ] if ensp_id is not None : if ensp_id in rv : sys . stderr . write ( \"Encountered ENSP id {} more than once\\n\" . format ( ensp_id )) rv [ ensp_id ] = hgnc_symbol return rv map_hgnc_to_ensps def map_hgnc_to_ensps ( fh ) Return map from HGNC symbol to many ENSPs Parameters fh : file-like database file from dl_ensembl_map: 4 column TSV of HGNC, ENSG, ENST, ENSP Returns rv : dict mapping of HGNC symbol to a set of ENSPs View Source def map_hgnc_to_ensps ( fh ): \"\"\" Return map from HGNC symbol to many ENSPs Parameters ---------- fh : file-like database file from dl_ensembl_map: 4 column TSV of HGNC, ENSG, ENST, ENSP Returns ------- rv : dict mapping of HGNC symbol to a set of ENSPs \"\"\" return parse_mapping_tsv ( fh , key_index = 0 , value_index = 3 ) parse_mapping_tsv def parse_mapping_tsv ( fh , key_index = 0 , value_index = 1 , delim = ' \\t ' ) Parse a TSV file like the file at ftp://ftp.ensembl.org/pub/release-94/tsv/homo_sapiens/Homo_sapiens.GRCh38.94.refseq.tsv.gz into a mapping View Source def parse_mapping_tsv ( fh , key_index = 0 , value_index = 1 , delim = '\\t' ) : \"\"\" Parse a TSV file like the file at ftp://ftp.ensembl.org/pub/release-94/tsv/homo_sapiens/Homo_sapiens.GRCh38.94.refseq.tsv.gz into a mapping \"\"\" rv = {} if type ( fh ) is str : fh = open ( fh , 'r' ) for line in fh : line = line . rstrip () words = line . split ( delim ) # some values for key_index may be missing , skip those entries in the file if key_index >= len ( words ) : continue key = words [ key_index ] value = None if ( len ( words ) > value_index ) : value = words [ value_index ] else : # TODO warn ? pass if value is not None : if key not in rv : rv [ key ] = set () rv [ key ] . add ( value ) return rv","title":"Ensembl"},{"location":"reference/prmf/ensembl/#module-prmfensembl","text":"Ensembl.org recommends the use of biomart to map HGNC, ENSG, ENST, ENSP Currently these are implemented in the R script dl_ensembl_map TODO add biomart python API functions here View Source \"\"\" Ensembl.org recommends the use of biomart to map HGNC, ENSG, ENST, ENSP Currently these are implemented in the R script dl_ensembl_map TODO add biomart python API functions here \"\"\" import sys import re import os , os.path import subprocess as sp import glob def download_mapping_tsv ( outdir , release = 'latest' ): \"\"\" Download an Ensembl identifier mapping file (HGNC <-> ENSP) Parameters ---------- outdir: str Directory to place downloaded file in; file will be named as it is on EBI's FTP server release: int or str either 'latest' or a specific release number such as '97' Returns ------- rv : str or None the downloaded filepath or None if it cannot be determined (due to glob wildcards matching multiple files) TODO ---- \"\"\" rv = None url = None if release == 'latest' : url = 'ftp://ftp.ensembl.org/pub/current_tsv/homo_sapiens/Homo_sapiens.GRCh38.*.refseq.tsv.gz' else : url = 'ftp://ftp.ensembl.org/pub/release-{}/tsv/homo_sapiens/Homo_sapiens.GRCh38.{}.refseq.tsv.gz' . format ( release , release ) bn = os . path . basename ( url ) fpath = os . path . join ( outdir , bn ) glob_rv = glob . glob ( fpath ) if len ( glob_rv ) == 1 : rv = glob_rv [ 0 ] args = [ 'wget' , url ] sp . check_call ( args , cwd = outdir ) return rv def parse_mapping_tsv ( fh , key_index = 0 , value_index = 1 , delim = ' \\t ' ): \"\"\" Parse a TSV file like the file at ftp://ftp.ensembl.org/pub/release-94/tsv/homo_sapiens/Homo_sapiens.GRCh38.94.refseq.tsv.gz into a mapping \"\"\" rv = {} if type ( fh ) is str : fh = open ( fh , 'r' ) for line in fh : line = line . rstrip () words = line . split ( delim ) # some values for key_index may be missing, skip those entries in the file if key_index >= len ( words ): continue key = words [ key_index ] value = None if ( len ( words ) > value_index ): value = words [ value_index ] else : # TODO warn? pass if value is not None : if key not in rv : rv [ key ] = set () rv [ key ] . add ( value ) return rv def map_hgnc_to_ensps ( fh ): \"\"\" Return map from HGNC symbol to many ENSPs Parameters ---------- fh : file-like database file from dl_ensembl_map: 4 column TSV of HGNC, ENSG, ENST, ENSP Returns ------- rv : dict mapping of HGNC symbol to a set of ENSPs \"\"\" return parse_mapping_tsv ( fh , key_index = 0 , value_index = 3 ) def filter_one_to_many ( hgnc_to_ensps_map , G ): \"\"\" Return a map from HGNC symbol to one ENSP. The one is chosen to be the first ENSP in G. ENSPs are examined in sorted order. Parameters ---------- hgnc_to_ensps_map : dict return value of map_hgnc_to_ensps copy_map : dict map hgnc to the number of ENSP in the map that also appear in G G : nx.Graph protein-protein interaction network such as STRING \"\"\" rv = {} copy_map = {} for hgnc , ensps in hgnc_to_ensps_map . items (): ensps = sorted ( ensps ) any_ensp_in_G = False for ensp in ensps : if ensp in G : any_ensp_in_G = True if hgnc in rv : # then this is the second ENSP in G if hgnc in copy_map : copy_map [ hgnc ] += 1 else : copy_map [ hgnc ] = 2 else : rv [ hgnc ] = ensp if not any_ensp_in_G : # then use the first ensp in sorted order if len ( ensps ) > 0 : rv [ hgnc ] = ensps [ 0 ] return rv , copy_map def map_ensp_to_hgnc ( fh ): rv = {} for line in fh : line = line . rstrip () words = line . split ( ' \\t ' ) hgnc_symbol = words [ 0 ] ensp_id = None if ( len ( words ) >= 4 ): ensp_id = words [ 3 ] if ensp_id is not None : if ensp_id in rv : sys . stderr . write ( \"Encountered ENSP id {} more than once \\n \" . format ( ensp_id )) rv [ ensp_id ] = hgnc_symbol return rv def apply_map ( word_map , gene_list_fh ): \"\"\" Read words (one per line) from gene_list_fh and apply word_map. Return mapped words as a set. Parameters ---------- word_map : dict mapping of HGNC symbol to set of ENSPs gene_list_fh : file-like newline delimited file of HGNC symbols Returns ------- rv : list sorted list of mapped ENSP identifiers \"\"\" rv = set () for line in gene_list_fh : hgnc_symbol = line . rstrip () ensp_ids = word_map . get ( hgnc_symbol ) if ensp_ids is None : sys . stderr . write ( \"Unrecognized HGNC symbol: {} \\n \" . format ( hgnc_symbol )) else : rv = rv . union ( ensp_ids ) rv = sorted ( rv ) return rv def apply_mapping ( word_map , io_pairs ): \"\"\" For infile, outfile pairs in <io_pairs>, read words separated by PCRE non-word-character regexp '\\W', apply <word_map>, and write mapped words to outfile. Note many words in infile do not need to appear in the <word_map> and will be left unmapped. \"\"\" sep_re = re . compile ( '\\W' ) word = \"\" for infile , outfile in io_pairs : with open ( infile , \"r\" ) as ifh : with open ( outfile , \"w\" ) as ofh : for line in ifh : for char in line : match_data = sep_re . match ( char ) if ( match_data is not None ): # then process word ensp = word_map . get ( word ) if ensp is None : ofh . write ( word + match_data . group ( 0 )) else : ofh . write ( ensp + match_data . group ( 0 )) word = \"\" else : word += char","title":"Module prmf.ensembl"},{"location":"reference/prmf/ensembl/#functions","text":"","title":"Functions"},{"location":"reference/prmf/ensembl/#apply_map","text":"def apply_map ( word_map , gene_list_fh ) Read words (one per line) from gene_list_fh and apply word_map. Return mapped words as a set.","title":"apply_map"},{"location":"reference/prmf/ensembl/#parameters","text":"word_map : dict mapping of HGNC symbol to set of ENSPs gene_list_fh : file-like newline delimited file of HGNC symbols","title":"Parameters"},{"location":"reference/prmf/ensembl/#returns","text":"rv : list sorted list of mapped ENSP identifiers View Source def apply_map ( word_map , gene_list_fh ): \"\"\" Read words (one per line) from gene_list_fh and apply word_map. Return mapped words as a set. Parameters ---------- word_map : dict mapping of HGNC symbol to set of ENSPs gene_list_fh : file-like newline delimited file of HGNC symbols Returns ------- rv : list sorted list of mapped ENSP identifiers \"\"\" rv = set () for line in gene_list_fh : hgnc_symbol = line . rstrip () ensp_ids = word_map . get ( hgnc_symbol ) if ensp_ids is None : sys . stderr . write ( \"Unrecognized HGNC symbol: {}\\n\" . format ( hgnc_symbol )) else : rv = rv . union ( ensp_ids ) rv = sorted ( rv ) return rv","title":"Returns"},{"location":"reference/prmf/ensembl/#apply_mapping","text":"def apply_mapping ( word_map , io_pairs ) For infile, outfile pairs in , read words separated by PCRE non-word-character regexp '\\W', apply , and write mapped words to outfile. Note many words in infile do not need to appear in the and will be left unmapped. View Source def apply_mapping ( word_map , io_pairs ): \"\"\" For infile, outfile pairs in <io_pairs>, read words separated by PCRE non-word-character regexp '\\W', apply <word_map>, and write mapped words to outfile. Note many words in infile do not need to appear in the <word_map> and will be left unmapped. \"\"\" sep_re = re . compile ( '\\W' ) word = \"\" for infile , outfile in io_pairs : with open ( infile , \"r\" ) as ifh : with open ( outfile , \"w\" ) as ofh : for line in ifh : for char in line : match_data = sep_re . match ( char ) if ( match_data is not None ): # then process word ensp = word_map . get ( word ) if ensp is None : ofh . write ( word + match_data . group ( 0 )) else : ofh . write ( ensp + match_data . group ( 0 )) word = \"\" else : word += char","title":"apply_mapping"},{"location":"reference/prmf/ensembl/#download_mapping_tsv","text":"def download_mapping_tsv ( outdir , release = 'latest' ) Download an Ensembl identifier mapping file (HGNC <-> ENSP)","title":"download_mapping_tsv"},{"location":"reference/prmf/ensembl/#parameters_1","text":"outdir: str Directory to place downloaded file in; file will be named as it is on EBI's FTP server release: int or str either 'latest' or a specific release number such as '97'","title":"Parameters"},{"location":"reference/prmf/ensembl/#returns_1","text":"rv : str or None the downloaded filepath or None if it cannot be determined (due to glob wildcards matching multiple files)","title":"Returns"},{"location":"reference/prmf/ensembl/#todo","text":"View Source def download_mapping_tsv ( outdir , release = 'latest' ) : \"\"\" Download an Ensembl identifier mapping file (HGNC <-> ENSP) Parameters ---------- outdir: str Directory to place downloaded file in; file will be named as it is on EBI's FTP server release: int or str either 'latest' or a specific release number such as '97' Returns ------- rv : str or None the downloaded filepath or None if it cannot be determined (due to glob wildcards matching multiple files) TODO ---- \"\"\" rv = None url = None if release == 'latest' : url = 'ftp://ftp.ensembl.org/pub/current_tsv/homo_sapiens/Homo_sapiens.GRCh38.*.refseq.tsv.gz' else : url = 'ftp://ftp.ensembl.org/pub/release-{}/tsv/homo_sapiens/Homo_sapiens.GRCh38.{}.refseq.tsv.gz' .format ( release , release ) bn = os.path.basename ( url ) fpath = os.path.join ( outdir , bn ) glob_rv = glob.glob ( fpath ) if len ( glob_rv ) == 1 : rv = glob_rv [ 0 ] args = [ 'wget' , url ] sp.check_call ( args , cwd = outdir ) return rv","title":"TODO"},{"location":"reference/prmf/ensembl/#filter_one_to_many","text":"def filter_one_to_many ( hgnc_to_ensps_map , G ) Return a map from HGNC symbol to one ENSP. The one is chosen to be the first ENSP in G. ENSPs are examined in sorted order.","title":"filter_one_to_many"},{"location":"reference/prmf/ensembl/#parameters_2","text":"hgnc_to_ensps_map : dict return value of map_hgnc_to_ensps copy_map : dict map hgnc to the number of ENSP in the map that also appear in G G : nx.Graph protein-protein interaction network such as STRING View Source def filter_one_to_many ( hgnc_to_ensps_map , G ) : \"\"\" Return a map from HGNC symbol to one ENSP. The one is chosen to be the first ENSP in G. ENSPs are examined in sorted order. Parameters ---------- hgnc_to_ensps_map : dict return value of map_hgnc_to_ensps copy_map : dict map hgnc to the number of ENSP in the map that also appear in G G : nx.Graph protein-protein interaction network such as STRING \"\"\" rv = {} copy_map = {} for hgnc , ensps in hgnc_to_ensps_map . items () : ensps = sorted ( ensps ) any_ensp_in_G = False for ensp in ensps : if ensp in G : any_ensp_in_G = True if hgnc in rv : # then this is the second ENSP in G if hgnc in copy_map : copy_map [ hgnc ] += 1 else : copy_map [ hgnc ] = 2 else : rv [ hgnc ] = ensp if not any_ensp_in_G : # then use the first ensp in sorted order if len ( ensps ) > 0 : rv [ hgnc ] = ensps [ 0 ] return rv , copy_map","title":"Parameters"},{"location":"reference/prmf/ensembl/#map_ensp_to_hgnc","text":"def map_ensp_to_hgnc ( fh ) View Source def map_ensp_to_hgnc ( fh ) : rv = {} for line in fh : line = line . rstrip () words = line . split ( '\\t' ) hgnc_symbol = words [ 0 ] ensp_id = None if ( len ( words ) >= 4 ) : ensp_id = words [ 3 ] if ensp_id is not None : if ensp_id in rv : sys . stderr . write ( \"Encountered ENSP id {} more than once\\n\" . format ( ensp_id )) rv [ ensp_id ] = hgnc_symbol return rv","title":"map_ensp_to_hgnc"},{"location":"reference/prmf/ensembl/#map_hgnc_to_ensps","text":"def map_hgnc_to_ensps ( fh ) Return map from HGNC symbol to many ENSPs","title":"map_hgnc_to_ensps"},{"location":"reference/prmf/ensembl/#parameters_3","text":"fh : file-like database file from dl_ensembl_map: 4 column TSV of HGNC, ENSG, ENST, ENSP","title":"Parameters"},{"location":"reference/prmf/ensembl/#returns_2","text":"rv : dict mapping of HGNC symbol to a set of ENSPs View Source def map_hgnc_to_ensps ( fh ): \"\"\" Return map from HGNC symbol to many ENSPs Parameters ---------- fh : file-like database file from dl_ensembl_map: 4 column TSV of HGNC, ENSG, ENST, ENSP Returns ------- rv : dict mapping of HGNC symbol to a set of ENSPs \"\"\" return parse_mapping_tsv ( fh , key_index = 0 , value_index = 3 )","title":"Returns"},{"location":"reference/prmf/ensembl/#parse_mapping_tsv","text":"def parse_mapping_tsv ( fh , key_index = 0 , value_index = 1 , delim = ' \\t ' ) Parse a TSV file like the file at ftp://ftp.ensembl.org/pub/release-94/tsv/homo_sapiens/Homo_sapiens.GRCh38.94.refseq.tsv.gz into a mapping View Source def parse_mapping_tsv ( fh , key_index = 0 , value_index = 1 , delim = '\\t' ) : \"\"\" Parse a TSV file like the file at ftp://ftp.ensembl.org/pub/release-94/tsv/homo_sapiens/Homo_sapiens.GRCh38.94.refseq.tsv.gz into a mapping \"\"\" rv = {} if type ( fh ) is str : fh = open ( fh , 'r' ) for line in fh : line = line . rstrip () words = line . split ( delim ) # some values for key_index may be missing , skip those entries in the file if key_index >= len ( words ) : continue key = words [ key_index ] value = None if ( len ( words ) > value_index ) : value = words [ value_index ] else : # TODO warn ? pass if value is not None : if key not in rv : rv [ key ] = set () rv [ key ] . add ( value ) return rv","title":"parse_mapping_tsv"},{"location":"reference/prmf/image_processing/","text":"Module prmf.image_processing View Source OLIVETTI_SHAPE = ( 64 , 64 ) def plot_olivetti_components ( plt , images , title = 'Components' ) : \"\"\" Parameters ---------- images : list of np.array \"\"\" gallery_row = 2 gallery_col = 3 image_shape = OLIVETTI_SHAPE plt . figure ( figsize = ( 2. * gallery_col , 2.26 * gallery_row )) plt . suptitle ( title , size = 16 ) for i , comp in enumerate ( images ) : plt . subplot ( gallery_row , gallery_col , i + 1 ) vmax = max ( comp . max (), - comp . min ()) plt . imshow ( comp . reshape ( image_shape ), cmap = plt . cm . gray , interpolation = 'nearest' , vmin =- vmax , vmax = vmax ) plt . xticks (()) plt . yticks (()) plt . subplots_adjust ( 0.01 , 0.05 , 0.99 , 0.93 , 0.04 , 0. ) def pixel_similarity ( n_row , n_col , i , j ) : x_i = i // n_row y_i = i % n_row x_j = j // n_row y_j = j % n_row dist = math . sqrt (( x_i - x_j ) ** 2 + ( y_i - y_j ) ** 2 ) return dist def pixel_to_index ( n_col , i , j ) : # Convert from pixel coordinates to vector coordinates return i * n_col + j def pixel_prior ( mat ) : \"\"\" In images, we expect the pixel intensity of nearby pixels to be only slightly different from the neighboring pixels \"\"\" G_pixel = nx . Graph () n_row , n_col = mat . shape for i , j in it . product ( range ( n_row ), range ( n_col )) : pixel_v = mat [ i,j ] neighbors = get_neighbors ( n_row , n_col , i , j ) for neighbor in neighbors : k_1 = pixel_to_index ( n_col , i , j ) k_2 = pixel_to_index ( n_col , * neighbor ) G_pixel . add_edge ( k_1 , k_2 , { 'weight' : 1 } ) return G_pixel def pixel_nn_prior ( mat , percentile = 50 ) : \"\"\" Identify the neighboring pixel with the most similar intensity \"\"\" G_nn = nx . Graph () n_row , n_col = mat . shape vec = mat . flatten () percentile_v = np . percentile ( vec , percentile ) stdev = np . std ( vec ) for i , j in it . product ( range ( n_row ), range ( n_col )) : pixel_v = mat [ i,j ] if ( pixel_v < percentile_v ) : continue neighbors = get_neighbors ( n_row , n_col , i , j ) best = None best_sim = 0 for neighbor in neighbors : neigh_v = mat [ neighbor[0 ] , neighbor [ 1 ] ] sim = rbf_similarity ( stdev , pixel_v , neigh_v ) if neigh_v > percentile_v and sim > best_sim : best = neighbor best_sim = sim if best is not None : k_1 = pixel_to_index ( n_col , i , j ) k_2 = pixel_to_index ( n_col , * best ) G_nn . add_edge ( k_1 , k_2 , { 'weight' : best_sim } ) return G_nn def vec_to_graph_comb ( vec , percentile = 50 , n_neighbors = 1 , n_row = 64 , n_col = 64 ) : \"\"\" Construct network that is a combination of nearest_neighbors in the image space and pixel intensity similarity among those neighbors \"\"\" mat = vec . reshape (( n_row , n_col )) G_pixel = pixel_prior ( mat ) G_nn = pixel_nn_prior ( mat , percentile ) G = combine_graphs ( [ G_pixel, G_nn ] , [ 0.5, 0.5 ] ) return G def vec_to_graph_prim ( vec , percentile = 50 , n_neighbors = 1 , n_row = 64 , n_col = 64 ) : \"\"\" Primitive version which only examines the neighboring pixels; always 1-NN \"\"\" G = nx . Graph () def pixel_to_index ( i , j ) : # Convert from pixel coordinates to vector coordinates return i * n_col + j stdev = np . std ( vec ) mat = vec . reshape (( n_row , n_col )) percentile_v = np . percentile ( vec , percentile ) for i , j in it . product ( range ( n_row ), range ( n_col )) : neighbors = [] pixel_v = mat [ i,j ] if ( pixel_v < percentile_v ) : continue if i != 0 : neighbors . append (( i - 1 , j )) if i != n_row - 1 : neighbors . append (( i + 1 , j )) if j != 0 : neighbors . append (( i , j - 1 )) if j != n_col - 1 : neighbors . append (( i , j + 1 )) best = None best_dist = 0 for neighbor in neighbors : neigh_v = mat [ neighbor[0 ] , neighbor [ 1 ] ] dist = rbf_similarity ( stdev , pixel_v , neigh_v ) if neigh_v > percentile_v and dist > best_dist : best = neighbor best_dist = dist if best is not None : k_1 = pixel_to_index ( i , j ) k_2 = pixel_to_index ( * best ) G . add_edge ( k_1 , k_2 , { 'weight' : best_dist } ) return G def get_neighbors ( n_row , n_col , i , j ) : \"\"\" Get neighboring pixel coordinates \"\"\" neighbors = [] if i != 0 : neighbors . append (( i - 1 , j )) if i != n_row - 1 : neighbors . append (( i + 1 , j )) if j != 0 : neighbors . append (( i , j - 1 )) if j != n_col - 1 : neighbors . append (( i , j + 1 )) return neighbors # TODO n_row and n_col def vec_to_graph ( vec , percentile = 50 , n_neighbors = 1 , n_row = 64 , n_col = 64 ) : vec = vec . flatten () stdev = np . std ( vec ) percentile_v = np . percentile ( vec , percentile ) def rbf_sim ( x , y ) : return rbf_similarity ( stdev , x , y ) def sim ( vec , i , j , alpha = 0.5 ) : return alpha * rbf_sim ( vec [ i ] , vec [ j ] ) + ( 1 - alpha ) * pixel_similarity ( n_row , n_col , i , j ) G = nx . Graph () # TODO slow implementation for i , j in it . combinations ( range ( vec . shape [ 0 ] ), 2 ) : # < percentile > percent edge weight threshold if ( vec [ i ] > percentile_v and vec [ j ] > percentile_v ) : G . add_edge ( i , j , { 'weight' : sim ( vec , i , j , alpha = 0.1 ) } ) G_prime = transform_nearest_neighbors ( G , n_neighbors ) return G_prime Variables OLIVETTI_SHAPE Functions get_neighbors def get_neighbors ( n_row , n_col , i , j ) Get neighboring pixel coordinates View Source def get_neighbors ( n_row , n_col , i , j ): \"\"\" Get neighboring pixel coordinates \"\"\" neighbors = [] if i != 0 : neighbors . append (( i - 1 , j )) if i != n_row - 1 : neighbors . append (( i + 1 , j )) if j != 0 : neighbors . append (( i , j - 1 )) if j != n_col - 1 : neighbors . append (( i , j + 1 )) return neighbors pixel_nn_prior def pixel_nn_prior ( mat , percentile = 50 ) Identify the neighboring pixel with the most similar intensity View Source def pixel_nn_prior ( mat , percentile = 50 ): \"\"\" Identify the neighboring pixel with the most similar intensity \"\"\" G_nn = nx . Graph () n_row , n_col = mat . shape vec = mat . flatten () percentile_v = np . percentile ( vec , percentile ) stdev = np . std ( vec ) for i , j in it . product ( range ( n_row ), range ( n_col )): pixel_v = mat [ i , j ] if ( pixel_v < percentile_v ): continue neighbors = get_neighbors ( n_row , n_col , i , j ) best = None best_sim = 0 for neighbor in neighbors : neigh_v = mat [ neighbor [ 0 ], neighbor [ 1 ]] sim = rbf_similarity ( stdev , pixel_v , neigh_v ) if neigh_v > percentile_v and sim > best_sim : best = neighbor best_sim = sim if best is not None : k_1 = pixel_to_index ( n_col , i , j ) k_2 = pixel_to_index ( n_col , * best ) G_nn . add_edge ( k_1 , k_2 , { 'weight' : best_sim } ) return G_nn pixel_prior def pixel_prior ( mat ) In images, we expect the pixel intensity of nearby pixels to be only slightly different from the neighboring pixels View Source def pixel_prior ( mat ): \"\"\" In images, we expect the pixel intensity of nearby pixels to be only slightly different from the neighboring pixels \"\"\" G_pixel = nx . Graph () n_row , n_col = mat . shape for i , j in it . product ( range ( n_row ), range ( n_col )): pixel_v = mat [ i , j ] neighbors = get_neighbors ( n_row , n_col , i , j ) for neighbor in neighbors : k_1 = pixel_to_index ( n_col , i , j ) k_2 = pixel_to_index ( n_col , * neighbor ) G_pixel . add_edge ( k_1 , k_2 , { 'weight' : 1 } ) return G_pixel pixel_similarity def pixel_similarity ( n_row , n_col , i , j ) View Source def pixel_similarity ( n_row , n_col , i , j ): x_i = i // n_row y_i = i % n_row x_j = j // n_row y_j = j % n_row dist = math . sqrt (( x_i - x_j ) ** 2 + ( y_i - y_j ) ** 2 ) return dist pixel_to_index def pixel_to_index ( n_col , i , j ) View Source def pixel_to_index ( n_col , i , j ): # Convert from pixel coordinates to vector coordinates return i * n_col + j plot_olivetti_components def plot_olivetti_components ( plt , images , title = 'Components' ) Parameters images : list of np.array View Source def plot_olivetti_components ( plt , images , title = 'Components' ): \"\"\" Parameters ---------- images : list of np.array \"\"\" gallery_row = 2 gallery_col = 3 image_shape = OLIVETTI_SHAPE plt . figure ( figsize = ( 2 . * gallery_col , 2 . 26 * gallery_row )) plt . suptitle ( title , size = 16 ) for i , comp in enumerate ( images ): plt . subplot ( gallery_row , gallery_col , i + 1 ) vmax = max ( comp . max (), - comp . min ()) plt . imshow ( comp . reshape ( image_shape ), cmap = plt . cm . gray , interpolation = 'nearest' , vmin =- vmax , vmax = vmax ) plt . xticks (()) plt . yticks (()) plt . subplots_adjust ( 0 . 01 , 0 . 05 , 0 . 99 , 0 . 93 , 0 . 04 , 0 .) vec_to_graph def vec_to_graph ( vec , percentile = 50 , n_neighbors = 1 , n_row = 64 , n_col = 64 ) View Source def vec_to_graph ( vec , percentile = 50 , n_neighbors = 1 , n_row = 64 , n_col = 64 ) : vec = vec . flatten () stdev = np . std ( vec ) percentile_v = np . percentile ( vec , percentile ) def rbf_sim ( x , y ) : return rbf_similarity ( stdev , x , y ) def sim ( vec , i , j , alpha = 0.5 ) : return alpha * rbf_sim ( vec [ i ] , vec [ j ] ) + ( 1 - alpha ) * pixel_similarity ( n_row , n_col , i , j ) G = nx . Graph () # TODO slow implementation for i , j in it . combinations ( range ( vec . shape [ 0 ] ), 2 ) : # < percentile > percent edge weight threshold if ( vec [ i ] > percentile_v and vec [ j ] > percentile_v ) : G . add_edge ( i , j , { 'weight' : sim ( vec , i , j , alpha = 0.1 ) } ) G_prime = transform_nearest_neighbors ( G , n_neighbors ) return G_prime vec_to_graph_comb def vec_to_graph_comb ( vec , percentile = 50 , n_neighbors = 1 , n_row = 64 , n_col = 64 ) Construct network that is a combination of nearest_neighbors in the image space and pixel intensity similarity among those neighbors View Source def vec_to_graph_comb ( vec , percentile = 50 , n_neighbors = 1 , n_row = 64 , n_col = 64 ): \"\"\" Construct network that is a combination of nearest_neighbors in the image space and pixel intensity similarity among those neighbors \"\"\" mat = vec . reshape (( n_row , n_col )) G_pixel = pixel_prior ( mat ) G_nn = pixel_nn_prior ( mat , percentile ) G = combine_graphs ([ G_pixel , G_nn ], [ 0 . 5 , 0 . 5 ]) return G vec_to_graph_prim def vec_to_graph_prim ( vec , percentile = 50 , n_neighbors = 1 , n_row = 64 , n_col = 64 ) Primitive version which only examines the neighboring pixels; always 1-NN View Source def vec_to_graph_prim ( vec , percentile = 50 , n_neighbors = 1 , n_row = 64 , n_col = 64 ): \"\"\" Primitive version which only examines the neighboring pixels; always 1-NN \"\"\" G = nx . Graph () def pixel_to_index ( i , j ): # Convert from pixel coordinates to vector coordinates return i * n_col + j stdev = np . std ( vec ) mat = vec . reshape (( n_row , n_col )) percentile_v = np . percentile ( vec , percentile ) for i , j in it . product ( range ( n_row ), range ( n_col )): neighbors = [] pixel_v = mat [ i , j ] if ( pixel_v < percentile_v ): continue if i != 0 : neighbors . append (( i - 1 , j )) if i != n_row - 1 : neighbors . append (( i + 1 , j )) if j != 0 : neighbors . append (( i , j - 1 )) if j != n_col - 1 : neighbors . append (( i , j + 1 )) best = None best_dist = 0 for neighbor in neighbors : neigh_v = mat [ neighbor [ 0 ], neighbor [ 1 ]] dist = rbf_similarity ( stdev , pixel_v , neigh_v ) if neigh_v > percentile_v and dist > best_dist : best = neighbor best_dist = dist if best is not None : k_1 = pixel_to_index ( i , j ) k_2 = pixel_to_index ( * best ) G . add_edge ( k_1 , k_2 , { 'weight' : best_dist } ) return G","title":"Image Processing"},{"location":"reference/prmf/image_processing/#module-prmfimage_processing","text":"View Source OLIVETTI_SHAPE = ( 64 , 64 ) def plot_olivetti_components ( plt , images , title = 'Components' ) : \"\"\" Parameters ---------- images : list of np.array \"\"\" gallery_row = 2 gallery_col = 3 image_shape = OLIVETTI_SHAPE plt . figure ( figsize = ( 2. * gallery_col , 2.26 * gallery_row )) plt . suptitle ( title , size = 16 ) for i , comp in enumerate ( images ) : plt . subplot ( gallery_row , gallery_col , i + 1 ) vmax = max ( comp . max (), - comp . min ()) plt . imshow ( comp . reshape ( image_shape ), cmap = plt . cm . gray , interpolation = 'nearest' , vmin =- vmax , vmax = vmax ) plt . xticks (()) plt . yticks (()) plt . subplots_adjust ( 0.01 , 0.05 , 0.99 , 0.93 , 0.04 , 0. ) def pixel_similarity ( n_row , n_col , i , j ) : x_i = i // n_row y_i = i % n_row x_j = j // n_row y_j = j % n_row dist = math . sqrt (( x_i - x_j ) ** 2 + ( y_i - y_j ) ** 2 ) return dist def pixel_to_index ( n_col , i , j ) : # Convert from pixel coordinates to vector coordinates return i * n_col + j def pixel_prior ( mat ) : \"\"\" In images, we expect the pixel intensity of nearby pixels to be only slightly different from the neighboring pixels \"\"\" G_pixel = nx . Graph () n_row , n_col = mat . shape for i , j in it . product ( range ( n_row ), range ( n_col )) : pixel_v = mat [ i,j ] neighbors = get_neighbors ( n_row , n_col , i , j ) for neighbor in neighbors : k_1 = pixel_to_index ( n_col , i , j ) k_2 = pixel_to_index ( n_col , * neighbor ) G_pixel . add_edge ( k_1 , k_2 , { 'weight' : 1 } ) return G_pixel def pixel_nn_prior ( mat , percentile = 50 ) : \"\"\" Identify the neighboring pixel with the most similar intensity \"\"\" G_nn = nx . Graph () n_row , n_col = mat . shape vec = mat . flatten () percentile_v = np . percentile ( vec , percentile ) stdev = np . std ( vec ) for i , j in it . product ( range ( n_row ), range ( n_col )) : pixel_v = mat [ i,j ] if ( pixel_v < percentile_v ) : continue neighbors = get_neighbors ( n_row , n_col , i , j ) best = None best_sim = 0 for neighbor in neighbors : neigh_v = mat [ neighbor[0 ] , neighbor [ 1 ] ] sim = rbf_similarity ( stdev , pixel_v , neigh_v ) if neigh_v > percentile_v and sim > best_sim : best = neighbor best_sim = sim if best is not None : k_1 = pixel_to_index ( n_col , i , j ) k_2 = pixel_to_index ( n_col , * best ) G_nn . add_edge ( k_1 , k_2 , { 'weight' : best_sim } ) return G_nn def vec_to_graph_comb ( vec , percentile = 50 , n_neighbors = 1 , n_row = 64 , n_col = 64 ) : \"\"\" Construct network that is a combination of nearest_neighbors in the image space and pixel intensity similarity among those neighbors \"\"\" mat = vec . reshape (( n_row , n_col )) G_pixel = pixel_prior ( mat ) G_nn = pixel_nn_prior ( mat , percentile ) G = combine_graphs ( [ G_pixel, G_nn ] , [ 0.5, 0.5 ] ) return G def vec_to_graph_prim ( vec , percentile = 50 , n_neighbors = 1 , n_row = 64 , n_col = 64 ) : \"\"\" Primitive version which only examines the neighboring pixels; always 1-NN \"\"\" G = nx . Graph () def pixel_to_index ( i , j ) : # Convert from pixel coordinates to vector coordinates return i * n_col + j stdev = np . std ( vec ) mat = vec . reshape (( n_row , n_col )) percentile_v = np . percentile ( vec , percentile ) for i , j in it . product ( range ( n_row ), range ( n_col )) : neighbors = [] pixel_v = mat [ i,j ] if ( pixel_v < percentile_v ) : continue if i != 0 : neighbors . append (( i - 1 , j )) if i != n_row - 1 : neighbors . append (( i + 1 , j )) if j != 0 : neighbors . append (( i , j - 1 )) if j != n_col - 1 : neighbors . append (( i , j + 1 )) best = None best_dist = 0 for neighbor in neighbors : neigh_v = mat [ neighbor[0 ] , neighbor [ 1 ] ] dist = rbf_similarity ( stdev , pixel_v , neigh_v ) if neigh_v > percentile_v and dist > best_dist : best = neighbor best_dist = dist if best is not None : k_1 = pixel_to_index ( i , j ) k_2 = pixel_to_index ( * best ) G . add_edge ( k_1 , k_2 , { 'weight' : best_dist } ) return G def get_neighbors ( n_row , n_col , i , j ) : \"\"\" Get neighboring pixel coordinates \"\"\" neighbors = [] if i != 0 : neighbors . append (( i - 1 , j )) if i != n_row - 1 : neighbors . append (( i + 1 , j )) if j != 0 : neighbors . append (( i , j - 1 )) if j != n_col - 1 : neighbors . append (( i , j + 1 )) return neighbors # TODO n_row and n_col def vec_to_graph ( vec , percentile = 50 , n_neighbors = 1 , n_row = 64 , n_col = 64 ) : vec = vec . flatten () stdev = np . std ( vec ) percentile_v = np . percentile ( vec , percentile ) def rbf_sim ( x , y ) : return rbf_similarity ( stdev , x , y ) def sim ( vec , i , j , alpha = 0.5 ) : return alpha * rbf_sim ( vec [ i ] , vec [ j ] ) + ( 1 - alpha ) * pixel_similarity ( n_row , n_col , i , j ) G = nx . Graph () # TODO slow implementation for i , j in it . combinations ( range ( vec . shape [ 0 ] ), 2 ) : # < percentile > percent edge weight threshold if ( vec [ i ] > percentile_v and vec [ j ] > percentile_v ) : G . add_edge ( i , j , { 'weight' : sim ( vec , i , j , alpha = 0.1 ) } ) G_prime = transform_nearest_neighbors ( G , n_neighbors ) return G_prime","title":"Module prmf.image_processing"},{"location":"reference/prmf/image_processing/#variables","text":"OLIVETTI_SHAPE","title":"Variables"},{"location":"reference/prmf/image_processing/#functions","text":"","title":"Functions"},{"location":"reference/prmf/image_processing/#get_neighbors","text":"def get_neighbors ( n_row , n_col , i , j ) Get neighboring pixel coordinates View Source def get_neighbors ( n_row , n_col , i , j ): \"\"\" Get neighboring pixel coordinates \"\"\" neighbors = [] if i != 0 : neighbors . append (( i - 1 , j )) if i != n_row - 1 : neighbors . append (( i + 1 , j )) if j != 0 : neighbors . append (( i , j - 1 )) if j != n_col - 1 : neighbors . append (( i , j + 1 )) return neighbors","title":"get_neighbors"},{"location":"reference/prmf/image_processing/#pixel_nn_prior","text":"def pixel_nn_prior ( mat , percentile = 50 ) Identify the neighboring pixel with the most similar intensity View Source def pixel_nn_prior ( mat , percentile = 50 ): \"\"\" Identify the neighboring pixel with the most similar intensity \"\"\" G_nn = nx . Graph () n_row , n_col = mat . shape vec = mat . flatten () percentile_v = np . percentile ( vec , percentile ) stdev = np . std ( vec ) for i , j in it . product ( range ( n_row ), range ( n_col )): pixel_v = mat [ i , j ] if ( pixel_v < percentile_v ): continue neighbors = get_neighbors ( n_row , n_col , i , j ) best = None best_sim = 0 for neighbor in neighbors : neigh_v = mat [ neighbor [ 0 ], neighbor [ 1 ]] sim = rbf_similarity ( stdev , pixel_v , neigh_v ) if neigh_v > percentile_v and sim > best_sim : best = neighbor best_sim = sim if best is not None : k_1 = pixel_to_index ( n_col , i , j ) k_2 = pixel_to_index ( n_col , * best ) G_nn . add_edge ( k_1 , k_2 , { 'weight' : best_sim } ) return G_nn","title":"pixel_nn_prior"},{"location":"reference/prmf/image_processing/#pixel_prior","text":"def pixel_prior ( mat ) In images, we expect the pixel intensity of nearby pixels to be only slightly different from the neighboring pixels View Source def pixel_prior ( mat ): \"\"\" In images, we expect the pixel intensity of nearby pixels to be only slightly different from the neighboring pixels \"\"\" G_pixel = nx . Graph () n_row , n_col = mat . shape for i , j in it . product ( range ( n_row ), range ( n_col )): pixel_v = mat [ i , j ] neighbors = get_neighbors ( n_row , n_col , i , j ) for neighbor in neighbors : k_1 = pixel_to_index ( n_col , i , j ) k_2 = pixel_to_index ( n_col , * neighbor ) G_pixel . add_edge ( k_1 , k_2 , { 'weight' : 1 } ) return G_pixel","title":"pixel_prior"},{"location":"reference/prmf/image_processing/#pixel_similarity","text":"def pixel_similarity ( n_row , n_col , i , j ) View Source def pixel_similarity ( n_row , n_col , i , j ): x_i = i // n_row y_i = i % n_row x_j = j // n_row y_j = j % n_row dist = math . sqrt (( x_i - x_j ) ** 2 + ( y_i - y_j ) ** 2 ) return dist","title":"pixel_similarity"},{"location":"reference/prmf/image_processing/#pixel_to_index","text":"def pixel_to_index ( n_col , i , j ) View Source def pixel_to_index ( n_col , i , j ): # Convert from pixel coordinates to vector coordinates return i * n_col + j","title":"pixel_to_index"},{"location":"reference/prmf/image_processing/#plot_olivetti_components","text":"def plot_olivetti_components ( plt , images , title = 'Components' )","title":"plot_olivetti_components"},{"location":"reference/prmf/image_processing/#parameters","text":"images : list of np.array View Source def plot_olivetti_components ( plt , images , title = 'Components' ): \"\"\" Parameters ---------- images : list of np.array \"\"\" gallery_row = 2 gallery_col = 3 image_shape = OLIVETTI_SHAPE plt . figure ( figsize = ( 2 . * gallery_col , 2 . 26 * gallery_row )) plt . suptitle ( title , size = 16 ) for i , comp in enumerate ( images ): plt . subplot ( gallery_row , gallery_col , i + 1 ) vmax = max ( comp . max (), - comp . min ()) plt . imshow ( comp . reshape ( image_shape ), cmap = plt . cm . gray , interpolation = 'nearest' , vmin =- vmax , vmax = vmax ) plt . xticks (()) plt . yticks (()) plt . subplots_adjust ( 0 . 01 , 0 . 05 , 0 . 99 , 0 . 93 , 0 . 04 , 0 .)","title":"Parameters"},{"location":"reference/prmf/image_processing/#vec_to_graph","text":"def vec_to_graph ( vec , percentile = 50 , n_neighbors = 1 , n_row = 64 , n_col = 64 ) View Source def vec_to_graph ( vec , percentile = 50 , n_neighbors = 1 , n_row = 64 , n_col = 64 ) : vec = vec . flatten () stdev = np . std ( vec ) percentile_v = np . percentile ( vec , percentile ) def rbf_sim ( x , y ) : return rbf_similarity ( stdev , x , y ) def sim ( vec , i , j , alpha = 0.5 ) : return alpha * rbf_sim ( vec [ i ] , vec [ j ] ) + ( 1 - alpha ) * pixel_similarity ( n_row , n_col , i , j ) G = nx . Graph () # TODO slow implementation for i , j in it . combinations ( range ( vec . shape [ 0 ] ), 2 ) : # < percentile > percent edge weight threshold if ( vec [ i ] > percentile_v and vec [ j ] > percentile_v ) : G . add_edge ( i , j , { 'weight' : sim ( vec , i , j , alpha = 0.1 ) } ) G_prime = transform_nearest_neighbors ( G , n_neighbors ) return G_prime","title":"vec_to_graph"},{"location":"reference/prmf/image_processing/#vec_to_graph_comb","text":"def vec_to_graph_comb ( vec , percentile = 50 , n_neighbors = 1 , n_row = 64 , n_col = 64 ) Construct network that is a combination of nearest_neighbors in the image space and pixel intensity similarity among those neighbors View Source def vec_to_graph_comb ( vec , percentile = 50 , n_neighbors = 1 , n_row = 64 , n_col = 64 ): \"\"\" Construct network that is a combination of nearest_neighbors in the image space and pixel intensity similarity among those neighbors \"\"\" mat = vec . reshape (( n_row , n_col )) G_pixel = pixel_prior ( mat ) G_nn = pixel_nn_prior ( mat , percentile ) G = combine_graphs ([ G_pixel , G_nn ], [ 0 . 5 , 0 . 5 ]) return G","title":"vec_to_graph_comb"},{"location":"reference/prmf/image_processing/#vec_to_graph_prim","text":"def vec_to_graph_prim ( vec , percentile = 50 , n_neighbors = 1 , n_row = 64 , n_col = 64 ) Primitive version which only examines the neighboring pixels; always 1-NN View Source def vec_to_graph_prim ( vec , percentile = 50 , n_neighbors = 1 , n_row = 64 , n_col = 64 ): \"\"\" Primitive version which only examines the neighboring pixels; always 1-NN \"\"\" G = nx . Graph () def pixel_to_index ( i , j ): # Convert from pixel coordinates to vector coordinates return i * n_col + j stdev = np . std ( vec ) mat = vec . reshape (( n_row , n_col )) percentile_v = np . percentile ( vec , percentile ) for i , j in it . product ( range ( n_row ), range ( n_col )): neighbors = [] pixel_v = mat [ i , j ] if ( pixel_v < percentile_v ): continue if i != 0 : neighbors . append (( i - 1 , j )) if i != n_row - 1 : neighbors . append (( i + 1 , j )) if j != 0 : neighbors . append (( i , j - 1 )) if j != n_col - 1 : neighbors . append (( i , j + 1 )) best = None best_dist = 0 for neighbor in neighbors : neigh_v = mat [ neighbor [ 0 ], neighbor [ 1 ]] dist = rbf_similarity ( stdev , pixel_v , neigh_v ) if neigh_v > percentile_v and dist > best_dist : best = neighbor best_dist = dist if best is not None : k_1 = pixel_to_index ( i , j ) k_2 = pixel_to_index ( * best ) G . add_edge ( k_1 , k_2 , { 'weight' : best_dist } ) return G","title":"vec_to_graph_prim"},{"location":"reference/prmf/intersect/","text":"Module prmf.intersect Functions related to analysis of intersection of manifold regularizors and iterative updates relating to the intersections View Source \"\"\" Functions related to analysis of intersection of manifold regularizors and iterative updates relating to the intersections \"\"\" import numpy as np import matplotlib.pyplot as plt def is_critical_update ( x1 , x2 , c ): \"\"\" Return True if x2 is on the other side of a critical point c than x1 \"\"\" dim = x1 . shape [ 0 ] delta = x1 - x2 slopes = np . ones ( dim ) for i in range ( 1 , dim ): slopes [ i ] = delta [ i ] / delta [ 0 ] slopes_neg_recip = - 1 * np . divide ( np . ones ( dim ), slopes ) # return vector of length <dim> def perp_line ( t ): return c + ( t - c [ 0 ]) * slopes_neg_recip x1_perp = perp_line ( x1 [ 0 ]) x2_perp = perp_line ( x2 [ 0 ]) x1_bool = ( x1 < x1_perp ) x2_bool = ( x2 < x2_perp ) x1_and_x2 = np . logical_and ( x1_bool , x2_bool ) rv = np . all ( x1_and_x2 ) return rv Functions is_critical_update def is_critical_update ( x1 , x2 , c ) Return True if x2 is on the other side of a critical point c than x1 View Source def is_critical_update ( x1 , x2 , c ) : \"\"\" Return True if x2 is on the other side of a critical point c than x1 \"\"\" dim = x1 . shape [ 0 ] delta = x1 - x2 slopes = np . ones ( dim ) for i in range ( 1 , dim ) : slopes [ i ] = delta [ i ] / delta [ 0 ] slopes_neg_recip = - 1 * np . divide ( np . ones ( dim ), slopes ) # return vector of length < dim > def perp_line ( t ) : return c + ( t - c [ 0 ] ) * slopes_neg_recip x1_perp = perp_line ( x1 [ 0 ] ) x2_perp = perp_line ( x2 [ 0 ] ) x1_bool = ( x1 < x1_perp ) x2_bool = ( x2 < x2_perp ) x1_and_x2 = np . logical_and ( x1_bool , x2_bool ) rv = np . all ( x1_and_x2 ) return rv","title":"Intersect"},{"location":"reference/prmf/intersect/#module-prmfintersect","text":"Functions related to analysis of intersection of manifold regularizors and iterative updates relating to the intersections View Source \"\"\" Functions related to analysis of intersection of manifold regularizors and iterative updates relating to the intersections \"\"\" import numpy as np import matplotlib.pyplot as plt def is_critical_update ( x1 , x2 , c ): \"\"\" Return True if x2 is on the other side of a critical point c than x1 \"\"\" dim = x1 . shape [ 0 ] delta = x1 - x2 slopes = np . ones ( dim ) for i in range ( 1 , dim ): slopes [ i ] = delta [ i ] / delta [ 0 ] slopes_neg_recip = - 1 * np . divide ( np . ones ( dim ), slopes ) # return vector of length <dim> def perp_line ( t ): return c + ( t - c [ 0 ]) * slopes_neg_recip x1_perp = perp_line ( x1 [ 0 ]) x2_perp = perp_line ( x2 [ 0 ]) x1_bool = ( x1 < x1_perp ) x2_bool = ( x2 < x2_perp ) x1_and_x2 = np . logical_and ( x1_bool , x2_bool ) rv = np . all ( x1_and_x2 ) return rv","title":"Module prmf.intersect"},{"location":"reference/prmf/intersect/#functions","text":"","title":"Functions"},{"location":"reference/prmf/intersect/#is_critical_update","text":"def is_critical_update ( x1 , x2 , c ) Return True if x2 is on the other side of a critical point c than x1 View Source def is_critical_update ( x1 , x2 , c ) : \"\"\" Return True if x2 is on the other side of a critical point c than x1 \"\"\" dim = x1 . shape [ 0 ] delta = x1 - x2 slopes = np . ones ( dim ) for i in range ( 1 , dim ) : slopes [ i ] = delta [ i ] / delta [ 0 ] slopes_neg_recip = - 1 * np . divide ( np . ones ( dim ), slopes ) # return vector of length < dim > def perp_line ( t ) : return c + ( t - c [ 0 ] ) * slopes_neg_recip x1_perp = perp_line ( x1 [ 0 ] ) x2_perp = perp_line ( x2 [ 0 ] ) x1_bool = ( x1 < x1_perp ) x2_bool = ( x2 < x2_perp ) x1_and_x2 = np . logical_and ( x1_bool , x2_bool ) rv = np . all ( x1_and_x2 ) return rv","title":"is_critical_update"},{"location":"reference/prmf/plot/","text":"Module prmf.plot View Source from . import * import networkx as nx import numpy as np import matplotlib as mpl import matplotlib.pyplot as plt import matplotlib.colors as colors # https://www.infobyip.com/detectmonitordpi.php DPI = 96 LINE_WIDTH = 2.0 LABEL_FONT_SIZE = 'xx-large' TICK_FONT_SIZE = 'x-large' def plot_vec ( data , ax , aspect = 3 , yticklabels = None , vmin = None , vmax = None , log = False , column_index = 0 , n_columns = 1 ): shape_tpl = data . shape data = data . reshape (( shape_tpl [ 0 ], 1 )) if vmin is None : vmin = np . min ( data ) if vmax is None : vmax = np . max ( data ) # create labels centered at each heatmap block row_range = np . arange ( shape_tpl [ 0 ]) ax . get_xaxis () . set_visible ( False ) ax . set_yticks ( row_range + 0.5 ) if yticklabels is None : yticklabels = map ( lambda x : str ( x ), row_range ) ax . set_yticklabels ( yticklabels , fontsize = TICK_FONT_SIZE ) if column_index == 0 : ax . set_ylabel ( \"Node\" , fontsize = LABEL_FONT_SIZE ) ax . set_title ( \"Scores\" , fontsize = LABEL_FONT_SIZE ) # only plot colorbar if column_index is the last column: n_columns - 1 colorbar = None if log : mappable = ax . pcolor ( data , cmap = plt . cm . Reds , edgecolors = 'k' , linewidth = LINE_WIDTH , norm = colors . LogNorm ( vmin = vmin , vmax = vmax ), vmin = vmin , vmax = vmax ) if column_index == n_columns - 1 : colorbar = plt . colorbar ( mappable , ax = ax , orientation = 'horizontal' , ticks = [ vmin , vmax ], pad = 0.05 , fraction = 0.08 , aspect = aspect , format = mpl . ticker . LogFormatter (), use_gridspec = True ) colorbar . set_ticks ([ vmin , vmax ]) colorbar . set_ticklabels ([ ' {:.0e} ' . format ( vmin ), ' {:.0e} ' . format ( vmax )]) else : mappable = ax . pcolor ( data , cmap = plt . cm . Reds , edgecolors = 'k' , linewidth = LINE_WIDTH , vmin = vmin , vmax = vmax ) if column_index == n_columns - 1 : colorbar = plt . colorbar ( mappable , ax = ax , orientation = 'horizontal' , ticks = [ vmin , vmax ], pad = 0.05 , fraction = 0.08 , aspect = aspect , use_gridspec = True ) if column_index == n_columns - 1 : colorbar . ax . tick_params ( labelsize = TICK_FONT_SIZE ) def plot_graph ( G , ax , pos = None , colormap = None , title = '' , title_fontsize = LABEL_FONT_SIZE , title_y = 1.0 , vmin = None , vmax = None , ** kwargs ): labels = {} for node in G . nodes (): labels [ node ] = str ( node ) ax . get_xaxis () . set_visible ( False ) ax . get_yaxis () . set_visible ( False ) ax . set_title ( title , fontsize = title_fontsize , y = title_y ) if pos is None : pos = nx . spring_layout ( G , k = 1 / np . sqrt ( G . order ()), iterations = 200 ) # draw scatter manually to add node borders xs = [] ys = [] for node_id , center in pos . items (): xs . append ( center [ 0 ]) ys . append ( center [ 1 ]) nx . draw_networkx_edges ( G , pos , ax = ax , width = LINE_WIDTH ) # choose a light red for contrast with black rgba = plt . cm . Reds ( 0.4 ) ax . scatter ( xs , ys , s = 600 , c = rgba , marker = 'o' , edgecolor = 'black' , linewidth = 2.0 , alpha = 1.0 ) nx . draw_networkx_labels ( G , pos , ax = ax , labels = labels , font_size = TICK_FONT_SIZE ) return pos def plot_pathway_interactors ( G , pathway_node , fig , ax , data , nodelist , title = None , title_fontsize = LABEL_FONT_SIZE , title_y = 1.0 , colormap = None , vmin = None , vmax = None , pos = None , ** kwargs ): if pathway_node not in G : raise FactorLibException ( \"pathway_node must be a node in G\" ) if colormap is None : colormap = plt . cm . Reds ax . get_xaxis () . set_visible ( False ) ax . get_yaxis () . set_visible ( False ) ax . axis ( 'off' ) if vmin is None : vmin = np . min ( data ) if vmax is None : vmax = np . max ( data ) x0 , y0 , x1 , y1 = ax . get_position () . get_points () . flatten () gs = mpl . gridspec . GridSpec ( 1 , 2 , width_ratios = [ 1 , 5 ], left = x0 , bottom = y0 , right = x1 , top = y1 ) ax_1 = plt . Subplot ( fig , gs [ 0 ]) ax_2 = plt . Subplot ( fig , gs [ 1 ]) fig . add_subplot ( ax_1 ) fig . add_subplot ( ax_2 ) # - {{ ax_1 heatmap # use node identifiers as heatmap labels but use node indices as graph labels because they dont fit well # associate node number with the identifier node_to_label = {} for i , node in enumerate ( nodelist ): node_to_label [ node ] = str ( i ) node_to_label [ pathway_node ] = pathway_node G = nx . relabel_nodes ( G , node_to_label ) label_list = [] for i , node in enumerate ( nodelist ): label_list . append ( ' {} : {} ' . format ( node , i )) plot_vec ( data , ax_1 , yticklabels = label_list , vmin = vmin , vmax = vmax , log = True ) # ax_1 heatmap }} - # - {{ ax_2 graph ax_2 . get_xaxis () . set_visible ( False ) ax_2 . get_yaxis () . set_visible ( False ) if pos is None : pos = nx . spring_layout ( G , k = 1 / np . sqrt ( G . order ()), iterations = 200 ) nx . draw_networkx_edges ( G , pos , ax = ax_2 , width = LINE_WIDTH ) # draw scatter manually to add node borders xs = [] ys = [] for node_id , center in pos . items (): if node_id != pathway_node : xs . append ( center [ 0 ]) ys . append ( center [ 1 ]) # choose a light red for contrast with black rgba = plt . cm . Reds ( 0.4 ) ax_2 . scatter ( xs , ys , s = 600 , c = rgba , marker = 'o' , edgecolor = 'black' , linewidth = LINE_WIDTH , alpha = 1.0 ) # label all nodes other than pathway_node G . remove_node ( pathway_node ) pathway_node_center = pos . pop ( pathway_node ) nx . draw_networkx_labels ( G , pos , ax = ax_2 , font_size = TICK_FONT_SIZE ) # label pathway_node ax_2 . text ( pathway_node_center [ 0 ], pathway_node_center [ 1 ], pathway_node , horizontalalignment = 'center' , verticalalignment = 'center' , fontsize = LABEL_FONT_SIZE , bbox = dict ( facecolor = rgba , edgecolor = 'black' , linewidth = LINE_WIDTH )) ax_2 . set_title ( title , fontsize = title_fontsize , y = title_y ) # ax_2 graph }} - return pos def plot_latent_and_graph ( G , fig , ax , data = None , nodelist = None , nodelabels = None , column_index = 0 , pos = None , colormap = None , max_col = 30 , log = True , title = '' , title_y = 1.0 , title_fontsize = LABEL_FONT_SIZE , vmin = None , vmax = None , ** kwargs ): \"\"\" Plot a vector and a graph \"\"\" if data is not None and nodelist is None : raise FactorLibException ( \"<data> and <nodelist> must be both set or both None\" ) if colormap is None : colormap = plt . cm . Reds ax . get_xaxis () . set_visible ( False ) ax . get_yaxis () . set_visible ( False ) ax . axis ( 'off' ) if data is None : # generate data to be smooth on graph comps = sorted ( nx . connected_components ( G ), key = lambda x : len ( x ), reverse = True ) comp = list ( comps [ 0 ]) data = np . zeros (( G . order (), 1 )) for node in comp : data [ node ] = 0.6 + random . uniform ( 0.1 , 0.3 ) node = random . choice ( comp ) data [ node ] = 0.95 for comp in comps [ 1 :]: for node in comp : data [ node ] = random . uniform ( 0.1 , 0.3 ) else : # use provided data data = data . reshape (( len ( nodelist ), 1 )) # split ax into 2+ subplots # if there are too many nodes to plot, separate heatmap into multiple pieces n_heatmaps = 1 width_ratios = [ 1 , 5 ] if data . shape [ 0 ] > max_col : n_heatmaps = 2 width_ratios = [ 1 , 1 , 1 , 4 ] n_col = len ( width_ratios ) x0 , y0 , x1 , y1 = ax . get_position () . get_points () . flatten () gs = mpl . gridspec . GridSpec ( 1 , n_col , width_ratios = width_ratios , left = x0 , bottom = y0 , right = x1 , top = y1 ) axes = [] for i in range ( n_heatmaps ): # skip every other ax in gridspec because it is used to make space for heatmap labels j = i * 2 ax = plt . Subplot ( fig , gs [ j ]) fig . add_subplot ( ax ) axes . append ( ax ) # add ax for graph ax = plt . Subplot ( fig , gs [ - 1 ]) fig . add_subplot ( ax ) axes . append ( ax ) # use node identifiers as heatmap labels but use node indices as graph labels because they dont fit well if nodelist is None : nodelist = sorted ( G . nodes ()) node_to_ind = {} for i , node in enumerate ( nodelist ): node_to_ind [ node ] = i G = nx . relabel_nodes ( G , node_to_ind ) if vmin is None : vmin = np . min ( data ) if vmax is None : vmax = np . max ( data ) if nodelabels is None : nodelabels = [] for i , node in enumerate ( nodelist ): nodelabels . append ( ' {} : {} ' . format ( node , i )) for i in range ( n_heatmaps ): data_start = 0 + max_col * i data_end = max_col * ( i + 1 ) data_this = data [ data_start : data_end , 0 ] yticklabels = nodelabels [ data_start : data_end ] plot_vec ( data_this , axes [ i ], yticklabels = yticklabels , vmin = vmin , vmax = vmax , log = log , column_index = i , n_columns = n_heatmaps ) ax = axes [ - 1 ] pos = plot_graph ( G , ax , pos = pos , title = title , fontsize = title_fontsize , title_y = title_y ) return pos def split_title ( title , n_lines = 2 ): title_len = len ( title ) cur_len = 0 new_words = [] words = title . split () for word in words : word_len = len ( word ) new_words . append ( word ) cur_len += word_len if ( cur_len ) >= title_len / float ( n_lines ): new_words . append ( ' \\n ' ) return ' ' . join ( new_words ) Variables DPI LABEL_FONT_SIZE LINE_WIDTH TICK_FONT_SIZE Functions plot_graph def plot_graph ( G , ax , pos = None , colormap = None , title = '' , title_fontsize = 'xx-large' , title_y = 1.0 , vmin = None , vmax = None , ** kwargs ) View Source def plot_graph ( G , ax , pos = None , colormap = None , title = '' , title_fontsize = LABEL_FONT_SIZE , title_y = 1.0 , vmin = None , vmax = None , ** kwargs ) : labels = {} for node in G . nodes () : labels [ node ] = str ( node ) ax . get_xaxis (). set_visible ( False ) ax . get_yaxis (). set_visible ( False ) ax . set_title ( title , fontsize = title_fontsize , y = title_y ) if pos is None : pos = nx . spring_layout ( G , k = 1 / np . sqrt ( G . order ()), iterations = 200 ) # draw scatter manually to add node borders xs = [] ys = [] for node_id , center in pos . items () : xs . append ( center [ 0 ] ) ys . append ( center [ 1 ] ) nx . draw_networkx_edges ( G , pos , ax = ax , width = LINE_WIDTH ) # choose a light red for contrast with black rgba = plt . cm . Reds ( 0.4 ) ax . scatter ( xs , ys , s = 600 , c = rgba , marker = 'o' , edgecolor = 'black' , linewidth = 2.0 , alpha = 1.0 ) nx . draw_networkx_labels ( G , pos , ax = ax , labels = labels , font_size = TICK_FONT_SIZE ) return pos plot_latent_and_graph def plot_latent_and_graph ( G , fig , ax , data = None , nodelist = None , nodelabels = None , column_index = 0 , pos = None , colormap = None , max_col = 30 , log = True , title = '' , title_y = 1.0 , title_fontsize = 'xx-large' , vmin = None , vmax = None , ** kwargs ) Plot a vector and a graph View Source def plot_latent_and_graph ( G , fig , ax , data = None , nodelist = None , nodelabels = None , column_index = 0 , pos = None , colormap = None , max_col = 30 , log = True , title = '' , title_y = 1.0 , title_fontsize = LABEL_FONT_SIZE , vmin = None , vmax = None , ** kwargs ) : \"\"\" Plot a vector and a graph \"\"\" if data is not None and nodelist is None : raise FactorLibException ( \"<data> and <nodelist> must be both set or both None\" ) if colormap is None : colormap = plt . cm . Reds ax . get_xaxis (). set_visible ( False ) ax . get_yaxis (). set_visible ( False ) ax . axis ( 'off' ) if data is None : # generate data to be smooth on graph comps = sorted ( nx . connected_components ( G ), key = lambda x : len ( x ), reverse = True ) comp = list ( comps [ 0 ] ) data = np . zeros (( G . order (), 1 )) for node in comp : data [ node ] = 0.6 + random . uniform ( 0.1 , 0.3 ) node = random . choice ( comp ) data [ node ] = 0.95 for comp in comps [ 1: ] : for node in comp : data [ node ] = random . uniform ( 0.1 , 0.3 ) else : # use provided data data = data . reshape (( len ( nodelist ), 1 )) # split ax into 2 + subplots # if there are too many nodes to plot , separate heatmap into multiple pieces n_heatmaps = 1 width_ratios = [ 1,5 ] if data . shape [ 0 ] > max_col : n_heatmaps = 2 width_ratios = [ 1,1,1,4 ] n_col = len ( width_ratios ) x0 , y0 , x1 , y1 = ax . get_position (). get_points (). flatten () gs = mpl . gridspec . GridSpec ( 1 , n_col , width_ratios = width_ratios , left = x0 , bottom = y0 , right = x1 , top = y1 ) axes = [] for i in range ( n_heatmaps ) : # skip every other ax in gridspec because it is used to make space for heatmap labels j = i * 2 ax = plt . Subplot ( fig , gs [ j ] ) fig . add_subplot ( ax ) axes . append ( ax ) # add ax for graph ax = plt . Subplot ( fig , gs [ -1 ] ) fig . add_subplot ( ax ) axes . append ( ax ) # use node identifiers as heatmap labels but use node indices as graph labels because they dont fit well if nodelist is None : nodelist = sorted ( G . nodes ()) node_to_ind = {} for i , node in enumerate ( nodelist ) : node_to_ind [ node ] = i G = nx . relabel_nodes ( G , node_to_ind ) if vmin is None : vmin = np . min ( data ) if vmax is None : vmax = np . max ( data ) if nodelabels is None : nodelabels = [] for i , node in enumerate ( nodelist ) : nodelabels . append ( '{}: {}' . format ( node , i )) for i in range ( n_heatmaps ) : data_start = 0 + max_col * i data_end = max_col * ( i + 1 ) data_this = data [ data_start:data_end,0 ] yticklabels = nodelabels [ data_start:data_end ] plot_vec ( data_this , axes [ i ] , yticklabels = yticklabels , vmin = vmin , vmax = vmax , log = log , column_index = i , n_columns = n_heatmaps ) ax = axes [ -1 ] pos = plot_graph ( G , ax , pos = pos , title = title , fontsize = title_fontsize , title_y = title_y ) return pos plot_pathway_interactors def plot_pathway_interactors ( G , pathway_node , fig , ax , data , nodelist , title = None , title_fontsize = 'xx-large' , title_y = 1.0 , colormap = None , vmin = None , vmax = None , pos = None , ** kwargs ) View Source def plot_pathway_interactors ( G , pathway_node , fig , ax , data , nodelist , title = None , title_fontsize = LABEL_FONT_SIZE , title_y = 1.0 , colormap = None , vmin = None , vmax = None , pos = None , ** kwargs ) : if pathway_node not in G : raise FactorLibException ( \"pathway_node must be a node in G\" ) if colormap is None : colormap = plt . cm . Reds ax . get_xaxis (). set_visible ( False ) ax . get_yaxis (). set_visible ( False ) ax . axis ( 'off' ) if vmin is None : vmin = np . min ( data ) if vmax is None : vmax = np . max ( data ) x0 , y0 , x1 , y1 = ax . get_position (). get_points (). flatten () gs = mpl . gridspec . GridSpec ( 1 , 2 , width_ratios =[ 1,5 ] , left = x0 , bottom = y0 , right = x1 , top = y1 ) ax_1 = plt . Subplot ( fig , gs [ 0 ] ) ax_2 = plt . Subplot ( fig , gs [ 1 ] ) fig . add_subplot ( ax_1 ) fig . add_subplot ( ax_2 ) # - {{ ax_1 heatmap # use node identifiers as heatmap labels but use node indices as graph labels because they dont fit well # associate node number with the identifier node_to_label = {} for i , node in enumerate ( nodelist ) : node_to_label [ node ] = str ( i ) node_to_label [ pathway_node ] = pathway_node G = nx . relabel_nodes ( G , node_to_label ) label_list = [] for i , node in enumerate ( nodelist ) : label_list . append ( '{}: {}' . format ( node , i )) plot_vec ( data , ax_1 , yticklabels = label_list , vmin = vmin , vmax = vmax , log = True ) # ax_1 heatmap }} - # - {{ ax_2 graph ax_2 . get_xaxis (). set_visible ( False ) ax_2 . get_yaxis (). set_visible ( False ) if pos is None : pos = nx . spring_layout ( G , k = 1 / np . sqrt ( G . order ()), iterations = 200 ) nx . draw_networkx_edges ( G , pos , ax = ax_2 , width = LINE_WIDTH ) # draw scatter manually to add node borders xs = [] ys = [] for node_id , center in pos . items () : if node_id != pathway_node : xs . append ( center [ 0 ] ) ys . append ( center [ 1 ] ) # choose a light red for contrast with black rgba = plt . cm . Reds ( 0.4 ) ax_2 . scatter ( xs , ys , s = 600 , c = rgba , marker = 'o' , edgecolor = 'black' , linewidth = LINE_WIDTH , alpha = 1.0 ) # label all nodes other than pathway_node G . remove_node ( pathway_node ) pathway_node_center = pos . pop ( pathway_node ) nx . draw_networkx_labels ( G , pos , ax = ax_2 , font_size = TICK_FONT_SIZE ) # label pathway_node ax_2 . text ( pathway_node_center [ 0 ] , pathway_node_center [ 1 ] , pathway_node , horizontalalignment = 'center' , verticalalignment = 'center' , fontsize = LABEL_FONT_SIZE , bbox = dict ( facecolor = rgba , edgecolor = 'black' , linewidth = LINE_WIDTH )) ax_2 . set_title ( title , fontsize = title_fontsize , y = title_y ) # ax_2 graph }} - return pos plot_vec def plot_vec ( data , ax , aspect = 3 , yticklabels = None , vmin = None , vmax = None , log = False , column_index = 0 , n_columns = 1 ) View Source def plot_vec ( data , ax , aspect = 3 , yticklabels = None , vmin = None , vmax = None , log = False , column_index = 0 , n_columns = 1 ): shape_tpl = data . shape data = data . reshape (( shape_tpl [ 0 ], 1 )) if vmin is None : vmin = np . min ( data ) if vmax is None : vmax = np . max ( data ) # create labels centered at each heatmap block row_range = np . arange ( shape_tpl [ 0 ]) ax . get_xaxis (). set_visible ( False ) ax . set_yticks ( row_range + 0 . 5 ) if yticklabels is None : yticklabels = map ( lambda x : str ( x ), row_range ) ax . set_yticklabels ( yticklabels , fontsize = TICK_FONT_SIZE ) if column_index == 0 : ax . set_ylabel ( \"Node\" , fontsize = LABEL_FONT_SIZE ) ax . set_title ( \"Scores\" , fontsize = LABEL_FONT_SIZE ) # only plot colorbar if column_index is the last column : n_columns - 1 colorbar = None if log : mappable = ax . pcolor ( data , cmap = plt . cm . Reds , edgecolors = 'k' , linewidth = LINE_WIDTH , norm = colors . LogNorm ( vmin = vmin , vmax = vmax ), vmin = vmin , vmax = vmax ) if column_index == n_columns - 1 : colorbar = plt . colorbar ( mappable , ax = ax , orientation = 'horizontal' , ticks = [ vmin , vmax ], pad = 0 . 05 , fraction = 0 . 08 , aspect = aspect , format = mpl . ticker . LogFormatter (), use_gridspec = True ) colorbar . set_ticks ([ vmin , vmax ]) colorbar . set_ticklabels ([ '{:.0e}' . format ( vmin ), '{:.0e}' . format ( vmax )]) else : mappable = ax . pcolor ( data , cmap = plt . cm . Reds , edgecolors = 'k' , linewidth = LINE_WIDTH , vmin = vmin , vmax = vmax ) if column_index == n_columns - 1 : colorbar = plt . colorbar ( mappable , ax = ax , orientation = 'horizontal' , ticks = [ vmin , vmax ], pad = 0 . 05 , fraction = 0 . 08 , aspect = aspect , use_gridspec = True ) if column_index == n_columns - 1 : colorbar . ax . tick_params ( labelsize = TICK_FONT_SIZE ) split_title def split_title ( title , n_lines = 2 ) View Source def split_title ( title , n_lines = 2 ): title_len = len ( title ) cur_len = 0 new_words = [] words = title . split () for word in words : word_len = len ( word ) new_words . append ( word ) cur_len += word_len if ( cur_len ) >= title_len / float ( n_lines ): new_words . append ( '\\n' ) return ' ' . join ( new_words )","title":"Plot"},{"location":"reference/prmf/plot/#module-prmfplot","text":"View Source from . import * import networkx as nx import numpy as np import matplotlib as mpl import matplotlib.pyplot as plt import matplotlib.colors as colors # https://www.infobyip.com/detectmonitordpi.php DPI = 96 LINE_WIDTH = 2.0 LABEL_FONT_SIZE = 'xx-large' TICK_FONT_SIZE = 'x-large' def plot_vec ( data , ax , aspect = 3 , yticklabels = None , vmin = None , vmax = None , log = False , column_index = 0 , n_columns = 1 ): shape_tpl = data . shape data = data . reshape (( shape_tpl [ 0 ], 1 )) if vmin is None : vmin = np . min ( data ) if vmax is None : vmax = np . max ( data ) # create labels centered at each heatmap block row_range = np . arange ( shape_tpl [ 0 ]) ax . get_xaxis () . set_visible ( False ) ax . set_yticks ( row_range + 0.5 ) if yticklabels is None : yticklabels = map ( lambda x : str ( x ), row_range ) ax . set_yticklabels ( yticklabels , fontsize = TICK_FONT_SIZE ) if column_index == 0 : ax . set_ylabel ( \"Node\" , fontsize = LABEL_FONT_SIZE ) ax . set_title ( \"Scores\" , fontsize = LABEL_FONT_SIZE ) # only plot colorbar if column_index is the last column: n_columns - 1 colorbar = None if log : mappable = ax . pcolor ( data , cmap = plt . cm . Reds , edgecolors = 'k' , linewidth = LINE_WIDTH , norm = colors . LogNorm ( vmin = vmin , vmax = vmax ), vmin = vmin , vmax = vmax ) if column_index == n_columns - 1 : colorbar = plt . colorbar ( mappable , ax = ax , orientation = 'horizontal' , ticks = [ vmin , vmax ], pad = 0.05 , fraction = 0.08 , aspect = aspect , format = mpl . ticker . LogFormatter (), use_gridspec = True ) colorbar . set_ticks ([ vmin , vmax ]) colorbar . set_ticklabels ([ ' {:.0e} ' . format ( vmin ), ' {:.0e} ' . format ( vmax )]) else : mappable = ax . pcolor ( data , cmap = plt . cm . Reds , edgecolors = 'k' , linewidth = LINE_WIDTH , vmin = vmin , vmax = vmax ) if column_index == n_columns - 1 : colorbar = plt . colorbar ( mappable , ax = ax , orientation = 'horizontal' , ticks = [ vmin , vmax ], pad = 0.05 , fraction = 0.08 , aspect = aspect , use_gridspec = True ) if column_index == n_columns - 1 : colorbar . ax . tick_params ( labelsize = TICK_FONT_SIZE ) def plot_graph ( G , ax , pos = None , colormap = None , title = '' , title_fontsize = LABEL_FONT_SIZE , title_y = 1.0 , vmin = None , vmax = None , ** kwargs ): labels = {} for node in G . nodes (): labels [ node ] = str ( node ) ax . get_xaxis () . set_visible ( False ) ax . get_yaxis () . set_visible ( False ) ax . set_title ( title , fontsize = title_fontsize , y = title_y ) if pos is None : pos = nx . spring_layout ( G , k = 1 / np . sqrt ( G . order ()), iterations = 200 ) # draw scatter manually to add node borders xs = [] ys = [] for node_id , center in pos . items (): xs . append ( center [ 0 ]) ys . append ( center [ 1 ]) nx . draw_networkx_edges ( G , pos , ax = ax , width = LINE_WIDTH ) # choose a light red for contrast with black rgba = plt . cm . Reds ( 0.4 ) ax . scatter ( xs , ys , s = 600 , c = rgba , marker = 'o' , edgecolor = 'black' , linewidth = 2.0 , alpha = 1.0 ) nx . draw_networkx_labels ( G , pos , ax = ax , labels = labels , font_size = TICK_FONT_SIZE ) return pos def plot_pathway_interactors ( G , pathway_node , fig , ax , data , nodelist , title = None , title_fontsize = LABEL_FONT_SIZE , title_y = 1.0 , colormap = None , vmin = None , vmax = None , pos = None , ** kwargs ): if pathway_node not in G : raise FactorLibException ( \"pathway_node must be a node in G\" ) if colormap is None : colormap = plt . cm . Reds ax . get_xaxis () . set_visible ( False ) ax . get_yaxis () . set_visible ( False ) ax . axis ( 'off' ) if vmin is None : vmin = np . min ( data ) if vmax is None : vmax = np . max ( data ) x0 , y0 , x1 , y1 = ax . get_position () . get_points () . flatten () gs = mpl . gridspec . GridSpec ( 1 , 2 , width_ratios = [ 1 , 5 ], left = x0 , bottom = y0 , right = x1 , top = y1 ) ax_1 = plt . Subplot ( fig , gs [ 0 ]) ax_2 = plt . Subplot ( fig , gs [ 1 ]) fig . add_subplot ( ax_1 ) fig . add_subplot ( ax_2 ) # - {{ ax_1 heatmap # use node identifiers as heatmap labels but use node indices as graph labels because they dont fit well # associate node number with the identifier node_to_label = {} for i , node in enumerate ( nodelist ): node_to_label [ node ] = str ( i ) node_to_label [ pathway_node ] = pathway_node G = nx . relabel_nodes ( G , node_to_label ) label_list = [] for i , node in enumerate ( nodelist ): label_list . append ( ' {} : {} ' . format ( node , i )) plot_vec ( data , ax_1 , yticklabels = label_list , vmin = vmin , vmax = vmax , log = True ) # ax_1 heatmap }} - # - {{ ax_2 graph ax_2 . get_xaxis () . set_visible ( False ) ax_2 . get_yaxis () . set_visible ( False ) if pos is None : pos = nx . spring_layout ( G , k = 1 / np . sqrt ( G . order ()), iterations = 200 ) nx . draw_networkx_edges ( G , pos , ax = ax_2 , width = LINE_WIDTH ) # draw scatter manually to add node borders xs = [] ys = [] for node_id , center in pos . items (): if node_id != pathway_node : xs . append ( center [ 0 ]) ys . append ( center [ 1 ]) # choose a light red for contrast with black rgba = plt . cm . Reds ( 0.4 ) ax_2 . scatter ( xs , ys , s = 600 , c = rgba , marker = 'o' , edgecolor = 'black' , linewidth = LINE_WIDTH , alpha = 1.0 ) # label all nodes other than pathway_node G . remove_node ( pathway_node ) pathway_node_center = pos . pop ( pathway_node ) nx . draw_networkx_labels ( G , pos , ax = ax_2 , font_size = TICK_FONT_SIZE ) # label pathway_node ax_2 . text ( pathway_node_center [ 0 ], pathway_node_center [ 1 ], pathway_node , horizontalalignment = 'center' , verticalalignment = 'center' , fontsize = LABEL_FONT_SIZE , bbox = dict ( facecolor = rgba , edgecolor = 'black' , linewidth = LINE_WIDTH )) ax_2 . set_title ( title , fontsize = title_fontsize , y = title_y ) # ax_2 graph }} - return pos def plot_latent_and_graph ( G , fig , ax , data = None , nodelist = None , nodelabels = None , column_index = 0 , pos = None , colormap = None , max_col = 30 , log = True , title = '' , title_y = 1.0 , title_fontsize = LABEL_FONT_SIZE , vmin = None , vmax = None , ** kwargs ): \"\"\" Plot a vector and a graph \"\"\" if data is not None and nodelist is None : raise FactorLibException ( \"<data> and <nodelist> must be both set or both None\" ) if colormap is None : colormap = plt . cm . Reds ax . get_xaxis () . set_visible ( False ) ax . get_yaxis () . set_visible ( False ) ax . axis ( 'off' ) if data is None : # generate data to be smooth on graph comps = sorted ( nx . connected_components ( G ), key = lambda x : len ( x ), reverse = True ) comp = list ( comps [ 0 ]) data = np . zeros (( G . order (), 1 )) for node in comp : data [ node ] = 0.6 + random . uniform ( 0.1 , 0.3 ) node = random . choice ( comp ) data [ node ] = 0.95 for comp in comps [ 1 :]: for node in comp : data [ node ] = random . uniform ( 0.1 , 0.3 ) else : # use provided data data = data . reshape (( len ( nodelist ), 1 )) # split ax into 2+ subplots # if there are too many nodes to plot, separate heatmap into multiple pieces n_heatmaps = 1 width_ratios = [ 1 , 5 ] if data . shape [ 0 ] > max_col : n_heatmaps = 2 width_ratios = [ 1 , 1 , 1 , 4 ] n_col = len ( width_ratios ) x0 , y0 , x1 , y1 = ax . get_position () . get_points () . flatten () gs = mpl . gridspec . GridSpec ( 1 , n_col , width_ratios = width_ratios , left = x0 , bottom = y0 , right = x1 , top = y1 ) axes = [] for i in range ( n_heatmaps ): # skip every other ax in gridspec because it is used to make space for heatmap labels j = i * 2 ax = plt . Subplot ( fig , gs [ j ]) fig . add_subplot ( ax ) axes . append ( ax ) # add ax for graph ax = plt . Subplot ( fig , gs [ - 1 ]) fig . add_subplot ( ax ) axes . append ( ax ) # use node identifiers as heatmap labels but use node indices as graph labels because they dont fit well if nodelist is None : nodelist = sorted ( G . nodes ()) node_to_ind = {} for i , node in enumerate ( nodelist ): node_to_ind [ node ] = i G = nx . relabel_nodes ( G , node_to_ind ) if vmin is None : vmin = np . min ( data ) if vmax is None : vmax = np . max ( data ) if nodelabels is None : nodelabels = [] for i , node in enumerate ( nodelist ): nodelabels . append ( ' {} : {} ' . format ( node , i )) for i in range ( n_heatmaps ): data_start = 0 + max_col * i data_end = max_col * ( i + 1 ) data_this = data [ data_start : data_end , 0 ] yticklabels = nodelabels [ data_start : data_end ] plot_vec ( data_this , axes [ i ], yticklabels = yticklabels , vmin = vmin , vmax = vmax , log = log , column_index = i , n_columns = n_heatmaps ) ax = axes [ - 1 ] pos = plot_graph ( G , ax , pos = pos , title = title , fontsize = title_fontsize , title_y = title_y ) return pos def split_title ( title , n_lines = 2 ): title_len = len ( title ) cur_len = 0 new_words = [] words = title . split () for word in words : word_len = len ( word ) new_words . append ( word ) cur_len += word_len if ( cur_len ) >= title_len / float ( n_lines ): new_words . append ( ' \\n ' ) return ' ' . join ( new_words )","title":"Module prmf.plot"},{"location":"reference/prmf/plot/#variables","text":"DPI LABEL_FONT_SIZE LINE_WIDTH TICK_FONT_SIZE","title":"Variables"},{"location":"reference/prmf/plot/#functions","text":"","title":"Functions"},{"location":"reference/prmf/plot/#plot_graph","text":"def plot_graph ( G , ax , pos = None , colormap = None , title = '' , title_fontsize = 'xx-large' , title_y = 1.0 , vmin = None , vmax = None , ** kwargs ) View Source def plot_graph ( G , ax , pos = None , colormap = None , title = '' , title_fontsize = LABEL_FONT_SIZE , title_y = 1.0 , vmin = None , vmax = None , ** kwargs ) : labels = {} for node in G . nodes () : labels [ node ] = str ( node ) ax . get_xaxis (). set_visible ( False ) ax . get_yaxis (). set_visible ( False ) ax . set_title ( title , fontsize = title_fontsize , y = title_y ) if pos is None : pos = nx . spring_layout ( G , k = 1 / np . sqrt ( G . order ()), iterations = 200 ) # draw scatter manually to add node borders xs = [] ys = [] for node_id , center in pos . items () : xs . append ( center [ 0 ] ) ys . append ( center [ 1 ] ) nx . draw_networkx_edges ( G , pos , ax = ax , width = LINE_WIDTH ) # choose a light red for contrast with black rgba = plt . cm . Reds ( 0.4 ) ax . scatter ( xs , ys , s = 600 , c = rgba , marker = 'o' , edgecolor = 'black' , linewidth = 2.0 , alpha = 1.0 ) nx . draw_networkx_labels ( G , pos , ax = ax , labels = labels , font_size = TICK_FONT_SIZE ) return pos","title":"plot_graph"},{"location":"reference/prmf/plot/#plot_latent_and_graph","text":"def plot_latent_and_graph ( G , fig , ax , data = None , nodelist = None , nodelabels = None , column_index = 0 , pos = None , colormap = None , max_col = 30 , log = True , title = '' , title_y = 1.0 , title_fontsize = 'xx-large' , vmin = None , vmax = None , ** kwargs ) Plot a vector and a graph View Source def plot_latent_and_graph ( G , fig , ax , data = None , nodelist = None , nodelabels = None , column_index = 0 , pos = None , colormap = None , max_col = 30 , log = True , title = '' , title_y = 1.0 , title_fontsize = LABEL_FONT_SIZE , vmin = None , vmax = None , ** kwargs ) : \"\"\" Plot a vector and a graph \"\"\" if data is not None and nodelist is None : raise FactorLibException ( \"<data> and <nodelist> must be both set or both None\" ) if colormap is None : colormap = plt . cm . Reds ax . get_xaxis (). set_visible ( False ) ax . get_yaxis (). set_visible ( False ) ax . axis ( 'off' ) if data is None : # generate data to be smooth on graph comps = sorted ( nx . connected_components ( G ), key = lambda x : len ( x ), reverse = True ) comp = list ( comps [ 0 ] ) data = np . zeros (( G . order (), 1 )) for node in comp : data [ node ] = 0.6 + random . uniform ( 0.1 , 0.3 ) node = random . choice ( comp ) data [ node ] = 0.95 for comp in comps [ 1: ] : for node in comp : data [ node ] = random . uniform ( 0.1 , 0.3 ) else : # use provided data data = data . reshape (( len ( nodelist ), 1 )) # split ax into 2 + subplots # if there are too many nodes to plot , separate heatmap into multiple pieces n_heatmaps = 1 width_ratios = [ 1,5 ] if data . shape [ 0 ] > max_col : n_heatmaps = 2 width_ratios = [ 1,1,1,4 ] n_col = len ( width_ratios ) x0 , y0 , x1 , y1 = ax . get_position (). get_points (). flatten () gs = mpl . gridspec . GridSpec ( 1 , n_col , width_ratios = width_ratios , left = x0 , bottom = y0 , right = x1 , top = y1 ) axes = [] for i in range ( n_heatmaps ) : # skip every other ax in gridspec because it is used to make space for heatmap labels j = i * 2 ax = plt . Subplot ( fig , gs [ j ] ) fig . add_subplot ( ax ) axes . append ( ax ) # add ax for graph ax = plt . Subplot ( fig , gs [ -1 ] ) fig . add_subplot ( ax ) axes . append ( ax ) # use node identifiers as heatmap labels but use node indices as graph labels because they dont fit well if nodelist is None : nodelist = sorted ( G . nodes ()) node_to_ind = {} for i , node in enumerate ( nodelist ) : node_to_ind [ node ] = i G = nx . relabel_nodes ( G , node_to_ind ) if vmin is None : vmin = np . min ( data ) if vmax is None : vmax = np . max ( data ) if nodelabels is None : nodelabels = [] for i , node in enumerate ( nodelist ) : nodelabels . append ( '{}: {}' . format ( node , i )) for i in range ( n_heatmaps ) : data_start = 0 + max_col * i data_end = max_col * ( i + 1 ) data_this = data [ data_start:data_end,0 ] yticklabels = nodelabels [ data_start:data_end ] plot_vec ( data_this , axes [ i ] , yticklabels = yticklabels , vmin = vmin , vmax = vmax , log = log , column_index = i , n_columns = n_heatmaps ) ax = axes [ -1 ] pos = plot_graph ( G , ax , pos = pos , title = title , fontsize = title_fontsize , title_y = title_y ) return pos","title":"plot_latent_and_graph"},{"location":"reference/prmf/plot/#plot_pathway_interactors","text":"def plot_pathway_interactors ( G , pathway_node , fig , ax , data , nodelist , title = None , title_fontsize = 'xx-large' , title_y = 1.0 , colormap = None , vmin = None , vmax = None , pos = None , ** kwargs ) View Source def plot_pathway_interactors ( G , pathway_node , fig , ax , data , nodelist , title = None , title_fontsize = LABEL_FONT_SIZE , title_y = 1.0 , colormap = None , vmin = None , vmax = None , pos = None , ** kwargs ) : if pathway_node not in G : raise FactorLibException ( \"pathway_node must be a node in G\" ) if colormap is None : colormap = plt . cm . Reds ax . get_xaxis (). set_visible ( False ) ax . get_yaxis (). set_visible ( False ) ax . axis ( 'off' ) if vmin is None : vmin = np . min ( data ) if vmax is None : vmax = np . max ( data ) x0 , y0 , x1 , y1 = ax . get_position (). get_points (). flatten () gs = mpl . gridspec . GridSpec ( 1 , 2 , width_ratios =[ 1,5 ] , left = x0 , bottom = y0 , right = x1 , top = y1 ) ax_1 = plt . Subplot ( fig , gs [ 0 ] ) ax_2 = plt . Subplot ( fig , gs [ 1 ] ) fig . add_subplot ( ax_1 ) fig . add_subplot ( ax_2 ) # - {{ ax_1 heatmap # use node identifiers as heatmap labels but use node indices as graph labels because they dont fit well # associate node number with the identifier node_to_label = {} for i , node in enumerate ( nodelist ) : node_to_label [ node ] = str ( i ) node_to_label [ pathway_node ] = pathway_node G = nx . relabel_nodes ( G , node_to_label ) label_list = [] for i , node in enumerate ( nodelist ) : label_list . append ( '{}: {}' . format ( node , i )) plot_vec ( data , ax_1 , yticklabels = label_list , vmin = vmin , vmax = vmax , log = True ) # ax_1 heatmap }} - # - {{ ax_2 graph ax_2 . get_xaxis (). set_visible ( False ) ax_2 . get_yaxis (). set_visible ( False ) if pos is None : pos = nx . spring_layout ( G , k = 1 / np . sqrt ( G . order ()), iterations = 200 ) nx . draw_networkx_edges ( G , pos , ax = ax_2 , width = LINE_WIDTH ) # draw scatter manually to add node borders xs = [] ys = [] for node_id , center in pos . items () : if node_id != pathway_node : xs . append ( center [ 0 ] ) ys . append ( center [ 1 ] ) # choose a light red for contrast with black rgba = plt . cm . Reds ( 0.4 ) ax_2 . scatter ( xs , ys , s = 600 , c = rgba , marker = 'o' , edgecolor = 'black' , linewidth = LINE_WIDTH , alpha = 1.0 ) # label all nodes other than pathway_node G . remove_node ( pathway_node ) pathway_node_center = pos . pop ( pathway_node ) nx . draw_networkx_labels ( G , pos , ax = ax_2 , font_size = TICK_FONT_SIZE ) # label pathway_node ax_2 . text ( pathway_node_center [ 0 ] , pathway_node_center [ 1 ] , pathway_node , horizontalalignment = 'center' , verticalalignment = 'center' , fontsize = LABEL_FONT_SIZE , bbox = dict ( facecolor = rgba , edgecolor = 'black' , linewidth = LINE_WIDTH )) ax_2 . set_title ( title , fontsize = title_fontsize , y = title_y ) # ax_2 graph }} - return pos","title":"plot_pathway_interactors"},{"location":"reference/prmf/plot/#plot_vec","text":"def plot_vec ( data , ax , aspect = 3 , yticklabels = None , vmin = None , vmax = None , log = False , column_index = 0 , n_columns = 1 ) View Source def plot_vec ( data , ax , aspect = 3 , yticklabels = None , vmin = None , vmax = None , log = False , column_index = 0 , n_columns = 1 ): shape_tpl = data . shape data = data . reshape (( shape_tpl [ 0 ], 1 )) if vmin is None : vmin = np . min ( data ) if vmax is None : vmax = np . max ( data ) # create labels centered at each heatmap block row_range = np . arange ( shape_tpl [ 0 ]) ax . get_xaxis (). set_visible ( False ) ax . set_yticks ( row_range + 0 . 5 ) if yticklabels is None : yticklabels = map ( lambda x : str ( x ), row_range ) ax . set_yticklabels ( yticklabels , fontsize = TICK_FONT_SIZE ) if column_index == 0 : ax . set_ylabel ( \"Node\" , fontsize = LABEL_FONT_SIZE ) ax . set_title ( \"Scores\" , fontsize = LABEL_FONT_SIZE ) # only plot colorbar if column_index is the last column : n_columns - 1 colorbar = None if log : mappable = ax . pcolor ( data , cmap = plt . cm . Reds , edgecolors = 'k' , linewidth = LINE_WIDTH , norm = colors . LogNorm ( vmin = vmin , vmax = vmax ), vmin = vmin , vmax = vmax ) if column_index == n_columns - 1 : colorbar = plt . colorbar ( mappable , ax = ax , orientation = 'horizontal' , ticks = [ vmin , vmax ], pad = 0 . 05 , fraction = 0 . 08 , aspect = aspect , format = mpl . ticker . LogFormatter (), use_gridspec = True ) colorbar . set_ticks ([ vmin , vmax ]) colorbar . set_ticklabels ([ '{:.0e}' . format ( vmin ), '{:.0e}' . format ( vmax )]) else : mappable = ax . pcolor ( data , cmap = plt . cm . Reds , edgecolors = 'k' , linewidth = LINE_WIDTH , vmin = vmin , vmax = vmax ) if column_index == n_columns - 1 : colorbar = plt . colorbar ( mappable , ax = ax , orientation = 'horizontal' , ticks = [ vmin , vmax ], pad = 0 . 05 , fraction = 0 . 08 , aspect = aspect , use_gridspec = True ) if column_index == n_columns - 1 : colorbar . ax . tick_params ( labelsize = TICK_FONT_SIZE )","title":"plot_vec"},{"location":"reference/prmf/plot/#split_title","text":"def split_title ( title , n_lines = 2 ) View Source def split_title ( title , n_lines = 2 ): title_len = len ( title ) cur_len = 0 new_words = [] words = title . split () for word in words : word_len = len ( word ) new_words . append ( word ) cur_len += word_len if ( cur_len ) >= title_len / float ( n_lines ): new_words . append ( '\\n' ) return ' ' . join ( new_words )","title":"split_title"},{"location":"reference/prmf/prmf_args/","text":"Module prmf.prmf_args Functions used in nmf_pathway.py that are shared by other scripts View Source \"\"\" Functions used in nmf_pathway.py that are shared by other scripts \"\"\" import argparse # TODO move these back to prmf_runner.py, use __init__.py and packaging to reference them in other locations def add_prmf_arguments ( parser ): parser . add_argument ( \"--data\" , type = str , required = True , help = \"n_obs x n_features matrix\" ) parser . add_argument ( \"--manifolds\" , nargs = '+' , help = \"graphml files to use as manifold. Node identifiers must appear in nodelist.\" ) parser . add_argument ( \"--manifolds-file\" , help = \"A file containing newline-delimited filepaths which are used as graphml files as in <manifolds>\" ) parser . add_argument ( \"--manifolds-init\" , nargs = '*' , help = 'If provided, use this list of manifolds to initialize PRMF. If given as a flag or the number of arguments is less than --k-latent, add randomly chosen manifolds to the initialization until we have --k-latent manifolds. If exactly --k-latent files are given, use those to initialize PRMF. If greater than --k-latent files are given, select a sample of size --k-latent from them.' ) parser . add_argument ( \"--node-attribute\" , help = \"Relabel nodes in manifolds/graphs so that their node identifiers come from this node attribute. If None, node identifiers are left as is\" , default = None ) parser . add_argument ( \"--outdir\" , type = str , required = True , help = \"Directory containing results\" ) parser . add_argument ( \"--nodelist\" , type = str , help = \"Association of node identifier to matrix indexes. If not provided, will be inferred from the header in <--data> and will fail if there is no header.\" ) parser . add_argument ( \"--k-latent\" , \"-k\" , default = 6 , help = \"Number of latent factors\" , type = int ) parser . add_argument ( \"--tolerence\" , type = float , default = 1e-3 ) parser . add_argument ( \"--seed\" , default = None ) parser . add_argument ( \"--gamma\" , default = 1.0 , help = \"Tradeoff between reconstruction error and manifold regularization term; Default = 1.0\" , type = float ) parser . add_argument ( \"--delta\" , default = 1.0 , help = \"Regularization parameter for penalty for ignoring manifold; Default = 1.0\" , type = float ) parser . add_argument ( \"--tradeoff\" , default =- 1 , type = float , help = \"If set, use the previous iterations objective function components to automatically update gamma and delta for the next iteration. Must be in [0,1]. Higher values place higher importance on the manifold regularization term over the reconstruction term. To disable automatic updating of gamma and delta, set tradeoff to -1. Default = -1.\" ) parser . add_argument ( \"--high-dimensional\" , default = True , type = bool , help = \"If True, ensure that <data> is of shape m x n with m < n ; otherwise ensure that X is m x n with m > n. Default = True.\" ) parser . add_argument ( \"--no-normalize\" , action = 'store_true' , help = \"If flag is provided, don't quantile normalize the data\" ) parser . add_argument ( \"--delimiter\" , default = \",\" , help = \"Field delimiter in <--data>\" ) parser . add_argument ( \"--m-samples\" , help = \"If provided, only use the first <--m-samples> rows in <--data>; useful for observing script behavior on a smaller dataset\" , type = int ) parser . add_argument ( \"--cross-validation\" , \"-c\" , type = float , help = \"If provided, use the --cross-validation value as a fraction of the samples to hold out and measure model performance with\" ) Functions add_prmf_arguments def add_prmf_arguments ( parser ) View Source def add_prmf_arguments ( parser ): parser . add_argument ( \"--data\" , type = str , required = True , help = \"n_obs x n_features matrix\" ) parser . add_argument ( \"--manifolds\" , nargs = '+' , help = \"graphml files to use as manifold. Node identifiers must appear in nodelist.\" ) parser . add_argument ( \"--manifolds-file\" , help = \"A file containing newline-delimited filepaths which are used as graphml files as in <manifolds>\" ) parser . add_argument ( \"--manifolds-init\" , nargs = '*' , help = 'If provided, use this list of manifolds to initialize PRMF. If given as a flag or the number of arguments is less than --k-latent, add randomly chosen manifolds to the initialization until we have --k-latent manifolds. If exactly --k-latent files are given, use those to initialize PRMF. If greater than --k-latent files are given, select a sample of size --k-latent from them.' ) parser . add_argument ( \"--node-attribute\" , help = \"Relabel nodes in manifolds/graphs so that their node identifiers come from this node attribute. If None, node identifiers are left as is\" , default = None ) parser . add_argument ( \"--outdir\" , type = str , required = True , help = \"Directory containing results\" ) parser . add_argument ( \"--nodelist\" , type = str , help = \"Association of node identifier to matrix indexes. If not provided, will be inferred from the header in <--data> and will fail if there is no header.\" ) parser . add_argument ( \"--k-latent\" , \"-k\" , default = 6 , help = \"Number of latent factors\" , type = int ) parser . add_argument ( \"--tolerence\" , type = float , default = 1 e - 3 ) parser . add_argument ( \"--seed\" , default = None ) parser . add_argument ( \"--gamma\" , default = 1 . 0 , help = \"Tradeoff between reconstruction error and manifold regularization term; Default = 1.0\" , type = float ) parser . add_argument ( \"--delta\" , default = 1 . 0 , help = \"Regularization parameter for penalty for ignoring manifold; Default = 1.0\" , type = float ) parser . add_argument ( \"--tradeoff\" , default =- 1 , type = float , help = \"If set, use the previous iterations objective function components to automatically update gamma and delta for the next iteration. Must be in [0,1]. Higher values place higher importance on the manifold regularization term over the reconstruction term. To disable automatic updating of gamma and delta, set tradeoff to -1. Default = -1.\" ) parser . add_argument ( \"--high-dimensional\" , default = True , type = bool , help = \"If True, ensure that <data> is of shape m x n with m < n ; otherwise ensure that X is m x n with m > n. Default = True.\" ) parser . add_argument ( \"--no-normalize\" , action = 'store_true' , help = \"If flag is provided, don't quantile normalize the data\" ) parser . add_argument ( \"--delimiter\" , default = \",\" , help = \"Field delimiter in <--data>\" ) parser . add_argument ( \"--m-samples\" , help = \"If provided, only use the first <--m-samples> rows in <--data>; useful for observing script behavior on a smaller dataset\" , type = int ) parser . add_argument ( \"--cross-validation\" , \"-c\" , type = float , help = \"If provided, use the --cross-validation value as a fraction of the samples to hold out and measure model performance with\" )","title":"Prmf Args"},{"location":"reference/prmf/prmf_args/#module-prmfprmf_args","text":"Functions used in nmf_pathway.py that are shared by other scripts View Source \"\"\" Functions used in nmf_pathway.py that are shared by other scripts \"\"\" import argparse # TODO move these back to prmf_runner.py, use __init__.py and packaging to reference them in other locations def add_prmf_arguments ( parser ): parser . add_argument ( \"--data\" , type = str , required = True , help = \"n_obs x n_features matrix\" ) parser . add_argument ( \"--manifolds\" , nargs = '+' , help = \"graphml files to use as manifold. Node identifiers must appear in nodelist.\" ) parser . add_argument ( \"--manifolds-file\" , help = \"A file containing newline-delimited filepaths which are used as graphml files as in <manifolds>\" ) parser . add_argument ( \"--manifolds-init\" , nargs = '*' , help = 'If provided, use this list of manifolds to initialize PRMF. If given as a flag or the number of arguments is less than --k-latent, add randomly chosen manifolds to the initialization until we have --k-latent manifolds. If exactly --k-latent files are given, use those to initialize PRMF. If greater than --k-latent files are given, select a sample of size --k-latent from them.' ) parser . add_argument ( \"--node-attribute\" , help = \"Relabel nodes in manifolds/graphs so that their node identifiers come from this node attribute. If None, node identifiers are left as is\" , default = None ) parser . add_argument ( \"--outdir\" , type = str , required = True , help = \"Directory containing results\" ) parser . add_argument ( \"--nodelist\" , type = str , help = \"Association of node identifier to matrix indexes. If not provided, will be inferred from the header in <--data> and will fail if there is no header.\" ) parser . add_argument ( \"--k-latent\" , \"-k\" , default = 6 , help = \"Number of latent factors\" , type = int ) parser . add_argument ( \"--tolerence\" , type = float , default = 1e-3 ) parser . add_argument ( \"--seed\" , default = None ) parser . add_argument ( \"--gamma\" , default = 1.0 , help = \"Tradeoff between reconstruction error and manifold regularization term; Default = 1.0\" , type = float ) parser . add_argument ( \"--delta\" , default = 1.0 , help = \"Regularization parameter for penalty for ignoring manifold; Default = 1.0\" , type = float ) parser . add_argument ( \"--tradeoff\" , default =- 1 , type = float , help = \"If set, use the previous iterations objective function components to automatically update gamma and delta for the next iteration. Must be in [0,1]. Higher values place higher importance on the manifold regularization term over the reconstruction term. To disable automatic updating of gamma and delta, set tradeoff to -1. Default = -1.\" ) parser . add_argument ( \"--high-dimensional\" , default = True , type = bool , help = \"If True, ensure that <data> is of shape m x n with m < n ; otherwise ensure that X is m x n with m > n. Default = True.\" ) parser . add_argument ( \"--no-normalize\" , action = 'store_true' , help = \"If flag is provided, don't quantile normalize the data\" ) parser . add_argument ( \"--delimiter\" , default = \",\" , help = \"Field delimiter in <--data>\" ) parser . add_argument ( \"--m-samples\" , help = \"If provided, only use the first <--m-samples> rows in <--data>; useful for observing script behavior on a smaller dataset\" , type = int ) parser . add_argument ( \"--cross-validation\" , \"-c\" , type = float , help = \"If provided, use the --cross-validation value as a fraction of the samples to hold out and measure model performance with\" )","title":"Module prmf.prmf_args"},{"location":"reference/prmf/prmf_args/#functions","text":"","title":"Functions"},{"location":"reference/prmf/prmf_args/#add_prmf_arguments","text":"def add_prmf_arguments ( parser ) View Source def add_prmf_arguments ( parser ): parser . add_argument ( \"--data\" , type = str , required = True , help = \"n_obs x n_features matrix\" ) parser . add_argument ( \"--manifolds\" , nargs = '+' , help = \"graphml files to use as manifold. Node identifiers must appear in nodelist.\" ) parser . add_argument ( \"--manifolds-file\" , help = \"A file containing newline-delimited filepaths which are used as graphml files as in <manifolds>\" ) parser . add_argument ( \"--manifolds-init\" , nargs = '*' , help = 'If provided, use this list of manifolds to initialize PRMF. If given as a flag or the number of arguments is less than --k-latent, add randomly chosen manifolds to the initialization until we have --k-latent manifolds. If exactly --k-latent files are given, use those to initialize PRMF. If greater than --k-latent files are given, select a sample of size --k-latent from them.' ) parser . add_argument ( \"--node-attribute\" , help = \"Relabel nodes in manifolds/graphs so that their node identifiers come from this node attribute. If None, node identifiers are left as is\" , default = None ) parser . add_argument ( \"--outdir\" , type = str , required = True , help = \"Directory containing results\" ) parser . add_argument ( \"--nodelist\" , type = str , help = \"Association of node identifier to matrix indexes. If not provided, will be inferred from the header in <--data> and will fail if there is no header.\" ) parser . add_argument ( \"--k-latent\" , \"-k\" , default = 6 , help = \"Number of latent factors\" , type = int ) parser . add_argument ( \"--tolerence\" , type = float , default = 1 e - 3 ) parser . add_argument ( \"--seed\" , default = None ) parser . add_argument ( \"--gamma\" , default = 1 . 0 , help = \"Tradeoff between reconstruction error and manifold regularization term; Default = 1.0\" , type = float ) parser . add_argument ( \"--delta\" , default = 1 . 0 , help = \"Regularization parameter for penalty for ignoring manifold; Default = 1.0\" , type = float ) parser . add_argument ( \"--tradeoff\" , default =- 1 , type = float , help = \"If set, use the previous iterations objective function components to automatically update gamma and delta for the next iteration. Must be in [0,1]. Higher values place higher importance on the manifold regularization term over the reconstruction term. To disable automatic updating of gamma and delta, set tradeoff to -1. Default = -1.\" ) parser . add_argument ( \"--high-dimensional\" , default = True , type = bool , help = \"If True, ensure that <data> is of shape m x n with m < n ; otherwise ensure that X is m x n with m > n. Default = True.\" ) parser . add_argument ( \"--no-normalize\" , action = 'store_true' , help = \"If flag is provided, don't quantile normalize the data\" ) parser . add_argument ( \"--delimiter\" , default = \",\" , help = \"Field delimiter in <--data>\" ) parser . add_argument ( \"--m-samples\" , help = \"If provided, only use the first <--m-samples> rows in <--data>; useful for observing script behavior on a smaller dataset\" , type = int ) parser . add_argument ( \"--cross-validation\" , \"-c\" , type = float , help = \"If provided, use the --cross-validation value as a fraction of the samples to hold out and measure model performance with\" )","title":"add_prmf_arguments"},{"location":"reference/prmf/script_utils/","text":"Module prmf.script_utils View Source import os , os.path import sys import subprocess as sp import multiprocessing as mp import networkx as nx import distutils.spawn import hashlib def args_to_list ( args_dict ): rv = [] keys = sorted ( args_dict . keys ()) for k in keys : k_cli = \"--\" + k . replace ( '_' , '-' ) v = args_dict [ k ] if v is not None : if type ( v ) is list : rv . append ( k_cli ) rv = rv + v else : rv . append ( k_cli ) rv . append ( str ( v )) return rv def add_file_and_dir_args ( parser ): parser . add_argument ( \"--infiles\" , nargs = '+' , type = str ) parser . add_argument ( \"--outfiles\" , nargs = '+' , type = str ) parser . add_argument ( \"--indir\" , \"-i\" , type = str ) parser . add_argument ( \"--outdir\" , \"-o\" , type = str ) def check_file_and_dir_args ( args ): # require file mode or directory mode do_file = False do_dir = False do_infiles_outdir = False if args . infiles is not None and args . outfiles is not None : do_file = True if len ( args . infiles ) != len ( args . outfiles ): sys . stderr . write ( \"Must provide the same number of infiles and outfiles \\n \" ) sys . exit ( 23 ) if not do_file : if args . indir is not None and args . outdir is not None : do_dir = True if not do_file and not do_dir : if args . infiles is not None and args . outdir is not None : do_infiles_outdir = True if not do_file and not do_dir and not do_infiles_outdir : sys . stderr . write ( \"Must provide --infiles and --outfiles or provide --indir and --outdir or provide --infiles and --outdir \\n \" ) sys . exit ( 22 ) io_pairs = [] if do_file : io_pairs = list ( zip ( args . infiles , args . outfiles )) elif do_dir : for ifn in os . listdir ( args . indir ): ifp = os . path . join ( args . indir , ifn ) ofp = os . path . join ( args . outdir , ifn ) io_pairs . append (( ifp , ofp )) else : # then do_infiles_outdir for ifp in args . infiles : ofp = os . path . join ( args . outdir , os . path . basename ( ifp )) io_pairs . append (( ifp , ofp )) return io_pairs def run_command ( outdir , cmd , * args , ** kwargs ): \"\"\"Run command and throw error if non-zero exit code Keyword Arguments ----------------- condor : boolean stdout_fh : io-like stderr_fh : io-like TODO ---- change interface to match format_vars \"\"\" if not 'condor' in kwargs : kwargs [ 'condor' ] = False args = [ cmd ] + list ( args ) if ( kwargs [ 'condor' ]): args = [ \"condor_submitter.sh\" ] + args stdout_fh = None if not 'stdout' in kwargs : stdout_fh = open ( os . path . join ( outdir , \"{}.out\" . format ( cmd )), \"w\" ) else : stdout_fh = kwargs [ 'stdout' ] stderr_fh = None if not 'stderr' in kwargs : stderr_fh = open ( os . path . join ( outdir , \"{}.err\" . format ( cmd )), \"w\" ) else : stderr_fh = kwargs [ 'stderr' ] sys . stdout . write ( \"[STATUS] Launching {} \\n \" . format ( str ( args ))) complete_proc = sp . check_call ( args , stdout = stdout_fh , stderr = stderr_fh ) def run_command_cp ( outdir , cmd , * args , ** kwargs ): \"\"\" \"Decorator\" around run_command to enable checkpointing \"\"\" if 'no_checkpoint' in kwargs : kwargs [ 'no_checkpoint' ] = True else : kwargs [ 'no_checkpoint' ] = False if kwargs [ 'no_checkpoint' ]: run_command ( outdir , cmd , * args , ** kwargs ) else : # identify this command and its arguments (TODO and its environment?) with a hash value to_hash = [ cmd ] + list ( args ) hash_v = hash ( \"\" . join ( to_hash )) hash_fp = os . path . join ( outdir , str ( hash_v )) # check if command has previously been run successfully if os . path . exists ( hash_fp ): # if so, dont run again pass else : run_command ( outdir , cmd , * args , ** kwargs ) # if no error from check_call, command successful, write a file indicating this with open ( hash_fp , 'w' ) as fh : fh . write ( \" \\t \" . join ( to_hash )) def get_condor_submit_fp (): \"\"\" TODO ---- allow a different environment variable to specify the location of this file \"\"\" home_dir = os . environ . get ( \"HOME\" ) if ( home_dir is None ): raise ValueError ( \"Required environment variable: HOME\" ) return os . path . join ( home_dir , \".condor\" , \"submitter.sub\" ) def format_vars ( job_id , exe = None , args = [], out = None , err = None , requirements = None , env = None , ** kwargs ): \"\"\" Parameters ------- job_id : str Condor DAG job identifier exe : str executable to run args : list of str arguments to exe out : str filepath to write stdout to err : str filepath to write stderr to requirements : str Condor-style requirements statement e.g. 'OpSysMajorVer == 7' env : str conda environment to execute job in Returns ------- vars_stmt : str \"\"\" VARS_FMT_STR = \"VARS {} executable= \\\" {} \\\" \" if ( exe is None ): raise ValueError ( \"keyword argument \\\" exe \\\" is required\" ) exe_path = get_exe_path ( exe ) exe_condor = exe_path # command condor will run if ( env is not None ): runner_exe = 'prmf_runner.sh' args = [ env , exe_path ] + args exe_condor = get_exe_path ( runner_exe ) vars_stmt = VARS_FMT_STR . format ( job_id , exe_condor , \" \" . join ( args )) if ( len ( args ) > 0 ): vars_stmt += \" arguments= \\\" {} \\\" \" . format ( \" \" . join ( args )) if ( out is not None ): vars_stmt += \" output= \\\" {} \\\" \" . format ( out ) if ( err is not None ): vars_stmt += \" error= \\\" {} \\\" \" . format ( err ) if ( requirements is not None ): vars_stmt += \"requirements = \\\" {} \\\" \" . format ( requirements ) return vars_stmt def get_exe_path ( inpath ): \"\"\" Wrapper around distutils.spawn.find_executable to provide warning if found executable is in cwd \"\"\" exe_path = distutils . spawn . find_executable ( inpath ) if exe_path is None : raise ValueError ( \"desired executable {} is not on the PATH nor is it in the current working directory\" . format ( inpath )) outpath = os . path . abspath ( exe_path ) cwd_exe_path = os . path . abspath ( os . path . join ( os . curdir , inpath )) if ( outpath == cwd_exe_path ): # this exe may surprise some users because find_executable behaves differently from \"which\" in # this respect: find_executable will also include the current working directory in the search # for the executable sys . stderr . write ( \"[warning] found executable {} is in current working directory and may be a different version of {} than found according to the command-line program \\\" which \\\"\\n \" . format ( outpath , cwd_exe_path )) return outpath def job_attrs_to_job_name ( exe = None , args = None , out = None , err = None , ** kwargs ): \"\"\" http://research.cs.wisc.edu/htcondor/manual/v8.7/2_10DAGMan_Applications.html#SECTION003102100000000000000 The JobName can be any string that contains no white space, except for the strings PARENT and CHILD (in upper, lower, or mixed case). JobName also cannot contain special characters ('.', '+') which are reserved for system use. \"\"\" hash_obj = hashlib . sha256 () hash_obj . update ( exe . encode ()) try : hash_obj . update ( \" \" . join ( args ) . encode ()) except TypeError as err : raise ValueError ( \"Invalid args specification for exe {}:\" . format ( exe , str ( err ))) job_name = hash_obj . hexdigest () return job_name def write_condor_dag ( dag_fp , digraph ): \"\"\" See run_digraph \"\"\" JOB_FMT_STR = \"JOB {} {}\" PARENT_FMT_STR = \"PARENT {} CHILD {}\" int_to_chr_offset = ord ( 'A' ) job_int_to_name = {} with open ( dag_fp , \"w\" ) as fh : # get generic condor job description filepath condor_submit_fp = get_condor_submit_fp () job_order = nx . topological_sort ( digraph ) for job_int in job_order : # write JOB declaration job_attrs = digraph . node [ job_int ] job_name = job_attrs_to_job_name ( ** job_attrs ) job_int_to_name [ job_int ] = job_name fh . write ( JOB_FMT_STR . format ( job_name , condor_submit_fp ) + \" \\n \" ) # write VARS declaration fh . write ( format_vars ( job_name , ** job_attrs ) + \" \\n \" ) for edge in digraph . edges_iter (): # write PARENT .. CHILD declaration fh . write ( PARENT_FMT_STR . format ( job_int_to_name [ edge [ 0 ]], job_int_to_name [ edge [ 1 ]]) + \" \\n \" ) def submit_condor_dag ( dag_fp ): # TODO output files go in cwd or location of dag file args = [ \"condor_submit_dag\" , dag_fp ] sp . check_call ( args ) def bfs_nodes ( G , source ): node_list = [] edge_list = list ( nx . bfs_edges ( G , source )) node_list . append ( edge_list [ 0 ][ 0 ]) for edge in edge_list : node_list . append ( edge [ 1 ]) return node_list def run_digraph ( outdir , digraph , condor = False , dry_run = False , root_node = 0 , exit_on_err = True , ** kwargs ): \"\"\" Run a set of jobs specified by a directed (acyclic) graph Parameters ---------- digraph : nx.DiGraph directed graph specifying job execution order; nodes in the graph are assumed to have node attribute 'args' which specifies the executable in args[0] and its arguments in args[1:] and node identifiers in [0..n] condor : bool if True, use condor_submitter.sh to submit jobs dry_run : bool if True, do not submit the DAG exit_on_err : bool if True (and condor False), stop execution of digraph when one of the job nodes fails Returns : TODO ------- job_ids : list of str \"\"\" job_ids = [] if ( condor ): # then create a DAG description file for Condor dag_fp = os . path . join ( outdir , \"digraph.dag\" ) # TODO job_ids? write_condor_dag ( dag_fp , digraph ) if ( not dry_run ): submit_condor_dag ( dag_fp ) else : # TODO even with the pool it appears only 1 process is running pool = mp . Pool ( processes = mp . cpu_count () - 1 ) digraph = digraph . copy () # add proc node attr pointing to a Popen object #job_order = bfs_nodes(digraph, root_node) job_order = nx . topological_sort ( digraph ) env_warned = False for job_id in job_order : job_attrs = digraph . node [ job_id ] if 'env' in job_attrs and not env_warned : sys . stderr . write ( \"[WARNING] one or more jobs have an environment specified, but this functionality has only been implemented for condor \\n \" ) env_warned = True # wait for any predecessors to finish #preds = digraph.predecessors(job_id) #for pred in preds: # digraph.node[pred]['async_result'].wait() #sum_v = 0 #for exit in pred_exits: # sum_v += exit #if(sum_v > 0): # # then a predecessor failed # raise RuntimeError(\"[ERROR] a predecessor to {} failed\".format(digraph.node[job_id]['exe'])) # launch this node's job args = [ job_attrs [ 'exe' ]] + job_attrs [ 'args' ] stdout_fh = None if ( 'out' in job_attrs ): stdout_fh = open ( job_attrs [ 'out' ], 'w' ) else : stdout_fh = sys . stdout stderr_fh = None if ( 'err' in job_attrs ): stderr_fh = open ( job_attrs [ 'err' ], 'w' ) else : stderr_fh = sys . stderr sys . stdout . write ( \"[STATUS] Launching {} > {} 2> {} \\n \" . format ( \" \" . join ([ digraph . node [ job_id ][ 'exe' ]] + digraph . node [ job_id ][ 'args' ]), job_attrs [ 'out' ], job_attrs [ 'err' ])) #def callback_for_job(exit_status): # digraph.node[job_id]['exit'] = exit_status #digraph.node[job_id]['exit'] = None # TODO for some reason providing stdout and stderr breaks everything? #async_result = pool.apply_async(sp.check_call, [args]) #, {'stdout': stdout_fh, 'stderr': stderr_fh}) #digraph.node[job_id]['async_result'] = async_result #async_result.wait() # TODO synchronous only #proc = sp.Popen(args, stdout=stdout_fh, stderr=stderr_fh) if ( not dry_run ): exit_code = - 1 if exit_on_err : exit_code = sp . check_call ( args , stdout = stdout_fh , stderr = stderr_fh ) else : exit_code = sp . call ( args , stdout = stdout_fh , stderr = stderr_fh ) print ( exit_code ) return job_ids def parse_condor_submit_stdout ( stdout ): \"\"\" Returns ------- job_id : str job identifier from condor_submit stdout TODO ---- are there python bindings for Condor that are more stable than parsing stdout? \"\"\" pass def get_revision_number (): \"\"\" Get revision number from git Raises ------ CalledProcessError if git command fails to find the revision number this function will propagate the error: we do not handle this exception because reproducibility is too often overlooked ValueError if environment variable CS799_REPO_DIR is not set \"\"\" env_var = 'PRMF_REPO_DIR' repo_dir = os . environ . get ( env_var ) if repo_dir is None : raise ValueError ( \"Missing environment variable {}\" . format ( env_var )) rev_number = sp . check_output ([ \"get_revision_no.sh\" ]) return rev_number . rstrip () def log_script ( argv ): \"\"\" Parameters ---------- argv : list of str return value of sys.argv \"\"\" sys . stderr . write ( \" \" . join ( argv ) + \" \\n \" ) #sys.stderr.write(\"Revision: {}\\n\".format(get_revision_number())) def mkdir_p ( dirpath ): try : os . makedirs ( dirpath ) except OSError as e : pass def str2bool ( v ): if isinstance ( v , bool ): return v if v . lower () in ( 'yes' , 'true' , 't' , 'y' , '1' ): return True elif v . lower () in ( 'no' , 'false' , 'f' , 'n' , '0' ): return False else : raise argparse . ArgumentTypeError ( 'Boolean value expected.' ) Functions add_file_and_dir_args def add_file_and_dir_args ( parser ) View Source def add_file_and_dir_args ( parser ): parser . add_argument ( \"--infiles\" , nargs = '+' , type = str ) parser . add_argument ( \"--outfiles\" , nargs = '+' , type = str ) parser . add_argument ( \"--indir\" , \"-i\" , type = str ) parser . add_argument ( \"--outdir\" , \"-o\" , type = str ) args_to_list def args_to_list ( args_dict ) View Source def args_to_list ( args_dict ) : rv = [] keys = sorted ( args_dict . keys ()) for k in keys : k_cli = \"--\" + k . replace ( '_' , '-' ) v = args_dict [ k ] if v is not None : if type ( v ) is list : rv . append ( k_cli ) rv = rv + v else : rv . append ( k_cli ) rv . append ( str ( v )) return rv bfs_nodes def bfs_nodes ( G , source ) View Source def bfs_nodes ( G , source ): node_list = [] edge_list = list ( nx . bfs_edges ( G , source )) node_list . append ( edge_list [ 0 ][ 0 ]) for edge in edge_list : node_list . append ( edge [ 1 ]) return node_list check_file_and_dir_args def check_file_and_dir_args ( args ) View Source def check_file_and_dir_args ( args ): # require file mode or directory mode do_file = False do_dir = False do_infiles_outdir = False if args . infiles is not None and args . outfiles is not None : do_file = True if len ( args . infiles ) != len ( args . outfiles ): sys . stderr . write ( \"Must provide the same number of infiles and outfiles\\n\" ) sys . exit ( 23 ) if not do_file : if args . indir is not None and args . outdir is not None : do_dir = True if not do_file and not do_dir : if args . infiles is not None and args . outdir is not None : do_infiles_outdir = True if not do_file and not do_dir and not do_infiles_outdir : sys . stderr . write ( \"Must provide --infiles and --outfiles or provide --indir and --outdir or provide --infiles and --outdir\\n\" ) sys . exit ( 22 ) io_pairs = [] if do_file : io_pairs = list ( zip ( args . infiles , args . outfiles )) elif do_dir : for ifn in os . listdir ( args . indir ): ifp = os . path . join ( args . indir , ifn ) ofp = os . path . join ( args . outdir , ifn ) io_pairs . append (( ifp , ofp )) else : # then do_infiles_outdir for ifp in args . infiles : ofp = os . path . join ( args . outdir , os . path . basename ( ifp )) io_pairs . append (( ifp , ofp )) return io_pairs format_vars def format_vars ( job_id , exe = None , args = [], out = None , err = None , requirements = None , env = None , ** kwargs ) Parameters job_id : str Condor DAG job identifier exe : str executable to run args : list of str arguments to exe out : str filepath to write stdout to err : str filepath to write stderr to requirements : str Condor-style requirements statement e.g. 'OpSysMajorVer == 7' env : str conda environment to execute job in Returns vars_stmt : str View Source def format_vars ( job_id , exe = None , args = [], out = None , err = None , requirements = None , env = None , ** kwargs ): \"\"\" Parameters ------- job_id : str Condor DAG job identifier exe : str executable to run args : list of str arguments to exe out : str filepath to write stdout to err : str filepath to write stderr to requirements : str Condor-style requirements statement e.g. 'OpSysMajorVer == 7' env : str conda environment to execute job in Returns ------- vars_stmt : str \"\"\" VARS_FMT_STR = \"VARS {} executable=\\\" {}\\ \"\" if ( exe is None ): raise ValueError ( \"keyword argument \\\" exe \\ \" is required\" ) exe_path = get_exe_path ( exe ) exe_condor = exe_path # command condor will run if ( env is not None ): runner_exe = 'prmf_runner.sh' args = [ env , exe_path ] + args exe_condor = get_exe_path ( runner_exe ) vars_stmt = VARS_FMT_STR . format ( job_id , exe_condor , \" \" . join ( args )) if ( len ( args ) > 0 ): vars_stmt += \" arguments=\\\" {}\\ \"\" . format ( \" \" . join ( args )) if ( out is not None ): vars_stmt += \" output=\\\" {}\\ \"\" . format ( out ) if ( err is not None ): vars_stmt += \" error=\\\" {}\\ \"\" . format ( err ) if ( requirements is not None ): vars_stmt += \"requirements = \\\" {}\\ \"\" . format ( requirements ) return vars_stmt get_condor_submit_fp def get_condor_submit_fp ( ) TODO allow a different environment variable to specify the location of this file View Source def get_condor_submit_fp (): \"\"\" TODO ---- allow a different environment variable to specify the location of this file \"\"\" home_dir = os . environ . get ( \"HOME\" ) if ( home_dir is None ): raise ValueError ( \"Required environment variable: HOME\" ) return os . path . join ( home_dir , \".condor\" , \"submitter.sub\" ) get_exe_path def get_exe_path ( inpath ) Wrapper around distutils.spawn.find_executable to provide warning if found executable is in cwd View Source def get_exe_path ( inpath ) : \"\"\" Wrapper around distutils.spawn.find_executable to provide warning if found executable is in cwd \"\"\" exe_path = distutils . spawn . find_executable ( inpath ) if exe_path is None : raise ValueError ( \"desired executable {} is not on the PATH nor is it in the current working directory\" . format ( inpath )) outpath = os . path . abspath ( exe_path ) cwd_exe_path = os . path . abspath ( os . path . join ( os . curdir , inpath )) if ( outpath == cwd_exe_path ) : # this exe may surprise some users because find_executable behaves differently from \"which\" in # this respect : find_executable will also include the current working directory in the search # for the executable sys . stderr . write ( \"[warning] found executable {} is in current working directory and may be a different version of {} than found according to the command-line program \\\" which \\ \"\\n\" . format ( outpath , cwd_exe_path )) return outpath get_revision_number def get_revision_number ( ) Get revision number from git Raises CalledProcessError if git command fails to find the revision number this function will propagate the error: we do not handle this exception because reproducibility is too often overlooked ValueError if environment variable CS799_REPO_DIR is not set View Source def get_revision_number (): \"\"\" Get revision number from git Raises ------ CalledProcessError if git command fails to find the revision number this function will propagate the error: we do not handle this exception because reproducibility is too often overlooked ValueError if environment variable CS799_REPO_DIR is not set \"\"\" env_var = 'PRMF_REPO_DIR' repo_dir = os . environ . get ( env_var ) if repo_dir is None : raise ValueError ( \"Missing environment variable {}\" . format ( env_var )) rev_number = sp . check_output ([ \"get_revision_no.sh\" ]) return rev_number . rstrip () job_attrs_to_job_name def job_attrs_to_job_name ( exe = None , args = None , out = None , err = None , ** kwargs ) http://research.cs.wisc.edu/htcondor/manual/v8.7/2_10DAGMan_Applications.html#SECTION003102100000000000000 The JobName can be any string that contains no white space, except for the strings PARENT and CHILD (in upper, lower, or mixed case). JobName also cannot contain special characters ('.', '+') which are reserved for system use. View Source def job_attrs_to_job_name ( exe = None , args = None , out = None , err = None , ** kwargs ): \"\"\" http://research.cs.wisc.edu/htcondor/manual/v8.7/2_10DAGMan_Applications.html#SECTION003102100000000000000 The JobName can be any string that contains no white space, except for the strings PARENT and CHILD (in upper, lower, or mixed case). JobName also cannot contain special characters ('.', '+') which are reserved for system use. \"\"\" hash_obj = hashlib . sha256 () hash_obj . update ( exe . encode ()) try : hash_obj . update ( \" \" . join ( args ). encode ()) except TypeError as err : raise ValueError ( \"Invalid args specification for exe {}:\" . format ( exe , str ( err ))) job_name = hash_obj . hexdigest () return job_name log_script def log_script ( argv ) Parameters argv : list of str return value of sys.argv View Source def log_script ( argv ): \"\"\" Parameters ---------- argv : list of str return value of sys.argv \"\"\" sys . stderr . write ( \" \" . join ( argv ) + \"\\n\" ) mkdir_p def mkdir_p ( dirpath ) View Source def mkdir_p ( dirpath ): try : os . makedirs ( dirpath ) except OSError as e : pass parse_condor_submit_stdout def parse_condor_submit_stdout ( stdout ) Returns job_id : str job identifier from condor_submit stdout TODO are there python bindings for Condor that are more stable than parsing stdout? View Source def parse_condor_submit_stdout ( stdout ): \"\"\" Returns ------- job_id : str job identifier from condor_submit stdout TODO ---- are there python bindings for Condor that are more stable than parsing stdout? \"\"\" pass run_command def run_command ( outdir , cmd , * args , ** kwargs ) Run command and throw error if non-zero exit code Keyword Arguments condor : boolean stdout_fh : io-like stderr_fh : io-like TODO change interface to match format_vars View Source def run_command ( outdir , cmd , * args , ** kwargs ) : \"\"\"Run command and throw error if non-zero exit code Keyword Arguments ----------------- condor : boolean stdout_fh : io-like stderr_fh : io-like TODO ---- change interface to match format_vars \"\"\" if not 'condor' in kwargs : kwargs [ 'condor' ] = False args = [ cmd ] + list ( args ) if ( kwargs [ 'condor' ] ) : args = [ \"condor_submitter.sh\" ] + args stdout_fh = None if not 'stdout' in kwargs : stdout_fh = open ( os . path . join ( outdir , \"{}.out\" . format ( cmd )), \"w\" ) else : stdout_fh = kwargs [ 'stdout' ] stderr_fh = None if not 'stderr' in kwargs : stderr_fh = open ( os . path . join ( outdir , \"{}.err\" . format ( cmd )), \"w\" ) else : stderr_fh = kwargs [ 'stderr' ] sys . stdout . write ( \"[STATUS] Launching {}\\n\" . format ( str ( args ))) complete_proc = sp . check_call ( args , stdout = stdout_fh , stderr = stderr_fh ) run_command_cp def run_command_cp ( outdir , cmd , * args , ** kwargs ) \"Decorator\" around run_command to enable checkpointing View Source def run_command_cp ( outdir , cmd , * args , ** kwargs ) : \"\"\" \" Decorator \" around run_command to enable checkpointing \"\"\" if 'no_checkpoint' in kwargs : kwargs [ 'no_checkpoint' ] = True else : kwargs [ 'no_checkpoint' ] = False if kwargs [ 'no_checkpoint' ] : run_command ( outdir , cmd , * args , ** kwargs ) else : # identify this command and its arguments ( TODO and its environment ? ) with a hash value to_hash = [ cmd ] + list ( args ) hash_v = hash ( \"\" . join ( to_hash )) hash_fp = os . path . join ( outdir , str ( hash_v )) # check if command has previously been run successfully if os . path . exists ( hash_fp ) : # if so , dont run again pass else : run_command ( outdir , cmd , * args , ** kwargs ) # if no error from check_call , command successful , write a file indicating this with open ( hash_fp , 'w' ) as fh : fh . write ( \"\\t\" . join ( to_hash )) run_digraph def run_digraph ( outdir , digraph , condor = False , dry_run = False , root_node = 0 , exit_on_err = True , ** kwargs ) Run a set of jobs specified by a directed (acyclic) graph Parameters digraph : nx.DiGraph directed graph specifying job execution order; nodes in the graph are assumed to have node attribute 'args' which specifies the executable in args[0] and its arguments in args[1:] and node identifiers in [0..n] condor : bool if True, use condor_submitter.sh to submit jobs dry_run : bool if True, do not submit the DAG exit_on_err : bool if True (and condor False), stop execution of digraph when one of the job nodes fails Returns : TODO job_ids : list of str View Source def run_digraph ( outdir , digraph , condor = False , dry_run = False , root_node = 0 , exit_on_err = True , ** kwargs ) : \"\"\" Run a set of jobs specified by a directed (acyclic) graph Parameters ---------- digraph : nx.DiGraph directed graph specifying job execution order; nodes in the graph are assumed to have node attribute 'args' which specifies the executable in args[0] and its arguments in args[1:] and node identifiers in [0..n] condor : bool if True, use condor_submitter.sh to submit jobs dry_run : bool if True, do not submit the DAG exit_on_err : bool if True (and condor False), stop execution of digraph when one of the job nodes fails Returns : TODO ------- job_ids : list of str \"\"\" job_ids = [] if ( condor ) : # then create a DAG description file for Condor dag_fp = os . path . join ( outdir , \"digraph.dag\" ) # TODO job_ids ? write_condor_dag ( dag_fp , digraph ) if ( not dry_run ) : submit_condor_dag ( dag_fp ) else : # TODO even with the pool it appears only 1 process is running pool = mp . Pool ( processes = mp . cpu_count () - 1 ) digraph = digraph . copy () # add proc node attr pointing to a Popen object #job_order = bfs_nodes ( digraph , root_node ) job_order = nx . topological_sort ( digraph ) env_warned = False for job_id in job_order : job_attrs = digraph . node [ job_id ] if 'env' in job_attrs and not env_warned : sys . stderr . write ( \"[WARNING] one or more jobs have an environment specified, but this functionality has only been implemented for condor\\n\" ) env_warned = True # wait for any predecessors to finish #preds = digraph . predecessors ( job_id ) #for pred in preds : # digraph . node [ pred ][ 'async_result' ] . wait () #sum_v = 0 #for exit in pred_exits : # sum_v += exit #if ( sum_v > 0 ) : # # then a predecessor failed # raise RuntimeError ( \"[ERROR] a predecessor to {} failed\" . format ( digraph . node [ job_id ][ 'exe' ] )) # launch this node 's job args = [job_attrs[' exe ']] + job_attrs[' args '] stdout_fh = None if(' out ' in job_attrs): stdout_fh = open(job_attrs[' out '], ' w ') else: stdout_fh = sys.stdout stderr_fh = None if(' err ' in job_attrs): stderr_fh = open(job_attrs[' err '], ' w ') else: stderr_fh = sys.stderr sys.stdout.write(\"[STATUS] Launching {} > {} 2> {}\\n\".format(\" \".join([digraph.node[job_id][' exe ']] + digraph.node[job_id][' args ']), job_attrs[' out '], job_attrs[' err '])) #def callback_for_job(exit_status): # digraph.node[job_id][' exit '] = exit_status #digraph.node[job_id][' exit '] = None # TODO for some reason providing stdout and stderr breaks everything? #async_result = pool.apply_async(sp.check_call, [args]) #, {' stdout ': stdout_fh, ' stderr ': stderr_fh}) #digraph.node[job_id][' async_result '] = async_result #async_result . wait () # TODO synchronous only #proc = sp . Popen ( args , stdout = stdout_fh , stderr = stderr_fh ) if ( not dry_run ) : exit_code = - 1 if exit_on_err : exit_code = sp . check_call ( args , stdout = stdout_fh , stderr = stderr_fh ) else : exit_code = sp . call ( args , stdout = stdout_fh , stderr = stderr_fh ) print ( exit_code ) return job_ids str2bool def str2bool ( v ) View Source def str2bool ( v ): if isinstance ( v , bool ): return v if v . lower () in ( 'yes' , 'true' , 't' , 'y' , '1' ): return True elif v . lower () in ( 'no' , 'false' , 'f' , 'n' , '0' ): return False else : raise argparse . ArgumentTypeError ( 'Boolean value expected.' ) submit_condor_dag def submit_condor_dag ( dag_fp ) View Source def submit_condor_dag ( dag_fp ) : # TODO output files go in cwd or location of dag file args = [ \"condor_submit_dag\", dag_fp ] sp . check_call ( args ) write_condor_dag def write_condor_dag ( dag_fp , digraph ) See run_digraph View Source def write_condor_dag ( dag_fp , digraph ) : \"\"\" See run_digraph \"\"\" JOB_FMT_STR = \"JOB {} {}\" PARENT_FMT_STR = \"PARENT {} CHILD {}\" int_to_chr_offset = ord ( 'A' ) job_int_to_name = {} with open ( dag_fp , \"w\" ) as fh : # get generic condor job description filepath condor_submit_fp = get_condor_submit_fp () job_order = nx . topological_sort ( digraph ) for job_int in job_order : # write JOB declaration job_attrs = digraph . node [ job_int ] job_name = job_attrs_to_job_name ( ** job_attrs ) job_int_to_name [ job_int ] = job_name fh . write ( JOB_FMT_STR . format ( job_name , condor_submit_fp ) + \"\\n\" ) # write VARS declaration fh . write ( format_vars ( job_name , ** job_attrs ) + \"\\n\" ) for edge in digraph . edges_iter () : # write PARENT .. CHILD declaration fh . write ( PARENT_FMT_STR . format ( job_int_to_name [ edge[0 ] ] , job_int_to_name [ edge[1 ] ] ) + \"\\n\" )","title":"Script Utils"},{"location":"reference/prmf/script_utils/#module-prmfscript_utils","text":"View Source import os , os.path import sys import subprocess as sp import multiprocessing as mp import networkx as nx import distutils.spawn import hashlib def args_to_list ( args_dict ): rv = [] keys = sorted ( args_dict . keys ()) for k in keys : k_cli = \"--\" + k . replace ( '_' , '-' ) v = args_dict [ k ] if v is not None : if type ( v ) is list : rv . append ( k_cli ) rv = rv + v else : rv . append ( k_cli ) rv . append ( str ( v )) return rv def add_file_and_dir_args ( parser ): parser . add_argument ( \"--infiles\" , nargs = '+' , type = str ) parser . add_argument ( \"--outfiles\" , nargs = '+' , type = str ) parser . add_argument ( \"--indir\" , \"-i\" , type = str ) parser . add_argument ( \"--outdir\" , \"-o\" , type = str ) def check_file_and_dir_args ( args ): # require file mode or directory mode do_file = False do_dir = False do_infiles_outdir = False if args . infiles is not None and args . outfiles is not None : do_file = True if len ( args . infiles ) != len ( args . outfiles ): sys . stderr . write ( \"Must provide the same number of infiles and outfiles \\n \" ) sys . exit ( 23 ) if not do_file : if args . indir is not None and args . outdir is not None : do_dir = True if not do_file and not do_dir : if args . infiles is not None and args . outdir is not None : do_infiles_outdir = True if not do_file and not do_dir and not do_infiles_outdir : sys . stderr . write ( \"Must provide --infiles and --outfiles or provide --indir and --outdir or provide --infiles and --outdir \\n \" ) sys . exit ( 22 ) io_pairs = [] if do_file : io_pairs = list ( zip ( args . infiles , args . outfiles )) elif do_dir : for ifn in os . listdir ( args . indir ): ifp = os . path . join ( args . indir , ifn ) ofp = os . path . join ( args . outdir , ifn ) io_pairs . append (( ifp , ofp )) else : # then do_infiles_outdir for ifp in args . infiles : ofp = os . path . join ( args . outdir , os . path . basename ( ifp )) io_pairs . append (( ifp , ofp )) return io_pairs def run_command ( outdir , cmd , * args , ** kwargs ): \"\"\"Run command and throw error if non-zero exit code Keyword Arguments ----------------- condor : boolean stdout_fh : io-like stderr_fh : io-like TODO ---- change interface to match format_vars \"\"\" if not 'condor' in kwargs : kwargs [ 'condor' ] = False args = [ cmd ] + list ( args ) if ( kwargs [ 'condor' ]): args = [ \"condor_submitter.sh\" ] + args stdout_fh = None if not 'stdout' in kwargs : stdout_fh = open ( os . path . join ( outdir , \"{}.out\" . format ( cmd )), \"w\" ) else : stdout_fh = kwargs [ 'stdout' ] stderr_fh = None if not 'stderr' in kwargs : stderr_fh = open ( os . path . join ( outdir , \"{}.err\" . format ( cmd )), \"w\" ) else : stderr_fh = kwargs [ 'stderr' ] sys . stdout . write ( \"[STATUS] Launching {} \\n \" . format ( str ( args ))) complete_proc = sp . check_call ( args , stdout = stdout_fh , stderr = stderr_fh ) def run_command_cp ( outdir , cmd , * args , ** kwargs ): \"\"\" \"Decorator\" around run_command to enable checkpointing \"\"\" if 'no_checkpoint' in kwargs : kwargs [ 'no_checkpoint' ] = True else : kwargs [ 'no_checkpoint' ] = False if kwargs [ 'no_checkpoint' ]: run_command ( outdir , cmd , * args , ** kwargs ) else : # identify this command and its arguments (TODO and its environment?) with a hash value to_hash = [ cmd ] + list ( args ) hash_v = hash ( \"\" . join ( to_hash )) hash_fp = os . path . join ( outdir , str ( hash_v )) # check if command has previously been run successfully if os . path . exists ( hash_fp ): # if so, dont run again pass else : run_command ( outdir , cmd , * args , ** kwargs ) # if no error from check_call, command successful, write a file indicating this with open ( hash_fp , 'w' ) as fh : fh . write ( \" \\t \" . join ( to_hash )) def get_condor_submit_fp (): \"\"\" TODO ---- allow a different environment variable to specify the location of this file \"\"\" home_dir = os . environ . get ( \"HOME\" ) if ( home_dir is None ): raise ValueError ( \"Required environment variable: HOME\" ) return os . path . join ( home_dir , \".condor\" , \"submitter.sub\" ) def format_vars ( job_id , exe = None , args = [], out = None , err = None , requirements = None , env = None , ** kwargs ): \"\"\" Parameters ------- job_id : str Condor DAG job identifier exe : str executable to run args : list of str arguments to exe out : str filepath to write stdout to err : str filepath to write stderr to requirements : str Condor-style requirements statement e.g. 'OpSysMajorVer == 7' env : str conda environment to execute job in Returns ------- vars_stmt : str \"\"\" VARS_FMT_STR = \"VARS {} executable= \\\" {} \\\" \" if ( exe is None ): raise ValueError ( \"keyword argument \\\" exe \\\" is required\" ) exe_path = get_exe_path ( exe ) exe_condor = exe_path # command condor will run if ( env is not None ): runner_exe = 'prmf_runner.sh' args = [ env , exe_path ] + args exe_condor = get_exe_path ( runner_exe ) vars_stmt = VARS_FMT_STR . format ( job_id , exe_condor , \" \" . join ( args )) if ( len ( args ) > 0 ): vars_stmt += \" arguments= \\\" {} \\\" \" . format ( \" \" . join ( args )) if ( out is not None ): vars_stmt += \" output= \\\" {} \\\" \" . format ( out ) if ( err is not None ): vars_stmt += \" error= \\\" {} \\\" \" . format ( err ) if ( requirements is not None ): vars_stmt += \"requirements = \\\" {} \\\" \" . format ( requirements ) return vars_stmt def get_exe_path ( inpath ): \"\"\" Wrapper around distutils.spawn.find_executable to provide warning if found executable is in cwd \"\"\" exe_path = distutils . spawn . find_executable ( inpath ) if exe_path is None : raise ValueError ( \"desired executable {} is not on the PATH nor is it in the current working directory\" . format ( inpath )) outpath = os . path . abspath ( exe_path ) cwd_exe_path = os . path . abspath ( os . path . join ( os . curdir , inpath )) if ( outpath == cwd_exe_path ): # this exe may surprise some users because find_executable behaves differently from \"which\" in # this respect: find_executable will also include the current working directory in the search # for the executable sys . stderr . write ( \"[warning] found executable {} is in current working directory and may be a different version of {} than found according to the command-line program \\\" which \\\"\\n \" . format ( outpath , cwd_exe_path )) return outpath def job_attrs_to_job_name ( exe = None , args = None , out = None , err = None , ** kwargs ): \"\"\" http://research.cs.wisc.edu/htcondor/manual/v8.7/2_10DAGMan_Applications.html#SECTION003102100000000000000 The JobName can be any string that contains no white space, except for the strings PARENT and CHILD (in upper, lower, or mixed case). JobName also cannot contain special characters ('.', '+') which are reserved for system use. \"\"\" hash_obj = hashlib . sha256 () hash_obj . update ( exe . encode ()) try : hash_obj . update ( \" \" . join ( args ) . encode ()) except TypeError as err : raise ValueError ( \"Invalid args specification for exe {}:\" . format ( exe , str ( err ))) job_name = hash_obj . hexdigest () return job_name def write_condor_dag ( dag_fp , digraph ): \"\"\" See run_digraph \"\"\" JOB_FMT_STR = \"JOB {} {}\" PARENT_FMT_STR = \"PARENT {} CHILD {}\" int_to_chr_offset = ord ( 'A' ) job_int_to_name = {} with open ( dag_fp , \"w\" ) as fh : # get generic condor job description filepath condor_submit_fp = get_condor_submit_fp () job_order = nx . topological_sort ( digraph ) for job_int in job_order : # write JOB declaration job_attrs = digraph . node [ job_int ] job_name = job_attrs_to_job_name ( ** job_attrs ) job_int_to_name [ job_int ] = job_name fh . write ( JOB_FMT_STR . format ( job_name , condor_submit_fp ) + \" \\n \" ) # write VARS declaration fh . write ( format_vars ( job_name , ** job_attrs ) + \" \\n \" ) for edge in digraph . edges_iter (): # write PARENT .. CHILD declaration fh . write ( PARENT_FMT_STR . format ( job_int_to_name [ edge [ 0 ]], job_int_to_name [ edge [ 1 ]]) + \" \\n \" ) def submit_condor_dag ( dag_fp ): # TODO output files go in cwd or location of dag file args = [ \"condor_submit_dag\" , dag_fp ] sp . check_call ( args ) def bfs_nodes ( G , source ): node_list = [] edge_list = list ( nx . bfs_edges ( G , source )) node_list . append ( edge_list [ 0 ][ 0 ]) for edge in edge_list : node_list . append ( edge [ 1 ]) return node_list def run_digraph ( outdir , digraph , condor = False , dry_run = False , root_node = 0 , exit_on_err = True , ** kwargs ): \"\"\" Run a set of jobs specified by a directed (acyclic) graph Parameters ---------- digraph : nx.DiGraph directed graph specifying job execution order; nodes in the graph are assumed to have node attribute 'args' which specifies the executable in args[0] and its arguments in args[1:] and node identifiers in [0..n] condor : bool if True, use condor_submitter.sh to submit jobs dry_run : bool if True, do not submit the DAG exit_on_err : bool if True (and condor False), stop execution of digraph when one of the job nodes fails Returns : TODO ------- job_ids : list of str \"\"\" job_ids = [] if ( condor ): # then create a DAG description file for Condor dag_fp = os . path . join ( outdir , \"digraph.dag\" ) # TODO job_ids? write_condor_dag ( dag_fp , digraph ) if ( not dry_run ): submit_condor_dag ( dag_fp ) else : # TODO even with the pool it appears only 1 process is running pool = mp . Pool ( processes = mp . cpu_count () - 1 ) digraph = digraph . copy () # add proc node attr pointing to a Popen object #job_order = bfs_nodes(digraph, root_node) job_order = nx . topological_sort ( digraph ) env_warned = False for job_id in job_order : job_attrs = digraph . node [ job_id ] if 'env' in job_attrs and not env_warned : sys . stderr . write ( \"[WARNING] one or more jobs have an environment specified, but this functionality has only been implemented for condor \\n \" ) env_warned = True # wait for any predecessors to finish #preds = digraph.predecessors(job_id) #for pred in preds: # digraph.node[pred]['async_result'].wait() #sum_v = 0 #for exit in pred_exits: # sum_v += exit #if(sum_v > 0): # # then a predecessor failed # raise RuntimeError(\"[ERROR] a predecessor to {} failed\".format(digraph.node[job_id]['exe'])) # launch this node's job args = [ job_attrs [ 'exe' ]] + job_attrs [ 'args' ] stdout_fh = None if ( 'out' in job_attrs ): stdout_fh = open ( job_attrs [ 'out' ], 'w' ) else : stdout_fh = sys . stdout stderr_fh = None if ( 'err' in job_attrs ): stderr_fh = open ( job_attrs [ 'err' ], 'w' ) else : stderr_fh = sys . stderr sys . stdout . write ( \"[STATUS] Launching {} > {} 2> {} \\n \" . format ( \" \" . join ([ digraph . node [ job_id ][ 'exe' ]] + digraph . node [ job_id ][ 'args' ]), job_attrs [ 'out' ], job_attrs [ 'err' ])) #def callback_for_job(exit_status): # digraph.node[job_id]['exit'] = exit_status #digraph.node[job_id]['exit'] = None # TODO for some reason providing stdout and stderr breaks everything? #async_result = pool.apply_async(sp.check_call, [args]) #, {'stdout': stdout_fh, 'stderr': stderr_fh}) #digraph.node[job_id]['async_result'] = async_result #async_result.wait() # TODO synchronous only #proc = sp.Popen(args, stdout=stdout_fh, stderr=stderr_fh) if ( not dry_run ): exit_code = - 1 if exit_on_err : exit_code = sp . check_call ( args , stdout = stdout_fh , stderr = stderr_fh ) else : exit_code = sp . call ( args , stdout = stdout_fh , stderr = stderr_fh ) print ( exit_code ) return job_ids def parse_condor_submit_stdout ( stdout ): \"\"\" Returns ------- job_id : str job identifier from condor_submit stdout TODO ---- are there python bindings for Condor that are more stable than parsing stdout? \"\"\" pass def get_revision_number (): \"\"\" Get revision number from git Raises ------ CalledProcessError if git command fails to find the revision number this function will propagate the error: we do not handle this exception because reproducibility is too often overlooked ValueError if environment variable CS799_REPO_DIR is not set \"\"\" env_var = 'PRMF_REPO_DIR' repo_dir = os . environ . get ( env_var ) if repo_dir is None : raise ValueError ( \"Missing environment variable {}\" . format ( env_var )) rev_number = sp . check_output ([ \"get_revision_no.sh\" ]) return rev_number . rstrip () def log_script ( argv ): \"\"\" Parameters ---------- argv : list of str return value of sys.argv \"\"\" sys . stderr . write ( \" \" . join ( argv ) + \" \\n \" ) #sys.stderr.write(\"Revision: {}\\n\".format(get_revision_number())) def mkdir_p ( dirpath ): try : os . makedirs ( dirpath ) except OSError as e : pass def str2bool ( v ): if isinstance ( v , bool ): return v if v . lower () in ( 'yes' , 'true' , 't' , 'y' , '1' ): return True elif v . lower () in ( 'no' , 'false' , 'f' , 'n' , '0' ): return False else : raise argparse . ArgumentTypeError ( 'Boolean value expected.' )","title":"Module prmf.script_utils"},{"location":"reference/prmf/script_utils/#functions","text":"","title":"Functions"},{"location":"reference/prmf/script_utils/#add_file_and_dir_args","text":"def add_file_and_dir_args ( parser ) View Source def add_file_and_dir_args ( parser ): parser . add_argument ( \"--infiles\" , nargs = '+' , type = str ) parser . add_argument ( \"--outfiles\" , nargs = '+' , type = str ) parser . add_argument ( \"--indir\" , \"-i\" , type = str ) parser . add_argument ( \"--outdir\" , \"-o\" , type = str )","title":"add_file_and_dir_args"},{"location":"reference/prmf/script_utils/#args_to_list","text":"def args_to_list ( args_dict ) View Source def args_to_list ( args_dict ) : rv = [] keys = sorted ( args_dict . keys ()) for k in keys : k_cli = \"--\" + k . replace ( '_' , '-' ) v = args_dict [ k ] if v is not None : if type ( v ) is list : rv . append ( k_cli ) rv = rv + v else : rv . append ( k_cli ) rv . append ( str ( v )) return rv","title":"args_to_list"},{"location":"reference/prmf/script_utils/#bfs_nodes","text":"def bfs_nodes ( G , source ) View Source def bfs_nodes ( G , source ): node_list = [] edge_list = list ( nx . bfs_edges ( G , source )) node_list . append ( edge_list [ 0 ][ 0 ]) for edge in edge_list : node_list . append ( edge [ 1 ]) return node_list","title":"bfs_nodes"},{"location":"reference/prmf/script_utils/#check_file_and_dir_args","text":"def check_file_and_dir_args ( args ) View Source def check_file_and_dir_args ( args ): # require file mode or directory mode do_file = False do_dir = False do_infiles_outdir = False if args . infiles is not None and args . outfiles is not None : do_file = True if len ( args . infiles ) != len ( args . outfiles ): sys . stderr . write ( \"Must provide the same number of infiles and outfiles\\n\" ) sys . exit ( 23 ) if not do_file : if args . indir is not None and args . outdir is not None : do_dir = True if not do_file and not do_dir : if args . infiles is not None and args . outdir is not None : do_infiles_outdir = True if not do_file and not do_dir and not do_infiles_outdir : sys . stderr . write ( \"Must provide --infiles and --outfiles or provide --indir and --outdir or provide --infiles and --outdir\\n\" ) sys . exit ( 22 ) io_pairs = [] if do_file : io_pairs = list ( zip ( args . infiles , args . outfiles )) elif do_dir : for ifn in os . listdir ( args . indir ): ifp = os . path . join ( args . indir , ifn ) ofp = os . path . join ( args . outdir , ifn ) io_pairs . append (( ifp , ofp )) else : # then do_infiles_outdir for ifp in args . infiles : ofp = os . path . join ( args . outdir , os . path . basename ( ifp )) io_pairs . append (( ifp , ofp )) return io_pairs","title":"check_file_and_dir_args"},{"location":"reference/prmf/script_utils/#format_vars","text":"def format_vars ( job_id , exe = None , args = [], out = None , err = None , requirements = None , env = None , ** kwargs )","title":"format_vars"},{"location":"reference/prmf/script_utils/#parameters","text":"job_id : str Condor DAG job identifier exe : str executable to run args : list of str arguments to exe out : str filepath to write stdout to err : str filepath to write stderr to requirements : str Condor-style requirements statement e.g. 'OpSysMajorVer == 7' env : str conda environment to execute job in","title":"Parameters"},{"location":"reference/prmf/script_utils/#returns","text":"vars_stmt : str View Source def format_vars ( job_id , exe = None , args = [], out = None , err = None , requirements = None , env = None , ** kwargs ): \"\"\" Parameters ------- job_id : str Condor DAG job identifier exe : str executable to run args : list of str arguments to exe out : str filepath to write stdout to err : str filepath to write stderr to requirements : str Condor-style requirements statement e.g. 'OpSysMajorVer == 7' env : str conda environment to execute job in Returns ------- vars_stmt : str \"\"\" VARS_FMT_STR = \"VARS {} executable=\\\" {}\\ \"\" if ( exe is None ): raise ValueError ( \"keyword argument \\\" exe \\ \" is required\" ) exe_path = get_exe_path ( exe ) exe_condor = exe_path # command condor will run if ( env is not None ): runner_exe = 'prmf_runner.sh' args = [ env , exe_path ] + args exe_condor = get_exe_path ( runner_exe ) vars_stmt = VARS_FMT_STR . format ( job_id , exe_condor , \" \" . join ( args )) if ( len ( args ) > 0 ): vars_stmt += \" arguments=\\\" {}\\ \"\" . format ( \" \" . join ( args )) if ( out is not None ): vars_stmt += \" output=\\\" {}\\ \"\" . format ( out ) if ( err is not None ): vars_stmt += \" error=\\\" {}\\ \"\" . format ( err ) if ( requirements is not None ): vars_stmt += \"requirements = \\\" {}\\ \"\" . format ( requirements ) return vars_stmt","title":"Returns"},{"location":"reference/prmf/script_utils/#get_condor_submit_fp","text":"def get_condor_submit_fp ( )","title":"get_condor_submit_fp"},{"location":"reference/prmf/script_utils/#todo","text":"allow a different environment variable to specify the location of this file View Source def get_condor_submit_fp (): \"\"\" TODO ---- allow a different environment variable to specify the location of this file \"\"\" home_dir = os . environ . get ( \"HOME\" ) if ( home_dir is None ): raise ValueError ( \"Required environment variable: HOME\" ) return os . path . join ( home_dir , \".condor\" , \"submitter.sub\" )","title":"TODO"},{"location":"reference/prmf/script_utils/#get_exe_path","text":"def get_exe_path ( inpath ) Wrapper around distutils.spawn.find_executable to provide warning if found executable is in cwd View Source def get_exe_path ( inpath ) : \"\"\" Wrapper around distutils.spawn.find_executable to provide warning if found executable is in cwd \"\"\" exe_path = distutils . spawn . find_executable ( inpath ) if exe_path is None : raise ValueError ( \"desired executable {} is not on the PATH nor is it in the current working directory\" . format ( inpath )) outpath = os . path . abspath ( exe_path ) cwd_exe_path = os . path . abspath ( os . path . join ( os . curdir , inpath )) if ( outpath == cwd_exe_path ) : # this exe may surprise some users because find_executable behaves differently from \"which\" in # this respect : find_executable will also include the current working directory in the search # for the executable sys . stderr . write ( \"[warning] found executable {} is in current working directory and may be a different version of {} than found according to the command-line program \\\" which \\ \"\\n\" . format ( outpath , cwd_exe_path )) return outpath","title":"get_exe_path"},{"location":"reference/prmf/script_utils/#get_revision_number","text":"def get_revision_number ( ) Get revision number from git","title":"get_revision_number"},{"location":"reference/prmf/script_utils/#raises","text":"CalledProcessError if git command fails to find the revision number this function will propagate the error: we do not handle this exception because reproducibility is too often overlooked ValueError if environment variable CS799_REPO_DIR is not set View Source def get_revision_number (): \"\"\" Get revision number from git Raises ------ CalledProcessError if git command fails to find the revision number this function will propagate the error: we do not handle this exception because reproducibility is too often overlooked ValueError if environment variable CS799_REPO_DIR is not set \"\"\" env_var = 'PRMF_REPO_DIR' repo_dir = os . environ . get ( env_var ) if repo_dir is None : raise ValueError ( \"Missing environment variable {}\" . format ( env_var )) rev_number = sp . check_output ([ \"get_revision_no.sh\" ]) return rev_number . rstrip ()","title":"Raises"},{"location":"reference/prmf/script_utils/#job_attrs_to_job_name","text":"def job_attrs_to_job_name ( exe = None , args = None , out = None , err = None , ** kwargs ) http://research.cs.wisc.edu/htcondor/manual/v8.7/2_10DAGMan_Applications.html#SECTION003102100000000000000 The JobName can be any string that contains no white space, except for the strings PARENT and CHILD (in upper, lower, or mixed case). JobName also cannot contain special characters ('.', '+') which are reserved for system use. View Source def job_attrs_to_job_name ( exe = None , args = None , out = None , err = None , ** kwargs ): \"\"\" http://research.cs.wisc.edu/htcondor/manual/v8.7/2_10DAGMan_Applications.html#SECTION003102100000000000000 The JobName can be any string that contains no white space, except for the strings PARENT and CHILD (in upper, lower, or mixed case). JobName also cannot contain special characters ('.', '+') which are reserved for system use. \"\"\" hash_obj = hashlib . sha256 () hash_obj . update ( exe . encode ()) try : hash_obj . update ( \" \" . join ( args ). encode ()) except TypeError as err : raise ValueError ( \"Invalid args specification for exe {}:\" . format ( exe , str ( err ))) job_name = hash_obj . hexdigest () return job_name","title":"job_attrs_to_job_name"},{"location":"reference/prmf/script_utils/#log_script","text":"def log_script ( argv )","title":"log_script"},{"location":"reference/prmf/script_utils/#parameters_1","text":"argv : list of str return value of sys.argv View Source def log_script ( argv ): \"\"\" Parameters ---------- argv : list of str return value of sys.argv \"\"\" sys . stderr . write ( \" \" . join ( argv ) + \"\\n\" )","title":"Parameters"},{"location":"reference/prmf/script_utils/#mkdir_p","text":"def mkdir_p ( dirpath ) View Source def mkdir_p ( dirpath ): try : os . makedirs ( dirpath ) except OSError as e : pass","title":"mkdir_p"},{"location":"reference/prmf/script_utils/#parse_condor_submit_stdout","text":"def parse_condor_submit_stdout ( stdout )","title":"parse_condor_submit_stdout"},{"location":"reference/prmf/script_utils/#returns_1","text":"job_id : str job identifier from condor_submit stdout","title":"Returns"},{"location":"reference/prmf/script_utils/#todo_1","text":"are there python bindings for Condor that are more stable than parsing stdout? View Source def parse_condor_submit_stdout ( stdout ): \"\"\" Returns ------- job_id : str job identifier from condor_submit stdout TODO ---- are there python bindings for Condor that are more stable than parsing stdout? \"\"\" pass","title":"TODO"},{"location":"reference/prmf/script_utils/#run_command","text":"def run_command ( outdir , cmd , * args , ** kwargs ) Run command and throw error if non-zero exit code","title":"run_command"},{"location":"reference/prmf/script_utils/#keyword-arguments","text":"condor : boolean stdout_fh : io-like stderr_fh : io-like","title":"Keyword Arguments"},{"location":"reference/prmf/script_utils/#todo_2","text":"change interface to match format_vars View Source def run_command ( outdir , cmd , * args , ** kwargs ) : \"\"\"Run command and throw error if non-zero exit code Keyword Arguments ----------------- condor : boolean stdout_fh : io-like stderr_fh : io-like TODO ---- change interface to match format_vars \"\"\" if not 'condor' in kwargs : kwargs [ 'condor' ] = False args = [ cmd ] + list ( args ) if ( kwargs [ 'condor' ] ) : args = [ \"condor_submitter.sh\" ] + args stdout_fh = None if not 'stdout' in kwargs : stdout_fh = open ( os . path . join ( outdir , \"{}.out\" . format ( cmd )), \"w\" ) else : stdout_fh = kwargs [ 'stdout' ] stderr_fh = None if not 'stderr' in kwargs : stderr_fh = open ( os . path . join ( outdir , \"{}.err\" . format ( cmd )), \"w\" ) else : stderr_fh = kwargs [ 'stderr' ] sys . stdout . write ( \"[STATUS] Launching {}\\n\" . format ( str ( args ))) complete_proc = sp . check_call ( args , stdout = stdout_fh , stderr = stderr_fh )","title":"TODO"},{"location":"reference/prmf/script_utils/#run_command_cp","text":"def run_command_cp ( outdir , cmd , * args , ** kwargs ) \"Decorator\" around run_command to enable checkpointing View Source def run_command_cp ( outdir , cmd , * args , ** kwargs ) : \"\"\" \" Decorator \" around run_command to enable checkpointing \"\"\" if 'no_checkpoint' in kwargs : kwargs [ 'no_checkpoint' ] = True else : kwargs [ 'no_checkpoint' ] = False if kwargs [ 'no_checkpoint' ] : run_command ( outdir , cmd , * args , ** kwargs ) else : # identify this command and its arguments ( TODO and its environment ? ) with a hash value to_hash = [ cmd ] + list ( args ) hash_v = hash ( \"\" . join ( to_hash )) hash_fp = os . path . join ( outdir , str ( hash_v )) # check if command has previously been run successfully if os . path . exists ( hash_fp ) : # if so , dont run again pass else : run_command ( outdir , cmd , * args , ** kwargs ) # if no error from check_call , command successful , write a file indicating this with open ( hash_fp , 'w' ) as fh : fh . write ( \"\\t\" . join ( to_hash ))","title":"run_command_cp"},{"location":"reference/prmf/script_utils/#run_digraph","text":"def run_digraph ( outdir , digraph , condor = False , dry_run = False , root_node = 0 , exit_on_err = True , ** kwargs ) Run a set of jobs specified by a directed (acyclic) graph","title":"run_digraph"},{"location":"reference/prmf/script_utils/#parameters_2","text":"digraph : nx.DiGraph directed graph specifying job execution order; nodes in the graph are assumed to have node attribute 'args' which specifies the executable in args[0] and its arguments in args[1:] and node identifiers in [0..n] condor : bool if True, use condor_submitter.sh to submit jobs dry_run : bool if True, do not submit the DAG exit_on_err : bool if True (and condor False), stop execution of digraph when one of the job nodes fails","title":"Parameters"},{"location":"reference/prmf/script_utils/#returns-todo","text":"job_ids : list of str View Source def run_digraph ( outdir , digraph , condor = False , dry_run = False , root_node = 0 , exit_on_err = True , ** kwargs ) : \"\"\" Run a set of jobs specified by a directed (acyclic) graph Parameters ---------- digraph : nx.DiGraph directed graph specifying job execution order; nodes in the graph are assumed to have node attribute 'args' which specifies the executable in args[0] and its arguments in args[1:] and node identifiers in [0..n] condor : bool if True, use condor_submitter.sh to submit jobs dry_run : bool if True, do not submit the DAG exit_on_err : bool if True (and condor False), stop execution of digraph when one of the job nodes fails Returns : TODO ------- job_ids : list of str \"\"\" job_ids = [] if ( condor ) : # then create a DAG description file for Condor dag_fp = os . path . join ( outdir , \"digraph.dag\" ) # TODO job_ids ? write_condor_dag ( dag_fp , digraph ) if ( not dry_run ) : submit_condor_dag ( dag_fp ) else : # TODO even with the pool it appears only 1 process is running pool = mp . Pool ( processes = mp . cpu_count () - 1 ) digraph = digraph . copy () # add proc node attr pointing to a Popen object #job_order = bfs_nodes ( digraph , root_node ) job_order = nx . topological_sort ( digraph ) env_warned = False for job_id in job_order : job_attrs = digraph . node [ job_id ] if 'env' in job_attrs and not env_warned : sys . stderr . write ( \"[WARNING] one or more jobs have an environment specified, but this functionality has only been implemented for condor\\n\" ) env_warned = True # wait for any predecessors to finish #preds = digraph . predecessors ( job_id ) #for pred in preds : # digraph . node [ pred ][ 'async_result' ] . wait () #sum_v = 0 #for exit in pred_exits : # sum_v += exit #if ( sum_v > 0 ) : # # then a predecessor failed # raise RuntimeError ( \"[ERROR] a predecessor to {} failed\" . format ( digraph . node [ job_id ][ 'exe' ] )) # launch this node 's job args = [job_attrs[' exe ']] + job_attrs[' args '] stdout_fh = None if(' out ' in job_attrs): stdout_fh = open(job_attrs[' out '], ' w ') else: stdout_fh = sys.stdout stderr_fh = None if(' err ' in job_attrs): stderr_fh = open(job_attrs[' err '], ' w ') else: stderr_fh = sys.stderr sys.stdout.write(\"[STATUS] Launching {} > {} 2> {}\\n\".format(\" \".join([digraph.node[job_id][' exe ']] + digraph.node[job_id][' args ']), job_attrs[' out '], job_attrs[' err '])) #def callback_for_job(exit_status): # digraph.node[job_id][' exit '] = exit_status #digraph.node[job_id][' exit '] = None # TODO for some reason providing stdout and stderr breaks everything? #async_result = pool.apply_async(sp.check_call, [args]) #, {' stdout ': stdout_fh, ' stderr ': stderr_fh}) #digraph.node[job_id][' async_result '] = async_result #async_result . wait () # TODO synchronous only #proc = sp . Popen ( args , stdout = stdout_fh , stderr = stderr_fh ) if ( not dry_run ) : exit_code = - 1 if exit_on_err : exit_code = sp . check_call ( args , stdout = stdout_fh , stderr = stderr_fh ) else : exit_code = sp . call ( args , stdout = stdout_fh , stderr = stderr_fh ) print ( exit_code ) return job_ids","title":"Returns : TODO"},{"location":"reference/prmf/script_utils/#str2bool","text":"def str2bool ( v ) View Source def str2bool ( v ): if isinstance ( v , bool ): return v if v . lower () in ( 'yes' , 'true' , 't' , 'y' , '1' ): return True elif v . lower () in ( 'no' , 'false' , 'f' , 'n' , '0' ): return False else : raise argparse . ArgumentTypeError ( 'Boolean value expected.' )","title":"str2bool"},{"location":"reference/prmf/script_utils/#submit_condor_dag","text":"def submit_condor_dag ( dag_fp ) View Source def submit_condor_dag ( dag_fp ) : # TODO output files go in cwd or location of dag file args = [ \"condor_submit_dag\", dag_fp ] sp . check_call ( args )","title":"submit_condor_dag"},{"location":"reference/prmf/script_utils/#write_condor_dag","text":"def write_condor_dag ( dag_fp , digraph ) See run_digraph View Source def write_condor_dag ( dag_fp , digraph ) : \"\"\" See run_digraph \"\"\" JOB_FMT_STR = \"JOB {} {}\" PARENT_FMT_STR = \"PARENT {} CHILD {}\" int_to_chr_offset = ord ( 'A' ) job_int_to_name = {} with open ( dag_fp , \"w\" ) as fh : # get generic condor job description filepath condor_submit_fp = get_condor_submit_fp () job_order = nx . topological_sort ( digraph ) for job_int in job_order : # write JOB declaration job_attrs = digraph . node [ job_int ] job_name = job_attrs_to_job_name ( ** job_attrs ) job_int_to_name [ job_int ] = job_name fh . write ( JOB_FMT_STR . format ( job_name , condor_submit_fp ) + \"\\n\" ) # write VARS declaration fh . write ( format_vars ( job_name , ** job_attrs ) + \"\\n\" ) for edge in digraph . edges_iter () : # write PARENT .. CHILD declaration fh . write ( PARENT_FMT_STR . format ( job_int_to_name [ edge[0 ] ] , job_int_to_name [ edge[1 ] ] ) + \"\\n\" )","title":"write_condor_dag"},{"location":"reference/prmf/string_db/","text":"Module prmf.string_db Functions to parse STRING protein-protein interaction database: head -n2 9606.protein.links.full.v10.5.txt protein1 protein2 neighborhood neighborhood_transferred fusion cooccurence homology coexpression coexpression_transferred experiments experiments_transferred database database_transferred textmining textmining_transferred combined_score 9606.ENSP00000000233 9606.ENSP00000263431 0 0 0 0 0 0 53 0 176 0 0 0 128 260 View Source \"\"\" Functions to parse STRING protein-protein interaction database: head -n2 9606.protein.links.full.v10.5.txt protein1 protein2 neighborhood neighborhood_transferred fusion cooccurence homology coexpression coexpression_transferred experiments experiments_transferred database database_transferred textmining textmining_transferred combined_score 9606.ENSP00000000233 9606.ENSP00000263431 0 0 0 0 0 0 53 0 176 0 0 0 128 260 \"\"\" import networkx as nx def parse_line_abc ( line , score_index =- 1 ): \"\"\" Filter STRING data to just protein,protein,confidence and remove taxonomy code from protein identifiers \"\"\" line = line . rstrip () words = line . split () p1 = words [ 0 ] p2 = words [ 1 ] # remove human taxonomy code prefix p1 = trim_taxonomy_code ( p1 ) p2 = trim_taxonomy_code ( p2 ) score = words [ score_index ] conf = int ( score ) / 1000.0 return ( p1 , p2 , conf ) def trim_taxonomy_code ( protein_id ): return protein_id [ 5 :] def parse_string_fh ( fh , threshold = 0.0 , score = 'combined_score' ): \"\"\" Parameters ---------- fh : file-like STRING db database file threshold : float edge weight confidence threshold expressed as a value in [0,1] include all edges with a confidence greater than or equal to the threshold score : str the name of a column in the string format to use as the edge confidence score default: combined_score Can be \"neighborhood\" \"fusion\" \"cooccurence\" \"coexpression\" \"experimental\" \"database\" \"textmining\" or \"combined_score\" Returns ------- G : nx.Graph protein-protein interaction network with Ensembl protein ids as node ids \"\"\" # skip header line_no = 1 header = fh . __next__ () columns = header . strip () . split () if score not in columns [ 2 :]: # TODO exception class raise Exception ( \"Invalid score column {}; must be one of {}\" . format ( score , \", \" . join ( columns [ 2 :]))) score_index = columns . index ( score ) G = nx . Graph () for line in fh : line_no += 1 p1 , p2 , conf = parse_line_abc ( line , score_index = score_index ) if ( conf >= threshold ): G . add_edge ( p1 , p2 , { 'weight' : conf }) return G def string_to_abc ( ifh , ofh ): \"\"\" Convert string database to \"abc\"-format Parameters ---------- ifh : file-like String database ofh : file-like Output file handle to write abc graph to \"\"\" # skip header ifh . next () for line in ifh : p1 , p2 , conf = parse_line_abc ( line ) ofh . write ( ' \\t ' . join ( map ( str , [ p1 , p2 , conf ])) + ' \\n ' ) Functions parse_line_abc def parse_line_abc ( line , score_index =- 1 ) Filter STRING data to just protein,protein,confidence and remove taxonomy code from protein identifiers View Source def parse_line_abc ( line , score_index =- 1 ) : \"\"\" Filter STRING data to just protein,protein,confidence and remove taxonomy code from protein identifiers \"\"\" line = line . rstrip () words = line . split () p1 = words [ 0 ] p2 = words [ 1 ] # remove human taxonomy code prefix p1 = trim_taxonomy_code ( p1 ) p2 = trim_taxonomy_code ( p2 ) score = words [ score_index ] conf = int ( score ) / 1000.0 return ( p1 , p2 , conf ) parse_string_fh def parse_string_fh ( fh , threshold = 0.0 , score = 'combined_score' ) Parameters fh : file-like STRING db database file threshold : float edge weight confidence threshold expressed as a value in [0,1] include all edges with a confidence greater than or equal to the threshold score : str the name of a column in the string format to use as the edge confidence score default: combined_score Can be \"neighborhood\" \"fusion\" \"cooccurence\" \"coexpression\" \"experimental\" \"database\" \"textmining\" or \"combined_score\" Returns G : nx.Graph protein-protein interaction network with Ensembl protein ids as node ids View Source def parse_string_fh ( fh , threshold = 0 . 0 , score = 'combined_score' ): \"\"\" Parameters ---------- fh : file-like STRING db database file threshold : float edge weight confidence threshold expressed as a value in [0,1] include all edges with a confidence greater than or equal to the threshold score : str the name of a column in the string format to use as the edge confidence score default: combined_score Can be \" neighborhood \" \" fusion \" \" cooccurence \" \" coexpression \" \" experimental \" \" database \" \" textmining \" or \" combined_score \" Returns ------- G : nx.Graph protein-protein interaction network with Ensembl protein ids as node ids \"\"\" # skip header line_no = 1 header = fh . __next__ () columns = header . strip (). split () if score not in columns [ 2 :]: # TODO exception class raise Exception ( \"Invalid score column {}; must be one of {}\" . format ( score , \", \" . join ( columns [ 2 :]))) score_index = columns . index ( score ) G = nx . Graph () for line in fh : line_no += 1 p1 , p2 , conf = parse_line_abc ( line , score_index = score_index ) if ( conf >= threshold ): G . add_edge ( p1 , p2 , { 'weight' : conf } ) return G string_to_abc def string_to_abc ( ifh , ofh ) Convert string database to \"abc\"-format Parameters ifh : file-like String database ofh : file-like Output file handle to write abc graph to View Source def string_to_abc ( ifh , ofh ): \"\"\" Convert string database to \" abc \"-format Parameters ---------- ifh : file-like String database ofh : file-like Output file handle to write abc graph to \"\"\" # skip header ifh . next () for line in ifh : p1 , p2 , conf = parse_line_abc ( line ) ofh . write ( '\\t' . join ( map ( str , [ p1 , p2 , conf ])) + '\\n' ) trim_taxonomy_code def trim_taxonomy_code ( protein_id ) View Source def trim_taxonomy_code ( protein_id ): return protein_id [ 5 :]","title":"String Db"},{"location":"reference/prmf/string_db/#module-prmfstring_db","text":"Functions to parse STRING protein-protein interaction database: head -n2 9606.protein.links.full.v10.5.txt protein1 protein2 neighborhood neighborhood_transferred fusion cooccurence homology coexpression coexpression_transferred experiments experiments_transferred database database_transferred textmining textmining_transferred combined_score 9606.ENSP00000000233 9606.ENSP00000263431 0 0 0 0 0 0 53 0 176 0 0 0 128 260 View Source \"\"\" Functions to parse STRING protein-protein interaction database: head -n2 9606.protein.links.full.v10.5.txt protein1 protein2 neighborhood neighborhood_transferred fusion cooccurence homology coexpression coexpression_transferred experiments experiments_transferred database database_transferred textmining textmining_transferred combined_score 9606.ENSP00000000233 9606.ENSP00000263431 0 0 0 0 0 0 53 0 176 0 0 0 128 260 \"\"\" import networkx as nx def parse_line_abc ( line , score_index =- 1 ): \"\"\" Filter STRING data to just protein,protein,confidence and remove taxonomy code from protein identifiers \"\"\" line = line . rstrip () words = line . split () p1 = words [ 0 ] p2 = words [ 1 ] # remove human taxonomy code prefix p1 = trim_taxonomy_code ( p1 ) p2 = trim_taxonomy_code ( p2 ) score = words [ score_index ] conf = int ( score ) / 1000.0 return ( p1 , p2 , conf ) def trim_taxonomy_code ( protein_id ): return protein_id [ 5 :] def parse_string_fh ( fh , threshold = 0.0 , score = 'combined_score' ): \"\"\" Parameters ---------- fh : file-like STRING db database file threshold : float edge weight confidence threshold expressed as a value in [0,1] include all edges with a confidence greater than or equal to the threshold score : str the name of a column in the string format to use as the edge confidence score default: combined_score Can be \"neighborhood\" \"fusion\" \"cooccurence\" \"coexpression\" \"experimental\" \"database\" \"textmining\" or \"combined_score\" Returns ------- G : nx.Graph protein-protein interaction network with Ensembl protein ids as node ids \"\"\" # skip header line_no = 1 header = fh . __next__ () columns = header . strip () . split () if score not in columns [ 2 :]: # TODO exception class raise Exception ( \"Invalid score column {}; must be one of {}\" . format ( score , \", \" . join ( columns [ 2 :]))) score_index = columns . index ( score ) G = nx . Graph () for line in fh : line_no += 1 p1 , p2 , conf = parse_line_abc ( line , score_index = score_index ) if ( conf >= threshold ): G . add_edge ( p1 , p2 , { 'weight' : conf }) return G def string_to_abc ( ifh , ofh ): \"\"\" Convert string database to \"abc\"-format Parameters ---------- ifh : file-like String database ofh : file-like Output file handle to write abc graph to \"\"\" # skip header ifh . next () for line in ifh : p1 , p2 , conf = parse_line_abc ( line ) ofh . write ( ' \\t ' . join ( map ( str , [ p1 , p2 , conf ])) + ' \\n ' )","title":"Module prmf.string_db"},{"location":"reference/prmf/string_db/#functions","text":"","title":"Functions"},{"location":"reference/prmf/string_db/#parse_line_abc","text":"def parse_line_abc ( line , score_index =- 1 ) Filter STRING data to just protein,protein,confidence and remove taxonomy code from protein identifiers View Source def parse_line_abc ( line , score_index =- 1 ) : \"\"\" Filter STRING data to just protein,protein,confidence and remove taxonomy code from protein identifiers \"\"\" line = line . rstrip () words = line . split () p1 = words [ 0 ] p2 = words [ 1 ] # remove human taxonomy code prefix p1 = trim_taxonomy_code ( p1 ) p2 = trim_taxonomy_code ( p2 ) score = words [ score_index ] conf = int ( score ) / 1000.0 return ( p1 , p2 , conf )","title":"parse_line_abc"},{"location":"reference/prmf/string_db/#parse_string_fh","text":"def parse_string_fh ( fh , threshold = 0.0 , score = 'combined_score' )","title":"parse_string_fh"},{"location":"reference/prmf/string_db/#parameters","text":"fh : file-like STRING db database file threshold : float edge weight confidence threshold expressed as a value in [0,1] include all edges with a confidence greater than or equal to the threshold score : str the name of a column in the string format to use as the edge confidence score default: combined_score Can be \"neighborhood\" \"fusion\" \"cooccurence\" \"coexpression\" \"experimental\" \"database\" \"textmining\" or \"combined_score\"","title":"Parameters"},{"location":"reference/prmf/string_db/#returns","text":"G : nx.Graph protein-protein interaction network with Ensembl protein ids as node ids View Source def parse_string_fh ( fh , threshold = 0 . 0 , score = 'combined_score' ): \"\"\" Parameters ---------- fh : file-like STRING db database file threshold : float edge weight confidence threshold expressed as a value in [0,1] include all edges with a confidence greater than or equal to the threshold score : str the name of a column in the string format to use as the edge confidence score default: combined_score Can be \" neighborhood \" \" fusion \" \" cooccurence \" \" coexpression \" \" experimental \" \" database \" \" textmining \" or \" combined_score \" Returns ------- G : nx.Graph protein-protein interaction network with Ensembl protein ids as node ids \"\"\" # skip header line_no = 1 header = fh . __next__ () columns = header . strip (). split () if score not in columns [ 2 :]: # TODO exception class raise Exception ( \"Invalid score column {}; must be one of {}\" . format ( score , \", \" . join ( columns [ 2 :]))) score_index = columns . index ( score ) G = nx . Graph () for line in fh : line_no += 1 p1 , p2 , conf = parse_line_abc ( line , score_index = score_index ) if ( conf >= threshold ): G . add_edge ( p1 , p2 , { 'weight' : conf } ) return G","title":"Returns"},{"location":"reference/prmf/string_db/#string_to_abc","text":"def string_to_abc ( ifh , ofh ) Convert string database to \"abc\"-format","title":"string_to_abc"},{"location":"reference/prmf/string_db/#parameters_1","text":"ifh : file-like String database ofh : file-like Output file handle to write abc graph to View Source def string_to_abc ( ifh , ofh ): \"\"\" Convert string database to \" abc \"-format Parameters ---------- ifh : file-like String database ofh : file-like Output file handle to write abc graph to \"\"\" # skip header ifh . next () for line in ifh : p1 , p2 , conf = parse_line_abc ( line ) ofh . write ( '\\t' . join ( map ( str , [ p1 , p2 , conf ])) + '\\n' )","title":"Parameters"},{"location":"reference/prmf/string_db/#trim_taxonomy_code","text":"def trim_taxonomy_code ( protein_id ) View Source def trim_taxonomy_code ( protein_id ): return protein_id [ 5 :]","title":"trim_taxonomy_code"}]}